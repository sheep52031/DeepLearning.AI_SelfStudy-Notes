1
00:00:02,585 --> 00:00:04,754
In this lesson, you will build a pipeline

2
00:00:02,585 --> 00:00:04,754
在這堂課中，你將建立一個管線

3
00:00:04,754 --> 00:00:09,754
for group relative policy optimization, or GRPO. One of the popular online RL

4
00:00:04,754 --> 00:00:09,754
群組相對策略優化，或稱 GRPO。這是目前熱門的線上強化學習

5
00:00:10,301 --> 00:00:11,636
methods.

6
00:00:10,301 --> 00:00:11,636
方法之一。

7
00:00:11,636 --> 00:00:13,096
Let's have some fun.

8
00:00:11,636 --> 00:00:13,096
讓我們來玩點有趣的。

9
00:00:13,096 --> 00:00:15,724
As you remember online reinforcement learning is trying

10
00:00:13,096 --> 00:00:15,724
正如你所記得的那樣，線上強化學習正試圖

11
00:00:15,724 --> 00:00:18,727
to let the model itself explore better responses.

12
00:00:15,724 --> 00:00:18,727
讓模型自行探索更好的回應方式。

13
00:00:18,977 --> 00:00:19,894
In the lab,

14
00:00:18,977 --> 00:00:19,894
在實驗室中，

15
00:00:19,894 --> 00:00:22,897
we'll start from curating a set of math problems.

16
00:00:19,894 --> 00:00:22,897
我們將從整理一系列數學題目開始。

17
00:00:22,897 --> 00:00:27,235
Send that to a current language model, and let's model generate multiple responses,

18
00:00:22,897 --> 00:00:27,235
將這段文字傳送給現有的語言模型，讓模型產生多種回應，

19
00:00:27,485 --> 00:00:30,572
we'll curate a reward function, which is a verifiable reward

20
00:00:27,485 --> 00:00:30,572
我們會策劃一個獎勵函數，這是一個可驗證的獎勵機制

21
00:00:30,739 --> 00:00:34,075
that checks whether the response matches the ground truth or not.

22
00:00:30,739 --> 00:00:34,075
用來檢查回應是否符合真實情況。

23
00:00:34,284 --> 00:00:38,371
Then we'll get a tuple of prompt responses and reward.

24
00:00:34,284 --> 00:00:38,371
接著我們就能獲得一組包含提示、回應和獎勵的元組。

25
00:00:38,496 --> 00:00:41,875
And we'll use GRPO to update the language model.

26
00:00:38,496 --> 00:00:41,875
我們將使用 GRPO 來更新語言模型。

27
00:00:42,000 --> 00:00:44,461
Great. Let's see all of this in the code.

28
00:00:42,000 --> 00:00:44,461
太好了。讓我們在程式碼中看看這一切。

29
00:00:44,461 --> 00:00:45,795
For online reinforcement learning,

30
00:00:44,461 --> 00:00:45,795
對於線上強化學習，

31
00:00:45,795 --> 00:00:49,090
as usual, we start with importing importing libraries.

32
00:00:45,795 --> 00:00:49,090
一如往常，我們從導入函式庫開始。

33
00:00:49,299 --> 00:00:54,299
And here, everything's very similar to DPO SFT, except that for an TRL

34
00:00:49,299 --> 00:00:54,299
而這裡的一切與 DPO SFT 非常相似，除了我們使用 TRL

35
00:00:55,180 --> 00:00:58,475
we're using GRPO Trainer and GRPO config

36
00:00:55,180 --> 00:00:58,475
我們正在使用 GRPO Trainer 和 GRPO config

37
00:00:58,558 --> 00:01:02,187
to set up the training environment for GRPO here.

38
00:00:58,558 --> 00:01:02,187
來為 GRPO 設定訓練環境。

39
00:01:02,395 --> 00:01:05,272
Unlike the previous two coding lessons where

40
00:01:02,395 --> 00:01:05,272
與前兩節編碼課程不同的是

41
00:01:05,272 --> 00:01:10,153
we only test model on a few example prompts, here, let's prepare

42
00:01:05,272 --> 00:01:10,153
我們只在幾個範例提示上測試模型，這裡，讓我們準備

43
00:01:10,153 --> 00:01:10,403
we only test model on a few example prompts, here, let's prepare

44
00:01:10,153 --> 00:01:10,403
我們只在幾個範例提示上測試模型，這裡，讓我們準備

45
00:01:10,403 --> 00:01:14,491
for an evaluation dataset for math, which is GSM8K to start with.

46
00:01:10,403 --> 00:01:14,491
一個數學評估資料集，從 GSM8K 開始。

47
00:01:14,491 --> 00:01:17,660
Let's still first set up the use GPU as false

48
00:01:14,491 --> 00:01:17,660
我們還是先將 use GPU 設為 false

49
00:01:18,119 --> 00:01:21,456
and feel free turn that as true if you run that on your own GPU machine.

50
00:01:18,119 --> 00:01:21,456
如果你在自己的 GPU 機器上運行，可以放心將此設為 true。

51
00:01:21,456 --> 00:01:22,123
and feel free turn that as true if you run that on your own GPU machine.

52
00:01:21,456 --> 00:01:22,123
如果你在自己的 GPU 機器上運行，可以放心將此設為 true。

53
00:01:22,165 --> 00:01:24,709
And we also need to set a persistent prompt

54
00:01:22,165 --> 00:01:24,709
我們還需要設定一個持續性的提示

55
00:01:24,709 --> 00:01:28,421
saying that you are a helpful assistant that solves problems step by step,

56
00:01:24,709 --> 00:01:28,421
說明你是一個會逐步解決問題的得力助手

57
00:01:28,671 --> 00:01:32,008
and trace always includes a final numeric answer inside a boxed.

58
00:01:28,671 --> 00:01:32,008
且追蹤過程總是會在方框內包含最終的數值答案。

59
00:01:32,509 --> 00:01:35,845
So this sentence is critical in making the model

60
00:01:32,509 --> 00:01:35,845
因此這句話對於讓模型

61
00:01:35,887 --> 00:01:37,764
outputting the final response in a good format, so that later

62
00:01:35,887 --> 00:01:37,764
以良好格式輸出最終回應至關重要，如此一來後續

63
00:01:37,764 --> 00:01:38,223
outputting the final response in a good format, so that later

64
00:01:37,764 --> 00:01:38,223
以良好格式輸出最終回應至關重要，如此一來後續

65
00:01:38,223 --> 00:01:39,766
outputting the final response in a good format, so that later

66
00:01:38,223 --> 00:01:39,766
以良好的格式輸出最終回應，以便後續

67
00:01:39,766 --> 00:01:43,520
we can easily extract the response and compare that with the prompts.

68
00:01:39,766 --> 00:01:43,520
能夠輕鬆提取回應並與提示進行比較。

69
00:01:43,812 --> 00:01:48,483
Next, let's define our reward function that can be useful and important

70
00:01:43,812 --> 00:01:48,483
接下來，讓我們定義一個既實用又重要的獎勵函數

71
00:01:48,608 --> 00:01:50,026
for both training

72
00:01:48,608 --> 00:01:50,026
適用於訓練過程

73
00:01:50,026 --> 00:01:53,154
using oRL and also evaluation with GSM8K.

74
00:01:50,026 --> 00:01:53,154
使用 oRL 並以 GSM8K 進行評估

75
00:01:53,154 --> 00:01:55,990
It takes the model's generated completions

76
00:01:53,154 --> 00:01:55,990
它會擷取模型生成的完整內容

77
00:01:55,990 --> 00:01:56,366
or the generated results and the ground truth.

78
00:01:55,990 --> 00:01:56,366
或是生成的結果與標準答案

79
00:01:56,366 --> 00:01:58,827
or the generated results and the ground truth.

80
00:01:56,366 --> 00:01:58,827
或是生成的結果與標準答案

81
00:01:58,868 --> 00:02:03,868
So what we're doing here is we first try to do regular expression mesh

82
00:01:58,868 --> 00:02:03,868
我們在這裡的做法是首先嘗試使用正規表達式匹配

83
00:02:04,415 --> 00:02:07,418
to capture the content inside the boxed

84
00:02:04,415 --> 00:02:07,418
來捕捉方框內的內容

85
00:02:07,418 --> 00:02:10,547
as we provided in the instruction of system prompt.

86
00:02:07,418 --> 00:02:10,547
如同我們在系統提示指令中所提供的

87
00:02:10,880 --> 00:02:14,175
After we see all the matches here, we'll just take

88
00:02:10,880 --> 00:02:14,175
當我們看到所有匹配結果後，就會直接採用

89
00:02:14,342 --> 00:02:17,679
the very first match and takes that alphas out of the model.

90
00:02:14,342 --> 00:02:17,679
在第一場比賽中，我們就將這些 alpha 值從模型中移除。

91
00:02:17,887 --> 00:02:22,559
And if there's no match, we'll just make the output of the model empty here.

92
00:02:17,887 --> 00:02:22,559
如果沒有匹配到任何結果，我們就會讓模型的輸出在此處保持空白。

93
00:02:22,642 --> 00:02:26,980
And next, we'll just directly compare the content with the ground truth.

94
00:02:22,642 --> 00:02:26,980
接下來，我們會直接將內容與標準答案進行比對。

95
00:02:27,188 --> 00:02:30,191
And if the content is the same as ground truth,

96
00:02:27,188 --> 00:02:30,191
若內容與標準答案相符，

97
00:02:30,441 --> 00:02:31,943
then the reward will just be one.

98
00:02:30,441 --> 00:02:31,943
那麼獎勵就只會是一。

99
00:02:31,943 --> 00:02:34,028
Otherwise, the reward will just be zero.

100
00:02:31,943 --> 00:02:34,028
否則，獎勵就只會是零。

101
00:02:34,028 --> 00:02:37,824
Now that we have a reward function defined, let's test how it works

102
00:02:34,028 --> 00:02:37,824
現在我們已經定義好獎勵函數，來測試看看它的運作

103
00:02:37,824 --> 00:02:38,491
in general.

104
00:02:37,824 --> 00:02:38,491
情況如何。

105
00:02:38,491 --> 00:02:41,953
Assume that we have a sample prediction which is coming from a certain

106
00:02:38,491 --> 00:02:41,953
假設我們有一個樣本預測來自某個

107
00:02:42,036 --> 00:02:43,079
and saying like

108
00:02:42,036 --> 00:02:43,079
並且表示如下

109
00:02:43,079 --> 00:02:46,833
First, there are a few steps to calculate the answer, followed by a final answer

110
00:02:43,079 --> 00:02:46,833
首先，有幾個步驟來計算答案，接著是最終答案

111
00:02:46,833 --> 00:02:50,795
which is boxed 72 and assume that the ground truth is also 72.

112
00:02:46,833 --> 00:02:50,795
標示為方框的 72，並假設實際正確答案也是 72。

113
00:02:50,879 --> 00:02:53,173
Then when we calculate the reward, the positive sample reward will just always be one.

114
00:02:50,879 --> 00:02:53,173
那麼當我們計算獎勵時，正向樣本的獎勵值永遠會是 1。

115
00:02:53,173 --> 00:02:55,341
Then when we calculate the reward, the positive sample reward will just always be one.

116
00:02:53,173 --> 00:02:55,341
那麼當我們計算獎勵時，正向樣本的獎勵值永遠會是 1。

117
00:02:55,425 --> 00:02:59,429
Next, let's see a negative example where if the sample prediction

118
00:02:55,425 --> 00:02:59,429
接著讓我們看一個負面例子，如果樣本預測

119
00:02:59,637 --> 00:03:04,058
is only a one-off, the content inside the boxed is 71, while the quantity is 72.

120
00:02:59,637 --> 00:03:04,058
僅有 1 的誤差，框內內容是 71，而實際數量是 72。

121
00:03:04,559 --> 00:03:05,018
Then, if you execute and calculate the reward function,

122
00:03:04,559 --> 00:03:05,018
接著，如果你執行並計算獎勵函數，

123
00:03:05,018 --> 00:03:05,977
Then, if you execute and calculate the reward function,

124
00:03:05,018 --> 00:03:05,977
接著，如果你執行並計算獎勵函數，

125
00:03:05,977 --> 00:03:07,312
Then, if you execute and calculate the reward function,

126
00:03:05,977 --> 00:03:07,312
接著，如果你執行並計算獎勵函數，

127
00:03:07,729 --> 00:03:09,397
then the reward will be zero.

128
00:03:07,729 --> 00:03:09,397
那麼獎勵將會是零。

129
00:03:09,397 --> 00:03:10,899
Now that we have the reward function,

130
00:03:09,397 --> 00:03:10,899
現在我們已經有了獎勵函數，

131
00:03:10,899 --> 00:03:12,984
we're ready to load the evaluation dataset.

132
00:03:10,899 --> 00:03:12,984
準備好載入評估資料集了。

133
00:03:12,984 --> 00:03:15,028
We'll load the dataset from OpenAI GSM8K

134
00:03:12,984 --> 00:03:15,028
我們將從 OpenAI GSM8K 載入資料集

135
00:03:16,070 --> 00:03:17,864
and load the test portion with that.

136
00:03:16,070 --> 00:03:17,864
並載入其中的測試部分。

137
00:03:17,864 --> 00:03:18,823
And we'll select the first 5000 to speed up the process

138
00:03:17,864 --> 00:03:18,823
我們將選取前 5000 筆資料來加速流程

139
00:03:18,823 --> 00:03:19,199
And we'll select the first 5000 to speed up the process

140
00:03:18,823 --> 00:03:19,199
我們將選取前 5000 筆資料來加速流程

141
00:03:19,199 --> 00:03:22,243
And we'll select the first 5000 to speed up the process

142
00:03:19,199 --> 00:03:22,243
我們將選取前 5000 筆資料來加速流程

143
00:03:22,243 --> 00:03:24,370
where we set the data from here to be five.

144
00:03:22,243 --> 00:03:24,370
我們將此處的資料設定為五筆。

145
00:03:24,370 --> 00:03:27,373
And we can display the dataset and see how it looks like.

146
00:03:24,370 --> 00:03:27,373
我們可以顯示資料集並查看它的樣貌。

147
00:03:27,373 --> 00:03:28,374
So you'll see it comes

148
00:03:27,373 --> 00:03:28,374
你會看到它包含了

149
00:03:28,374 --> 00:03:31,002
with some questions amassed along with some answers as ground truth.

150
00:03:28,374 --> 00:03:31,002
一些收集的問題以及作為基準答案的解答。

151
00:03:31,002 --> 00:03:32,253
with some questions amassed along with some answers as ground truth.

152
00:03:31,002 --> 00:03:32,253
一些收集的問題以及作為基準答案的解答。

153
00:03:32,587 --> 00:03:34,839
And in this case the answer is always hidden

154
00:03:32,587 --> 00:03:34,839
在這種情況下，答案總是隱藏在

155
00:03:34,839 --> 00:03:36,007
after the four shops here.

156
00:03:34,839 --> 00:03:36,007
這四家店鋪之後。

157
00:03:36,007 --> 00:03:36,758
after the four shops here.

158
00:03:36,007 --> 00:03:36,758
這四家店鋪之後。

159
00:03:36,758 --> 00:03:39,761
And so we need to extract the answer as ground truth.

160
00:03:36,758 --> 00:03:39,761
因此我們需要提取答案作為基準真值。

161
00:03:39,844 --> 00:03:43,890
So now that we have such long dataset with prompts and answers,

162
00:03:39,844 --> 00:03:43,890
既然我們現在有了這麼龐大的提示與回答資料集

163
00:03:44,015 --> 00:03:47,018
we can define a new post-processing function

164
00:03:44,015 --> 00:03:47,018
我們可以定義一個新的後處理函數

165
00:03:47,185 --> 00:03:50,813
that tries to match the answer first from the shop signal

166
00:03:47,185 --> 00:03:50,813
這個函數會優先從商店信號中嘗試匹配答案

167
00:03:50,813 --> 00:03:51,231
that tries to match the answer first from the shop signal

168
00:03:50,813 --> 00:03:51,231
這個函數會優先從商店信號中嘗試匹配答案

169
00:03:51,356 --> 00:03:56,356
and then we always says the ground truth to be the next item here.

170
00:03:51,356 --> 00:03:56,356
然後我們總是將下一個項目視為這裡的基準答案。

171
00:03:56,986 --> 00:03:59,405
In this way, we can not only have the ground truth,

172
00:03:56,986 --> 00:03:59,405
這樣一來，我們不僅能獲得基準答案，

173
00:03:59,405 --> 00:04:03,243
but also reset a prompt which includes both a system prompt

174
00:03:59,405 --> 00:04:03,243
還能重新設定一個包含系統提示的指令，

175
00:04:03,243 --> 00:04:06,871
we defined before that instructs the model to put answer in boxed

176
00:04:03,243 --> 00:04:06,871
也就是我們先前定義的、要求模型將答案放入方框中的提示

177
00:04:07,121 --> 00:04:10,208
along with the user prompt, which is a question itself.

178
00:04:07,121 --> 00:04:10,208
連同使用者提示（本身即為一個問題）

179
00:04:10,416 --> 00:04:14,921
Then we're ready to map the pre-processed dataset and updates

180
00:04:10,416 --> 00:04:14,921
接著我們準備將預處理過的資料集與更新內容對應

181
00:04:14,963 --> 00:04:16,589
in the new evaluation dataset.

182
00:04:14,963 --> 00:04:16,589
至新的評估資料集中

183
00:04:16,589 --> 00:04:19,175
Let's take a look at how the new dataset looks like.

184
00:04:16,589 --> 00:04:19,175
讓我們來看看新資料集的樣貌

185
00:04:19,175 --> 00:04:23,513
You'll see after some post-processing, the dataset only have two columns.

186
00:04:19,175 --> 00:04:23,513
經過一些後處理後，你會發現資料集只剩下兩欄。

187
00:04:23,554 --> 00:04:26,557
One is ground truth, which is exactly the ground truth

188
00:04:23,554 --> 00:04:26,557
一欄是基準答案，也就是從原始回應中

189
00:04:26,557 --> 00:04:29,769
number extracted from the original responses.

190
00:04:26,557 --> 00:04:29,769
準確提取出來的數值。

191
00:04:29,894 --> 00:04:31,688
A second, is a prompt, which is always a system prompt,

192
00:04:29,894 --> 00:04:31,688
另一欄則是提示詞，固定都是系統提示詞，

193
00:04:31,688 --> 00:04:32,021
A second, is a prompt, which is always a system prompt,

194
00:04:31,688 --> 00:04:32,021
第二個是一個提示詞，永遠都是系統提示詞，

195
00:04:32,021 --> 00:04:33,022
A second, is a prompt, which is always a system prompt,

196
00:04:32,021 --> 00:04:33,022
第二個是一個提示詞，永遠都是系統提示詞，

197
00:04:33,189 --> 00:04:34,816
followed by some questions here.

198
00:04:33,189 --> 00:04:34,816
接著這裡會有一些問題。

199
00:04:34,816 --> 00:04:38,444
Now that we already have the dataset post-process,

200
00:04:34,816 --> 00:04:38,444
現在我們已經完成資料集的後處理，

201
00:04:38,736 --> 00:04:41,531
we're ready to load model and evaluate the model.

202
00:04:38,736 --> 00:04:41,531
我們準備好載入模型並評估模型效能。

203
00:04:41,572 --> 00:04:45,285
here. We loaded the Qwen 2.5-0.5B instruct model and evaluated

204
00:04:41,572 --> 00:04:45,285
這裡我們載入了 Qwen 2.5-0.5B 指令模型並進行評估

205
00:04:45,285 --> 00:04:50,081
on the loaded on five prompts from the GSM8K test dataset. To evaluate

206
00:04:45,285 --> 00:04:50,081
針對 GSM8K 測試資料集中的五個提示進行評估

207
00:04:50,081 --> 00:04:54,294
this model or start from an empty list of predictions and ground truth labels.

208
00:04:50,081 --> 00:04:54,294
評估此模型時，我們從空的預測結果清單和真實標籤開始

209
00:04:54,419 --> 00:04:55,920
We go through all the post-process dataset

210
00:04:54,419 --> 00:04:55,920
我們遍歷所有後處理的數據集

211
00:04:55,920 --> 00:04:57,255
We go through all the post-process dataset

212
00:04:55,920 --> 00:04:57,255
我們遍歷所有後處理的數據集

213
00:04:57,297 --> 00:05:00,675
and ask the input prompt and ground truth, and then we generate

214
00:04:57,297 --> 00:05:00,675
並詢問輸入提示和真實答案，然後我們生成

215
00:05:00,717 --> 00:05:04,137
responses using our process generate responses

216
00:05:00,717 --> 00:05:04,137
使用我們的流程生成回應

217
00:05:04,387 --> 00:05:07,974
function feeding the model tokenizer and the full message here.

218
00:05:04,387 --> 00:05:07,974
將模型的分詞器和完整訊息餵入此函式

219
00:05:08,266 --> 00:05:11,769
Then we can append the predictions and append the labels,

220
00:05:08,266 --> 00:05:11,769
接著我們可以附加預測結果並附加標籤

221
00:05:11,894 --> 00:05:14,939
and that prints the response as ground truth for you to take a look.

222
00:05:11,894 --> 00:05:14,939
這會印出回應作為參考基準供您查看

223
00:05:15,023 --> 00:05:18,443
And eventually we can use this reward function to calculate

224
00:05:15,023 --> 00:05:18,443
最終我們可以使用這個獎勵函式來計算

225
00:05:18,651 --> 00:05:21,529
how many responses are matching the ground truth.

226
00:05:18,651 --> 00:05:21,529
有多少回應與真實答案相符。

227
00:05:21,529 --> 00:05:23,823
And eventually we can report accuracy here.

228
00:05:21,529 --> 00:05:23,823
最終我們可以在這裡報告準確率。

229
00:05:23,823 --> 00:05:28,119
This generation process might take longer, so we'll speed in the post edit

230
00:05:23,823 --> 00:05:28,119
這個生成過程可能需要更長時間，所以我們會在後續編輯中加快速度

231
00:05:28,244 --> 00:05:30,204
Now that the evaluation is done, on the five prompts,

232
00:05:28,244 --> 00:05:30,204
現在評估已完成，針對這五個提示，

233
00:05:30,204 --> 00:05:30,705
Now that the evaluation is done, on the five prompts,

234
00:05:30,204 --> 00:05:30,705
現在評估已經完成，針對這五個提示，

235
00:05:30,705 --> 00:05:35,084
we're ready to check the whether the responses match for the ground truths.

236
00:05:30,705 --> 00:05:35,084
我們準備好檢查回應是否符合實際情況。

237
00:05:35,126 --> 00:05:40,126
So for the first answer, we'll see that there are no boxed provided in the answer.

238
00:05:35,126 --> 00:05:40,126
所以對於第一個答案，我們會發現回答中沒有提供框選內容。

239
00:05:40,465 --> 00:05:40,840
So the answer won't be instructed.

240
00:05:40,465 --> 00:05:40,840
因此這個答案將不會被計分。

241
00:05:40,840 --> 00:05:42,508
So the answer won't be instructed.

242
00:05:40,840 --> 00:05:42,508
因此答案不會被指示。

243
00:05:42,508 --> 00:05:46,846
And thus the model is not fully structured and cannot be matched to the ground truth.

244
00:05:42,508 --> 00:05:46,846
因此模型並非完全結構化，也無法與真實情況相符。

245
00:05:47,138 --> 00:05:48,431
For a second answer,

246
00:05:47,138 --> 00:05:48,431
至於第二個答案，

247
00:05:48,431 --> 00:05:51,225
we see that the model posts boxed three inside

248
00:05:48,431 --> 00:05:51,225
我們看到模型給出了框內數字三

249
00:05:51,225 --> 00:05:54,270
this answer, which matches the ground truth. For search one,

250
00:05:51,225 --> 00:05:54,270
這個答案與標準答案相符。至於第一個搜尋結果，

251
00:05:54,437 --> 00:05:58,691
unfortunately, the model hasn't finished and due to the token limit,

252
00:05:54,437 --> 00:05:58,691
很遺憾，由於達到 token 限制，模型尚未完成運算，

253
00:05:58,775 --> 00:06:01,778
so that we still didn't see any match with the ground truth.

254
00:05:58,775 --> 00:06:01,778
因此我們仍未看到任何與標準答案相符的結果。

255
00:06:01,778 --> 00:06:06,574
For a fourth one, we see a boxed 180, which doesn't match the ground truth here.

256
00:06:01,778 --> 00:06:06,574
第四個結果中，我們看到一個標示為 180 的方框答案，但這與此處的標準答案不符。

257
00:06:06,866 --> 00:06:11,245
And lastly, for the last example, the model also hasn't finished and

258
00:06:06,866 --> 00:06:11,245
最後一個例子中，模型同樣尚未完成運算

259
00:06:12,372 --> 00:06:12,955
the ground truth is 20.

260
00:06:12,372 --> 00:06:12,955
而正確答案應為 20

261
00:06:12,955 --> 00:06:15,875
So, in total there are only like one out of five examples

262
00:06:12,955 --> 00:06:15,875
總計五個例子中僅有一個正確

263
00:06:15,875 --> 00:06:16,751
So, in total there are only like one out of five examples

264
00:06:15,875 --> 00:06:16,751
總計五個例子中僅有一個正確

265
00:06:16,793 --> 00:06:18,795
which matches the ground truth.

266
00:06:16,793 --> 00:06:18,795
這與真實情況相符。

267
00:06:18,795 --> 00:06:21,381
So the evaluation accuracy here is 20%.

268
00:06:18,795 --> 00:06:21,381
因此此處的評估準確率為 20%。

269
00:06:21,381 --> 00:06:25,426
So in practice, we would recommend you to allow much more maximum

270
00:06:21,381 --> 00:06:25,426
所以在實際操作中，我們會建議您允許生成時使用更多

271
00:06:25,426 --> 00:06:26,260
number of tokens in generation and also evaluate on the full dataset.

272
00:06:25,426 --> 00:06:26,260
的最大 token 數量，並且在完整資料集上進行評估。

273
00:06:26,260 --> 00:06:29,806
number of tokens in generation and also evaluate on the full dataset.

274
00:06:26,260 --> 00:06:29,806
生成的詞元數量，同時也在完整資料集上進行評估。

275
00:06:29,972 --> 00:06:32,308
Since only evaluating on a few samples

276
00:06:29,972 --> 00:06:32,308
由於僅評估少數樣本

277
00:06:32,308 --> 00:06:35,186
might be come with a very large variance here.

278
00:06:32,308 --> 00:06:35,186
可能會在此處帶來極大的變異性。

279
00:06:35,186 --> 00:06:38,731
As we finish designing the evaluation process we first go through

280
00:06:35,186 --> 00:06:38,731
在完成評估流程設計後，我們首先進行

281
00:06:38,773 --> 00:06:43,569
training process and leave the evaluation of our fully trained model at the end.

282
00:06:38,773 --> 00:06:43,569
訓練過程並將完整訓練模型的評估留到最後。

283
00:06:43,611 --> 00:06:45,488
So first let's start with loading

284
00:06:43,611 --> 00:06:45,488
那麼首先讓我們從載入

285
00:06:45,488 --> 00:06:48,783
the trained dataset. We'll again load in the dataset from GSM8K,

286
00:06:45,488 --> 00:06:48,783
訓練好的資料集開始。我們將再次從 GSM8K 載入資料集，

287
00:06:49,409 --> 00:06:52,620
which comes with a trained portion, the split from the test portion.

288
00:06:49,409 --> 00:06:52,620
其中包含訓練部分與測試部分的分割。

289
00:06:52,745 --> 00:06:54,247
And then we apply the same post-processing function to the trained

290
00:06:52,745 --> 00:06:54,247
接著我們對訓練後的資料套用相同的後處理函數

291
00:06:54,247 --> 00:06:55,748
And then we apply the same post-processing function to the trained

292
00:06:54,247 --> 00:06:55,748
接著我們對訓練後的資料套用相同的後處理函數

293
00:06:55,748 --> 00:06:58,751
dataset and remove unnecessary columns here.

294
00:06:55,748 --> 00:06:58,751
並在此移除不必要的欄位

295
00:06:58,960 --> 00:07:02,463
And if we're not using GPU, we only select the first ten ground truths for training.

296
00:06:58,960 --> 00:07:02,463
若未使用 GPU，我們僅選取前十筆真實數據進行訓練

297
00:07:02,463 --> 00:07:04,006
And if we're not using GPU, we only select the first ten ground truths for training.

298
00:07:02,463 --> 00:07:04,006
如果我們沒有使用 GPU，我們只會選擇前十個真實數據進行訓練。

299
00:07:04,298 --> 00:07:06,426
And I'm creating the first example here

300
00:07:04,298 --> 00:07:06,426
我在這裡創建第一個範例

301
00:07:06,426 --> 00:07:09,971
so that we can see how the ground truth and the prompt looks like.

302
00:07:06,426 --> 00:07:09,971
這樣我們就能看到真實數據和提示的樣貌。

303
00:07:10,304 --> 00:07:13,766
Now we are ready to kick off our GRPO training. As usual,

304
00:07:10,304 --> 00:07:13,766
現在我們準備好開始 GRPO 訓練了。一如往常，

305
00:07:13,933 --> 00:07:17,228
we also need a GRPO config to speed set up first,

306
00:07:13,933 --> 00:07:17,228
我們還需要先設定一個 GRPO 配置來加速流程，

307
00:07:17,311 --> 00:07:20,106
which includes the batch size related hyperparameter, the epochs,

308
00:07:17,311 --> 00:07:20,106
其中包括與批次大小相關的超參數、訓練週期數、

309
00:07:20,106 --> 00:07:20,773
which includes the batch size related hyperparameter, the epochs,

310
00:07:20,106 --> 00:07:20,773
其中包括與批次大小相關的超參數、訓練週期數、

311
00:07:20,857 --> 00:07:22,400
the learning rate, and logging steps.

312
00:07:20,857 --> 00:07:22,400
學習率以及記錄間隔步數。

313
00:07:22,400 --> 00:07:25,069
And here, the key hyperparameters

314
00:07:22,400 --> 00:07:25,069
而這裡的關鍵超參數

315
00:07:25,069 --> 00:07:25,945
that is in GRPO

316
00:07:25,069 --> 00:07:25,945
也就是在 GRPO 中

317
00:07:25,945 --> 00:07:28,906
here is this number of generations.

318
00:07:25,945 --> 00:07:28,906
就是這個生成次數。

319
00:07:28,906 --> 00:07:31,951
Remember that in GRPO we are generating multiple responses

320
00:07:28,906 --> 00:07:31,951
請記住在 GRPO 中我們會生成多個回應

321
00:07:31,951 --> 00:07:33,327
for the same prompt.

322
00:07:31,951 --> 00:07:33,327
針對同一個提示。

323
00:07:33,327 --> 00:07:35,413
And here the number of generations just controls

324
00:07:33,327 --> 00:07:35,413
而這裡的生成次數只是控制

325
00:07:35,413 --> 00:07:36,038
how many responses you generate for the same prompt.

326
00:07:35,413 --> 00:07:36,038
你為同一個提示生成多少個回應。

327
00:07:36,038 --> 00:07:37,957
how many responses you generate for the same prompt.

328
00:07:36,038 --> 00:07:37,957
你為同一個提示生成多少個回應。

329
00:07:38,791 --> 00:07:42,211
And here we're setting that to be four so that we can speed up the training.

330
00:07:38,791 --> 00:07:42,211
我們將這個值設定為 4，這樣可以加快訓練速度。

331
00:07:42,420 --> 00:07:46,883
And in practice you can set that as high as 64 or even 128

332
00:07:42,420 --> 00:07:46,883
實際上你可以將這個值設到 64 甚至 128

333
00:07:47,091 --> 00:07:48,926
so that there will be diverse enough responses.

334
00:07:47,091 --> 00:07:48,926
這樣就能產生足夠多元的回應。

335
00:07:48,926 --> 00:07:49,760
so that there will be diverse enough responses.

336
00:07:48,926 --> 00:07:49,760
這樣就能產生足夠多元的回應。

337
00:07:49,760 --> 00:07:51,888
You can compare in between the group.

338
00:07:49,760 --> 00:07:51,888
你可以在群組之間進行比較。

339
00:07:51,888 --> 00:07:56,017
Now we have the GRPO config, the dataset, and the reward function defined well,

340
00:07:51,888 --> 00:07:56,017
現在我們已經定義好 GRPO 配置、數據集和獎勵函數，

341
00:07:56,017 --> 00:08:00,062
we're ready to kick off the GRPO training. Since training GRPO

342
00:07:56,017 --> 00:08:00,062
我們準備開始進行 GRPO 訓練。由於在 CPU 機器上訓練

343
00:08:00,104 --> 00:08:00,480
0.5B model can take very long on CPU machine

344
00:08:00,104 --> 00:08:00,480
0.5B 模型可能需要很長時間

345
00:08:00,480 --> 00:08:03,566
0.5B model can take very long on CPU machine

346
00:08:00,480 --> 00:08:03,566
0.5B 模型在 CPU 機器上可能耗時非常久

347
00:08:03,608 --> 00:08:08,608
or read now only using HuggingFace small model to speed off the process.

348
00:08:03,608 --> 00:08:08,608
或是現在僅使用 HuggingFace 小型模型來加速處理流程

349
00:08:08,863 --> 00:08:09,489
And similarly,

350
00:08:08,863 --> 00:08:09,489
同樣地

351
00:08:09,489 --> 00:08:13,910
we pass model config a function and see the whole process of training.

352
00:08:09,489 --> 00:08:13,910
我們傳遞模型配置函數並觀察整個訓練過程

353
00:08:13,951 --> 00:08:16,913
Then we can pass the model config reward

354
00:08:13,951 --> 00:08:16,913
接著我們可以將模型配置獎勵函數

355
00:08:16,913 --> 00:08:18,039
function and train dataset to GRPO trainer and to kick off the training here.

356
00:08:16,913 --> 00:08:18,039
與訓練資料集傳遞給 GRPO 訓練器，並在此啟動訓練。

357
00:08:18,039 --> 00:08:20,666
function and train dataset to GRPO trainer and to kick off the training here.

358
00:08:18,039 --> 00:08:20,666
與訓練資料集傳遞給 GRPO 訓練器，並在此啟動訓練。

359
00:08:20,833 --> 00:08:25,004
This might take a very long time, so we'll speed it up in the post

360
00:08:20,833 --> 00:08:25,004
這可能會耗費很長時間，所以我們會在後續加速處理

361
00:08:25,004 --> 00:08:25,630
edits.

362
00:08:25,004 --> 00:08:25,630
編輯中加快速度。

363
00:08:25,630 --> 00:08:27,006
Now the training is done

364
00:08:25,630 --> 00:08:27,006
現在訓練已經完成

365
00:08:27,006 --> 00:08:30,301
and you might find that the training loss here is always zero.

366
00:08:27,006 --> 00:08:30,301
你可能會發現這裡的訓練損失始終為零。

367
00:08:30,718 --> 00:08:33,721
The reason behind this is that we're starting from a very small model,

368
00:08:30,718 --> 00:08:33,721
背後的原因是我們從一個非常小的模型開始，

369
00:08:33,929 --> 00:08:36,933
which cannot get most of the question correct.

370
00:08:33,929 --> 00:08:36,933
它無法正確回答大部分問題。

371
00:08:37,058 --> 00:08:38,893
And that's why, in GRRPO the relative reward is all zero,

372
00:08:37,058 --> 00:08:38,893
這就是為什麼在 GRRPO 中，相對獎勵全部為零，

373
00:08:38,893 --> 00:08:39,143
And that's why, in GRRPO the relative reward is all zero,

374
00:08:38,893 --> 00:08:39,143
這就是為什麼在 GRRPO 中，相對獎勵全部為零，

375
00:08:39,143 --> 00:08:40,811
And that's why, in GRRPO the relative reward is all zero,

376
00:08:39,143 --> 00:08:40,811
這就是為什麼在 GRRPO 中，相對獎勵全部為零，

377
00:08:41,020 --> 00:08:41,687
since the model never gets the answers correct.

378
00:08:41,020 --> 00:08:41,687
因為模型從未答對過答案。

379
00:08:41,687 --> 00:08:41,979
since the model never gets the answers correct.

380
00:08:41,687 --> 00:08:41,979
因為模型從來沒有答對過。

381
00:08:41,979 --> 00:08:43,188
since the model never gets the answers correct.

382
00:08:41,979 --> 00:08:43,188
因為模型從來沒有答對過。

383
00:08:43,188 --> 00:08:43,898
since the model never gets the answers correct.

384
00:08:43,188 --> 00:08:43,898
因為模型從來沒有答對過。

385
00:08:43,940 --> 00:08:47,985
When you switch to a larger model like Qwen 2.5B, you'll see a meaningful training

386
00:08:43,940 --> 00:08:47,985
當你切換到更大的模型，比如 Qwen 2.5B 時，你會看到有意義的訓練效果

387
00:08:47,985 --> 00:08:51,447
loss and meaningful improvement in the GRPO training process.

388
00:08:47,985 --> 00:08:51,447
GRPO 訓練過程中的損失與顯著改進

389
00:08:51,864 --> 00:08:54,700
Now that we have finished the job here process,

390
00:08:51,864 --> 00:08:54,700
既然我們已經完成這裡的處理程序

391
00:08:54,700 --> 00:08:58,120
let's take a look at the evaluation results of the fully trained Qwen model.

392
00:08:54,700 --> 00:08:58,120
讓我們來看看完整訓練後的 Qwen 模型評估結果

393
00:08:58,287 --> 00:09:02,959
I said this fully trained Qwen as true so that we can load previous model

394
00:08:58,287 --> 00:09:02,959
我之所以稱這個完整訓練的 Qwen 為真實版本，是為了能載入先前的模型

395
00:09:02,959 --> 00:09:06,045
I trained with a larger amount of resource using GPUs.

396
00:09:02,959 --> 00:09:06,045
我使用 GPU 進行了大量資源的訓練。

397
00:09:06,420 --> 00:09:11,050
Feel free to set this as false and evaluate the HuggingFace small

398
00:09:06,420 --> 00:09:11,050
您可以自由將此設為 false 並評估 HuggingFace 的小型模型

399
00:09:11,050 --> 00:09:15,471
LLM model trained by our small GRPO trainer on a smaller dataset.

400
00:09:11,050 --> 00:09:15,471
LLM 模型由我們的小型 GRPO 訓練器在較小資料集上訓練完成

401
00:09:15,805 --> 00:09:20,685
And now we are generating the evaluation results for the fully trained Qwen model.

402
00:09:15,805 --> 00:09:20,685
現在我們正在為完整訓練的 Qwen 模型生成評估結果

403
00:09:20,935 --> 00:09:22,228
It might take some time, so we'll speed it up in the post edits.

404
00:09:20,935 --> 00:09:22,228
這可能需要一些時間，所以我們會在後續編輯中加快速度。

405
00:09:22,228 --> 00:09:22,979
It might take some time, so we'll speed it up in the post edits.

406
00:09:22,228 --> 00:09:22,979
這可能需要一些時間，所以我們會在後續編輯中加快速度。

407
00:09:22,979 --> 00:09:23,646
It might take some time, so we'll speed it up in the post edits.

408
00:09:22,979 --> 00:09:23,646
這可能需要一些時間，所以我們會在後續編輯中加快速度。

409
00:09:23,646 --> 00:09:24,313
It might take some time, so we'll speed it up in the post edits.

410
00:09:23,646 --> 00:09:24,313
這可能需要一些時間，所以我們會在後續編輯中加快速度。

411
00:09:24,313 --> 00:09:26,691
And this evaluation is now completed.

412
00:09:24,313 --> 00:09:26,691
而這項評估現在已經完成。

413
00:09:26,691 --> 00:09:29,694
Let's take a temporary loss here for the first response.

414
00:09:26,691 --> 00:09:29,694
讓我們暫時接受第一次回應的錯誤。

415
00:09:30,069 --> 00:09:33,864
The response has a boxed 20 though the ground truth is 18.

416
00:09:30,069 --> 00:09:33,864
雖然正確答案應為 18，但回應中給出的數字是框起來的 20。

417
00:09:33,864 --> 00:09:36,867
So it's a mismatch. For a second one,

418
00:09:33,864 --> 00:09:36,867
所以這是不相符的。至於第二個回應，

419
00:09:37,118 --> 00:09:39,662
the response is boxed 3 and ground truth are 3.

420
00:09:37,118 --> 00:09:39,662
回應框中的數字是 3，而實際正確答案也是 3。

421
00:09:39,662 --> 00:09:40,413
So it's a match.

422
00:09:39,662 --> 00:09:40,413
所以這是一個匹配的結果。

423
00:09:40,746 --> 00:09:43,541
And for third one still haven't finished.

424
00:09:40,746 --> 00:09:43,541
至於第三個，目前尚未完成。

425
00:09:43,541 --> 00:09:46,877
So there's no match between this one. And the fourth one,

426
00:09:43,541 --> 00:09:46,877
所以這一個沒有匹配。而第四個的話，

427
00:09:46,919 --> 00:09:48,462
the model is also able to catch and 540.

428
00:09:46,919 --> 00:09:48,462
該模型也能夠捕捉到 540。

429
00:09:48,462 --> 00:09:48,838
the model is also able to catch and 540.

430
00:09:48,462 --> 00:09:48,838
該模型也能夠捕捉到 540。

431
00:09:48,838 --> 00:09:49,171
the model is also able to catch and 540.

432
00:09:48,838 --> 00:09:49,171
該模型也能夠捕捉到 540。

433
00:09:49,171 --> 00:09:49,672
Correct.

434
00:09:49,171 --> 00:09:49,672
正確。

435
00:09:49,672 --> 00:09:53,551
And for the last one the boxed answer is 40, though the ground truth of 20.

436
00:09:49,672 --> 00:09:53,551
而最後一個框起來的答案是 40，儘管正確答案其實是 20。

437
00:09:53,926 --> 00:09:56,137
The total evaluation accuracy is 40.

438
00:09:53,926 --> 00:09:56,137
總體評估準確度為 40。

439
00:09:56,137 --> 00:09:57,763
You all have to have a fully meaningful

440
00:09:56,137 --> 00:09:57,763
你們都必須進行一個完整且有意義的

441
00:09:57,763 --> 00:10:00,349
comparison between the trained model and the previous model,

442
00:09:57,763 --> 00:10:00,349
訓練模型與先前模型之間的比較，

443
00:10:00,349 --> 00:10:01,350
please run the entire

444
00:10:00,349 --> 00:10:01,350
請執行完整的

445
00:10:01,350 --> 00:10:02,268
please run the entire

446
00:10:01,350 --> 00:10:02,268
請執行完整的

447
00:10:02,268 --> 00:10:05,730
GMS8k test instead of only others five samples.

448
00:10:02,268 --> 00:10:05,730
GMS8k 測試，而不僅僅是其他五個樣本。

449
00:10:05,730 --> 00:10:08,357
So the result here was for Qwen model

450
00:10:05,730 --> 00:10:08,357
所以這裡的結果是針對 Qwen 模型

451
00:10:08,357 --> 00:10:12,612
I previous trained using GRPO on a larger computational resource

452
00:10:08,357 --> 00:10:12,612
我之前在更大的運算資源上使用 GRPO 進行訓練

453
00:10:12,612 --> 00:10:16,115
including GPU with slightly different config parameters.

454
00:10:12,612 --> 00:10:16,115
包括 GPU 和稍微不同的配置參數

455
00:10:16,240 --> 00:10:17,742
Please feel free to change this

456
00:10:16,240 --> 00:10:17,742
歡迎隨時更改這個設定

457
00:10:17,742 --> 00:10:21,537
fully trained Qwen to false to see the results on the small model,

458
00:10:17,742 --> 00:10:21,537
將完整訓練的 Qwen 設為 false 以觀察小模型的結果,

459
00:10:21,662 --> 00:10:26,167
we did GRPO using a very small dataset to speed up the training process

460
00:10:21,662 --> 00:10:26,167
我們使用非常小的數據集進行 GRPO 以加速訓練過程

461
00:10:26,417 --> 00:10:29,795
and getting the chance to see the full GRPO trainee without waiting

462
00:10:26,417 --> 00:10:29,795
並且有機會在無需等待太久的情況下看到完整的 GRPO 訓練結果

463
00:10:29,795 --> 00:10:32,798
too long was a limited computational resources that we had here.

464
00:10:29,795 --> 00:10:32,798
這是我們在此有限的計算資源下所能做到的。

465
00:10:32,798 --> 00:10:36,761
In this lesson, we have one show the whole process of building up

466
00:10:32,798 --> 00:10:36,761
在這堂課中，我們完整展示了建立

467
00:10:36,761 --> 00:10:40,640
a mass evaluation dataset, creating a reward function, and trained

468
00:10:36,761 --> 00:10:40,640
一個大規模評估數據集、創建獎勵函數，並在現有指令模型基礎上

469
00:10:40,640 --> 00:10:45,640
GRPO on top of an existing instruct model to improve its mass capability.

470
00:10:40,640 --> 00:10:45,640
訓練 GRPO 以提升其數值能力的整個流程。

471
00:10:46,228 --> 00:10:49,315
In this lesson, we went over the entire process

472
00:10:46,228 --> 00:10:49,315
本節課程我們完整走過了整個流程

473
00:10:49,482 --> 00:10:53,110
of designing the reward model for a mass dataset, designing the evaluation

474
00:10:49,482 --> 00:10:53,110
設計獎勵模型以處理大規模數據集

475
00:10:53,110 --> 00:10:56,113
process and going through the four GRPO cycle

476
00:10:53,110 --> 00:10:56,113
設計評估流程並完成四個 GRPO 循環

477
00:10:56,238 --> 00:11:00,284
to train the Qwen model and improve its mass capability.

478
00:10:56,238 --> 00:11:00,284
訓練 Qwen 模型並提升其大規模運算能力

