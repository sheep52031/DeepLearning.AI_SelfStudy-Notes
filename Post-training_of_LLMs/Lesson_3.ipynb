{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc935f2b-a58b-4110-8918-96c868047b38",
   "metadata": {},
   "source": [
    "# L3: Supervised Fine-Tuning (SFT) ç›£ç£å¼å¾®èª¿\n",
    "\n",
    "## èª²ç¨‹æ¦‚è¿°\n",
    "\n",
    "åœ¨é€™å€‹èª²ç¨‹ä¸­ï¼Œæˆ‘å€‘å°‡å­¸ç¿’ç›£ç£å¼å¾®èª¿ï¼ˆSFTï¼‰çš„åŸºæœ¬æ¦‚å¿µå’Œå¯¦ä½œæ–¹æ³•ã€‚SFTæ˜¯ä¸€ç¨®å°‡åŸºç¤èªè¨€æ¨¡å‹è½‰æ›ç‚ºèƒ½å¤ éµå¾ªæŒ‡ä»¤çš„å°è©±æ¨¡å‹çš„é‡è¦æŠ€è¡“ã€‚\n",
    "\n",
    "### ä¸»è¦å­¸ç¿’ç›®æ¨™ï¼š\n",
    "1. **ç†è§£ SFT çš„åŸºæœ¬åŸç†**ï¼šå­¸ç¿’å¦‚ä½•é€éæ¨¡ä»¿ç¯„ä¾‹å›æ‡‰ä¾†è¨“ç·´æ¨¡å‹\n",
    "2. **æŒæ¡ SFT çš„å·¥ä½œæµç¨‹**ï¼šå¾è³‡æ–™æº–å‚™åˆ°æ¨¡å‹è¨“ç·´çš„å®Œæ•´éç¨‹\n",
    "3. **å¯¦ä½œ SFT è¨“ç·´**ï¼šä½¿ç”¨çœŸå¯¦è³‡æ–™é›†é€²è¡Œæ¨¡å‹å¾®èª¿\n",
    "4. **æ¯”è¼ƒè¨“ç·´å‰å¾Œçš„æ•ˆæœ**ï¼šè§€å¯Ÿæ¨¡å‹åœ¨å¾®èª¿å‰å¾Œçš„å·®ç•°\n",
    "\n",
    "### èª²ç¨‹é‡é»ï¼š\n",
    "- **SFT çš„æ•¸å­¸åŸç†**ï¼šè² å°æ•¸ä¼¼ç„¶æå¤±å‡½æ•¸çš„æœ€å°åŒ–\n",
    "- **è³‡æ–™å“è³ªçš„é‡è¦æ€§**ï¼šé«˜å“è³ªè³‡æ–™æ¯”å¤§é‡è³‡æ–™æ›´é‡è¦\n",
    "- **åƒæ•¸æ•ˆç‡å¾®èª¿**ï¼šLoRA ç­‰æŠ€è¡“çš„æ‡‰ç”¨\n",
    "- **å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹**ï¼šå¾åŸºç¤æ¨¡å‹åˆ°æŒ‡ä»¤æ¨¡å‹çš„è½‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7248ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/sheep52031/DeepLearning.AI_SelfStudy-Notes.git\n",
    "# !ls\n",
    "# %cd DeepLearning.AI_SelfStudy-Notes\n",
    "\n",
    "import os\n",
    "base_dir = 'DeepLearning.AI_SelfStudy-Notes/Post-training_of_LLMs'\n",
    "\n",
    "# !pip install -r Post-training_of_LLMs/requirements.txt --no-deps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfc986-9ac7-4a2d-9dd0-a76841c7f46d",
   "metadata": {},
   "source": [
    "## åŒ¯å…¥å¿…è¦çš„å‡½å¼åº«\n",
    "\n",
    "é€™å€‹éƒ¨åˆ†æˆ‘å€‘å°‡åŒ¯å…¥é€²è¡Œ SFT è¨“ç·´æ‰€éœ€çš„æ ¸å¿ƒå‡½å¼åº«ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3304e49d-bd1e-469b-a5b4-5edb16ecf344",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "import torch  # PyTorch æ·±åº¦å­¸ç¿’æ¡†æ¶\n",
    "import pandas as pd  # è³‡æ–™è™•ç†å’Œåˆ†æ\n",
    "from datasets import load_dataset, Dataset  # HuggingFace è³‡æ–™é›†è¼‰å…¥\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM  # æ¨¡å‹å’Œåˆ†è©å™¨\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig  # SFT è¨“ç·´å·¥å…·"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc63b02-5e9a-4a83-b042-4a2386cf5976",
   "metadata": {},
   "source": [
    "## è¨­å®šè¼”åŠ©å‡½å¼\n",
    "\n",
    "é€™äº›è¼”åŠ©å‡½å¼å°‡å¹«åŠ©æˆ‘å€‘é€²è¡Œæ¨¡å‹è¼‰å…¥ã€å›æ‡‰ç”Ÿæˆå’Œæ¸¬è©¦ç­‰æ“ä½œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69982ae0-755e-48cf-ba4c-3b83b091fd9a",
   "metadata": {
    "height": 557
   },
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, user_message, system_message=None, \n",
    "                       max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæ¨¡å‹å›æ‡‰çš„å‡½å¼ï¼ˆä¿®å¾©é‡è¤‡å•é¡Œç‰ˆæœ¬ï¼‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    - model: èªè¨€æ¨¡å‹\n",
    "    - tokenizer: åˆ†è©å™¨\n",
    "    - user_message: ä½¿ç”¨è€…è¨Šæ¯\n",
    "    - system_message: ç³»çµ±è¨Šæ¯ï¼ˆå¯é¸ï¼‰\n",
    "    - max_new_tokens: ç”Ÿæˆçš„æœ€å¤§æ–°è©å…ƒæ•¸é‡\n",
    "    \n",
    "    è¿”å›:\n",
    "    - response: æ¨¡å‹ç”Ÿæˆçš„å›æ‡‰\n",
    "    \"\"\"\n",
    "    # ä½¿ç”¨åˆ†è©å™¨çš„èŠå¤©æ¨¡æ¿æ ¼å¼åŒ–å°è©±\n",
    "    messages = []\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    \n",
    "    # æˆ‘å€‘å‡è¨­è³‡æ–™éƒ½æ˜¯å–®è¼ªå°è©±\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "    # æ‡‰ç”¨èŠå¤©æ¨¡æ¿\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "\n",
    "    # å°‡æç¤ºè½‰æ›ç‚ºæ¨¡å‹è¼¸å…¥æ ¼å¼\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # ç”Ÿæˆå›æ‡‰ï¼ˆæ·»åŠ é‡è¤‡æ‡²ç½°å’Œæº«åº¦æ§åˆ¶ï¼‰\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,  # æ”¹ç‚ºTrueä»¥å¢åŠ å¤šæ¨£æ€§\n",
    "            temperature=0.7,  # æ·»åŠ æº«åº¦æ§åˆ¶\n",
    "            top_p=0.9,  # æ·»åŠ top-pæ¡æ¨£\n",
    "            repetition_penalty=1.2,  # é—œéµï¼šæ·»åŠ é‡è¤‡æ‡²ç½°\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # æå–ç”Ÿæˆçš„éƒ¨åˆ†\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    generated_ids = outputs[0][input_len:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234e5b05-a493-4683-91fd-7417885efc0f",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "def test_model_with_questions(model, tokenizer, questions, \n",
    "                              system_message=None, title=\"Model Output\"):\n",
    "    \"\"\"\n",
    "    æ¸¬è©¦æ¨¡å‹å°ä¸€ç³»åˆ—å•é¡Œçš„å›æ‡‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    - model: èªè¨€æ¨¡å‹\n",
    "    - tokenizer: åˆ†è©å™¨\n",
    "    - questions: å•é¡Œåˆ—è¡¨\n",
    "    - system_message: ç³»çµ±è¨Šæ¯ï¼ˆå¯é¸ï¼‰\n",
    "    - title: è¼¸å‡ºæ¨™é¡Œ\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        response = generate_responses(model, tokenizer, question, \n",
    "                                      system_message)\n",
    "        print(f\"\\nModel Input {i}:\\n{question}\\nModel Output {i}:\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c273931-6827-4ee1-af1a-83a99bf94bf7",
   "metadata": {
    "height": 387
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, use_gpu = False):\n",
    "    \"\"\"\n",
    "    è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨ï¼ˆä¿®å¾©ç‰ˆï¼‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    - model_name: æ¨¡å‹åç¨±æˆ–è·¯å¾‘\n",
    "    - use_gpu: æ˜¯å¦ä½¿ç”¨ GPU\n",
    "    \n",
    "    è¿”å›:\n",
    "    - model: è¼‰å…¥çš„æ¨¡å‹\n",
    "    - tokenizer: è¼‰å…¥çš„åˆ†è©å™¨\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    import torch\n",
    "    \n",
    "    # æ¸…ç† GPU è¨˜æ†¶é«”\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # è¼‰å…¥åˆ†è©å™¨\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # è¼‰å…¥æ¨¡å‹\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        try:\n",
    "            model.to(\"cuda\")\n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            print(f\"âš ï¸  GPU è¨˜æ†¶é«”ä¸è¶³ï¼Œå°‡ä½¿ç”¨ CPU: {e}\")\n",
    "            model.to(\"cpu\")\n",
    "            use_gpu = False\n",
    "    \n",
    "    # ä¿®å¾©åˆ†è©å™¨é…ç½® - é€™æ˜¯é—œéµä¿®å¾©\n",
    "    if not tokenizer.chat_template:\n",
    "        tokenizer.chat_template = \"\"\"{% for message in messages %}{% if message['role'] == 'system' %}System: {{ message['content'] }}\\n{% elif message['role'] == 'user' %}User: {{ message['content'] }}\\nAssistant:{% elif message['role'] == 'assistant' %} {{ message['content'] }}{% if not loop.last %}\\n{% endif %}{% endif %}{% endfor %}{% if add_generation_prompt %} {% endif %}\"\"\"\n",
    "    \n",
    "    # é‡è¦ï¼šæ­£ç¢ºè¨­ç½® tokenizer çš„ç‰¹æ®Š token\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # ç¢ºä¿ tokenizer æœ‰æ­£ç¢ºçš„ pad_token_id\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    print(f\"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œä½¿ç”¨è¨­å‚™: {'GPU' if use_gpu else 'CPU'}\")\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        print(f\"GPU è¨˜æ†¶é«”ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f}GB\")\n",
    "        \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd15e1d-6ecd-4337-a5dd-1602da354f62",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "def display_dataset(dataset):\n",
    "    \"\"\"\n",
    "    é¡¯ç¤ºè³‡æ–™é›†çš„å‰å¹¾å€‹ç¯„ä¾‹\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    - dataset: è¦é¡¯ç¤ºçš„è³‡æ–™é›†\n",
    "    \"\"\"\n",
    "    # è¦–è¦ºåŒ–è³‡æ–™é›†\n",
    "    rows = []\n",
    "    for i in range(3):\n",
    "        example = dataset[i]\n",
    "        # æå–ä½¿ç”¨è€…è¨Šæ¯\n",
    "        user_msg = next(m['content'] for m in example['messages']\n",
    "                        if m['role'] == 'user')\n",
    "        # æå–åŠ©æ‰‹å›æ‡‰\n",
    "        assistant_msg = next(m['content'] for m in example['messages']\n",
    "                             if m['role'] == 'assistant')\n",
    "        rows.append({\n",
    "            'User Prompt': user_msg,\n",
    "            'Assistant Response': assistant_msg\n",
    "        })\n",
    "    \n",
    "    # é¡¯ç¤ºç‚ºè¡¨æ ¼\n",
    "    df = pd.DataFrame(rows)\n",
    "    pd.set_option('display.max_colwidth', None)  # é¿å…æˆªæ–·é•·å­—ä¸²\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ac817-43a4-43c9-88f3-9825b96b84b7",
   "metadata": {},
   "source": [
    "## è¼‰å…¥åŸºç¤æ¨¡å‹ä¸¦æ¸¬è©¦ç°¡å–®å•é¡Œ\n",
    "\n",
    "é¦–å…ˆæˆ‘å€‘è¼‰å…¥ä¸€å€‹åŸºç¤æ¨¡å‹ï¼ˆæœªç¶“éæŒ‡ä»¤å¾®èª¿çš„æ¨¡å‹ï¼‰ï¼Œä¸¦æ¸¬è©¦å®ƒå°ç°¡å–®å•é¡Œçš„å›æ‡‰èƒ½åŠ›ã€‚é€™å°‡å¹«åŠ©æˆ‘å€‘ç†è§£ SFT å‰å¾Œçš„å·®ç•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fed78c2-ea93-4ac2-bd6f-5d4391de7c8d",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# è¨­å®šæ˜¯å¦ä½¿ç”¨ GPUï¼ˆåœ¨ Kaggle ç’°å¢ƒä¸­è¨­ç‚º Trueï¼‰\n",
    "USE_GPU = True\n",
    "\n",
    "# å®šç¾©æ¸¬è©¦å•é¡Œ\n",
    "questions = [\n",
    "    \"Give me an 1-sentence introduction of LLM.\",  # è¦æ±‚ç°¡çŸ­ä»‹ç´¹ LLM\n",
    "    \"Calculate 1+1-1\",  # ç°¡å–®æ•¸å­¸è¨ˆç®—\n",
    "    \"What's the difference between thread and process?\"  # æŠ€è¡“æ¦‚å¿µè§£é‡‹\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¾ HuggingFace Hub ä¸‹è¼‰ Qwen3-0.6B-Base æ¨¡å‹\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# ä¸‹è¼‰æ¨¡å‹åˆ°æœ¬åœ°ç›®éŒ„\n",
    "snapshot_download(\n",
    "    repo_id=\"Qwen/Qwen3-0.6B-Base\", # æŒ‡å®šè¦ä¸‹è¼‰çš„æ¨¡å‹ï¼ˆQwen3 0.6B åŸºç¤ç‰ˆæœ¬ï¼‰\n",
    "    local_dir=\"./Qwen3-0.6B-Base\", # è¨­å®šæœ¬åœ°å„²å­˜è·¯å¾‘\n",
    "    local_dir_use_symlinks=False  # é—œé–‰ç¬¦è™Ÿé€£çµï¼Œç¢ºä¿æª”æ¡ˆå®Œæ•´è¤‡è£½\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086a902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ Transformers åº«ä¸­çš„è‡ªå‹•åˆ†è©å™¨å’Œå› æœèªè¨€æ¨¡å‹\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# è¨­å®šæ¨¡å‹ç›®éŒ„è·¯å¾‘\n",
    "local_dir = \"Qwen3-0.6B-Base\"\n",
    "\n",
    "# è¼‰å…¥åˆ†è©å™¨ï¼ˆè² è²¬å°‡æ–‡å­—è½‰æ›ç‚ºæ¨¡å‹å¯ç†è§£çš„æ•¸å­—åºåˆ—ï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir, trust_remote_code=True) # ä¿¡ä»»é ç«¯ç¨‹å¼ç¢¼ï¼Œå…è¨±åŸ·è¡Œæ¨¡å‹è‡ªå®šç¾©çš„ç¨‹å¼ç¢¼\n",
    "\n",
    "# è¼‰å…¥èªè¨€æ¨¡å‹ï¼ˆè² è²¬ç”Ÿæˆæ–‡å­—å›æ‡‰ï¼‰\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir, trust_remote_code=True) # ä¿¡ä»»é ç«¯ç¨‹å¼ç¢¼ï¼Œç¢ºä¿æ¨¡å‹èƒ½æ­£å¸¸è¼‰å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba426c74-4d93-42b3-b2c7-5791fb9bf3c5",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (Before SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "gÅ‚Ä™bine\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "larÄ±nÄ±zÄ±  âš‡â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…â‹…\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "âš‡â‹…â‹„â‹¯â‹…â‹€Â·â‹…â‹ƒ\n",
      " âˆâ‹…â‹â€¦â‹…â‹‚Â·Â·â‹†\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_model_with_questions(model, tokenizer, questions, \n",
    "                          title=\"Base Model (Before SFT) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885253e0-3a9a-4b42-a36b-a8a3ddd340a1",
   "metadata": {},
   "source": [
    "## Qwen3-0.6B æ¨¡å‹çš„ SFT çµæœ\n",
    "\n",
    "åœ¨é€™å€‹éƒ¨åˆ†ï¼Œæˆ‘å€‘å°‡æª¢è¦–å…ˆå‰å®Œæˆçš„ SFT è¨“ç·´çµæœã€‚ç”±æ–¼è³‡æºé™åˆ¶ï¼Œæˆ‘å€‘ä¸æœƒåœ¨åƒ Qwen3-0.6B é€™æ¨£ç›¸å°è¼ƒå¤§çš„æ¨¡å‹ä¸Šé€²è¡Œå®Œæ•´è¨“ç·´ã€‚\n",
    "\n",
    "### å°æ¯”åˆ†æï¼š\n",
    "- **åŸºç¤æ¨¡å‹ï¼ˆSFTå‰ï¼‰**ï¼šåªæœƒç”Ÿæˆéš¨æ©Ÿç¬¦è™Ÿï¼Œç„¡æ³•ç†è§£æŒ‡ä»¤\n",
    "- **å¾®èª¿æ¨¡å‹ï¼ˆSFTå¾Œï¼‰**ï¼šèƒ½å¤ ç†è§£ä¸¦å›æ‡‰ä½¿ç”¨è€…çš„å•é¡Œ\n",
    "\n",
    "é€™å€‹å°æ¯”æ¸…æ¥šåœ°å±•ç¤ºäº† SFT çš„å¨åŠ› - å®ƒèƒ½å°‡ä¸€å€‹åªæœƒé æ¸¬ä¸‹ä¸€å€‹è©çš„åŸºç¤æ¨¡å‹ï¼Œè½‰æ›ç‚ºèƒ½å¤ é€²è¡Œæœ‰æ„ç¾©å°è©±çš„åŠ©æ‰‹æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f6f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"banghua/Qwen3-0.6B-SFT\",\n",
    "    local_dir=\"./Qwen3-0.6B-SFT\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e86f13c-c969-4c7e-8702-d074ee7a2ce6",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œä½¿ç”¨è¨­å‚™: GPU\n",
      "GPU è¨˜æ†¶é«”ä½¿ç”¨: 2.22GB / 9.77GB\n",
      "\n",
      "=== Base Model (After SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "A study in artificial intelligence and natural language processing, focusing on the development of advanced language models that can enhance human communication through more efficient and accurate text representation.\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "The result is 2. Here's the step-by-step calculation:\n",
      "\n",
      "1 + 1 - 1 = 3 - 1 = 2\n",
      "\n",
      "So, 1 + 1 - 1 equals 2.\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "In computer science, a thread is an instance of a program that executes one instruction at a time. Each thread has its own memory space, which means it cannot access variables or data from other threads unless they are shared.\n",
      "\n",
      "On the other hand, a process is the fundamental unit for execution in operating systems. It consists of multiple threads working together to accomplish tasks such as running a specific program, executing code, creating resources (such as file handles), etc.\n",
      "\n",
      "Processes have their own memory space, whereas threads\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\"./Qwen3-0.6B-SFT\", USE_GPU)\n",
    "\n",
    "test_model_with_questions(model, tokenizer, questions, \n",
    "                          title=\"Base Model (After SFT) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf299ee-aa84-4c43-8d7b-f0998077e2cb",
   "metadata": {},
   "source": [
    "## åœ¨å°å‹æ¨¡å‹ä¸Šé€²è¡Œ SFT è¨“ç·´\n",
    "\n",
    "æ¥ä¸‹ä¾†æˆ‘å€‘å°‡å¯¦éš›é€²è¡Œ SFT è¨“ç·´çš„å®Œæ•´æµç¨‹ã€‚æˆ‘å€‘å°‡ä½¿ç”¨ä¸€å€‹è¼ƒå°çš„æ¨¡å‹å’Œè³‡æ–™é›†ä¾†ç¢ºä¿è¨“ç·´éç¨‹èƒ½åœ¨æœ‰é™çš„è¨ˆç®—è³‡æºä¸ŠåŸ·è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5cb7ea-e157-418e-84f5-34ecbed823ad",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ğŸ’» &nbsp; <b>æ³¨æ„ï¼š</b> æˆ‘å€‘åœ¨å°å‹æ¨¡å‹ <code>HuggingFaceTB/SmolLM2-135M</code> å’Œè¼ƒå°çš„è¨“ç·´è³‡æ–™é›†ä¸Šé€²è¡Œ SFTï¼Œä»¥ç¢ºä¿å®Œæ•´çš„è¨“ç·´éç¨‹èƒ½åœ¨æœ‰é™çš„è¨ˆç®—è³‡æºä¸Šé‹è¡Œã€‚å¦‚æœä½ åœ¨è‡ªå·±çš„æ©Ÿå™¨ä¸Šé‹è¡Œç­†è¨˜æœ¬ä¸¦ä¸”æœ‰ GPU è³‡æºï¼Œå¯ä»¥åˆ‡æ›åˆ°æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚ <code>Qwen/Qwen3-0.6B-Base</code>ï¼‰ä¾†é€²è¡Œå®Œæ•´çš„ SFT è¨“ç·´ä¸¦é‡ç¾ä¸Šè¿°çµæœã€‚</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdb07589-049d-432e-8001-e6e9175ad806",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5324d14f054d5eb8a3d02ced0830d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œä½¿ç”¨è¨­å‚™: CPU\n"
     ]
    }
   ],
   "source": [
    "# ä¸‹è¼‰ HuggingFaceTB/SmolLM2-135M æ¨¡å‹\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# ä¸‹è¼‰å°æ¨¡å‹åˆ°æœ¬åœ°ç›®éŒ„\n",
    "snapshot_download(\n",
    "    repo_id=\"HuggingFaceTB/SmolLM2-135M\",\n",
    "    local_dir=\"./models/HuggingFaceTB/SmolLM2-135M\",\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨å°æ¨¡å‹ä¸¦è¨­å®šç‚ºCPUæ¨¡å¼ä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ\n",
    "model_name = \"./models/HuggingFaceTB/SmolLM2-135M\"\n",
    "USE_GPU = False  # æ”¹ç‚ºFalseä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, USE_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20d52c3e-9c6c-43c3-bd95-92d60b9c9a8f",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨“ç·´è³‡æ–™é›†ç¯„ä¾‹ï¼š\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Prompt</th>\n",
       "      <th>Assistant Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- The left child should have a value less than the parent node's value, and the right child should have a value greater than the parent node's value.</td>\n",
       "      <td>This statement is correct. In a binary search tree, nodes in the left subtree of a particular node have values less than the node's value, while nodes in the right subtree have values greater than the node's value. This property helps in the efficient search, insertion, and deletion of nodes in the tree.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To pass three levels must be the plan.\\nThen tackle Two, when that is done.\\nOf 100 that start, at the end will be 20.\\nFinQuiz is a website that helps you prepare.\\nUse it to be stress-free, and not lose your hair.\\nThen, take the exam with a smile on your face.\\nBe confident that you will gain your place.\\nSo make this the goal to which you aspire. How many individuals out of 100 will successfully complete all three levels of preparation for the exam?</td>\n",
       "      <td>Based on the given information, out of 100 individuals who start, only 20 will make it to the end. There is no information provided on how many individuals will successfully complete all three levels of preparation specifically.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you translate the text material into Spanish or any other language?: He really is, you know.\\nThings a hero should show.\\nHe loves me more than a zillion things.\\nHe loves me when I sing my jolly folktale rhymes.\\nHe's good, not just good, in fact he's great!\\nBut because he's my best mate!\\nWOW !!! I love it!!!!</td>\n",
       "      <td>Â¿Puede traducir el texto a espaÃ±ol o a cualquier otro idioma?: \\nRealmente lo es, ya sabes.\\nCosas que un hÃ©roe deberÃ­a demostrar.\\nMe quiere mÃ¡s que un millÃ³n de cosas.\\nMe quiere cuando canto mis alegres rimas de cuentos populares.\\nEs bueno, no solo bueno, Â¡de hecho es genial!\\nÂ¡Pero porque es mi mejor amigo!\\nÂ¡WOW! Â¡Me encanta!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                 User Prompt  \\\n",
       "0                                                                                                                                                                                                                                                                                                                      - The left child should have a value less than the parent node's value, and the right child should have a value greater than the parent node's value.   \n",
       "1  To pass three levels must be the plan.\\nThen tackle Two, when that is done.\\nOf 100 that start, at the end will be 20.\\nFinQuiz is a website that helps you prepare.\\nUse it to be stress-free, and not lose your hair.\\nThen, take the exam with a smile on your face.\\nBe confident that you will gain your place.\\nSo make this the goal to which you aspire. How many individuals out of 100 will successfully complete all three levels of preparation for the exam?   \n",
       "2                                                                                                                                             Can you translate the text material into Spanish or any other language?: He really is, you know.\\nThings a hero should show.\\nHe loves me more than a zillion things.\\nHe loves me when I sing my jolly folktale rhymes.\\nHe's good, not just good, in fact he's great!\\nBut because he's my best mate!\\nWOW !!! I love it!!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                              Assistant Response  \n",
       "0                              This statement is correct. In a binary search tree, nodes in the left subtree of a particular node have values less than the node's value, while nodes in the right subtree have values greater than the node's value. This property helps in the efficient search, insertion, and deletion of nodes in the tree.  \n",
       "1                                                                                                           Based on the given information, out of 100 individuals who start, only 20 will make it to the end. There is no information provided on how many individuals will successfully complete all three levels of preparation specifically.  \n",
       "2  Â¿Puede traducir el texto a espaÃ±ol o a cualquier otro idioma?: \\nRealmente lo es, ya sabes.\\nCosas que un hÃ©roe deberÃ­a demostrar.\\nMe quiere mÃ¡s que un millÃ³n de cosas.\\nMe quiere cuando canto mis alegres rimas de cuentos populares.\\nEs bueno, no solo bueno, Â¡de hecho es genial!\\nÂ¡Pero porque es mi mejor amigo!\\nÂ¡WOW! Â¡Me encanta!  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è³‡æ–™é›†å¤§å°ï¼š2000 å€‹æ¨£æœ¬\n",
      "\n",
      "è³‡æ–™é›†ç‰¹é»ï¼š\n",
      "- åŒ…å«å¤šæ¨£åŒ–çš„æŒ‡ä»¤å’Œå›æ‡‰å°\n",
      "- æ¶µè“‹å•ç­”ã€ç¿»è­¯ã€è¨ˆç®—ç­‰å¤šç¨®ä»»å‹™\n",
      "- æ¯å€‹æ¨£æœ¬éƒ½åŒ…å«ä½¿ç”¨è€…æç¤ºå’ŒåŠ©æ‰‹å›æ‡‰\n",
      "- å¢åŠ åˆ°500å€‹æ¨£æœ¬ä»¥æ”¹å–„å°æ¨¡å‹è¨“ç·´æ•ˆæœ\n",
      "- å°æ¨¡å‹éœ€è¦æ›´å¤šæ¨£æœ¬æ‰èƒ½å­¸åˆ°è‰¯å¥½çš„å°è©±æ¨¡å¼\n"
     ]
    }
   ],
   "source": [
    "# è¼‰å…¥è¨“ç·´è³‡æ–™é›†\n",
    "train_dataset = load_dataset(\"banghua/DL-SFT-Dataset\")[\"train\"]\n",
    "\n",
    "# å¢åŠ è³‡æ–™é›†å¤§å°ä»¥æ”¹å–„å°æ¨¡å‹çš„è¨“ç·´æ•ˆæœï¼ˆå¾100å¢åŠ åˆ°500ï¼‰\n",
    "train_dataset = train_dataset.select(range(2000))\n",
    "\n",
    "# é¡¯ç¤ºè³‡æ–™é›†çš„å‰å¹¾å€‹ç¯„ä¾‹\n",
    "print(\"è¨“ç·´è³‡æ–™é›†ç¯„ä¾‹ï¼š\")\n",
    "display_dataset(train_dataset)\n",
    "\n",
    "print(f\"\\nè³‡æ–™é›†å¤§å°ï¼š{len(train_dataset)} å€‹æ¨£æœ¬\")\n",
    "print(\"\\nè³‡æ–™é›†ç‰¹é»ï¼š\")\n",
    "print(\"- åŒ…å«å¤šæ¨£åŒ–çš„æŒ‡ä»¤å’Œå›æ‡‰å°\")\n",
    "print(\"- æ¶µè“‹å•ç­”ã€ç¿»è­¯ã€è¨ˆç®—ç­‰å¤šç¨®ä»»å‹™\") \n",
    "print(\"- æ¯å€‹æ¨£æœ¬éƒ½åŒ…å«ä½¿ç”¨è€…æç¤ºå’ŒåŠ©æ‰‹å›æ‡‰\")\n",
    "print(\"- å¢åŠ åˆ°500å€‹æ¨£æœ¬ä»¥æ”¹å–„å°æ¨¡å‹è¨“ç·´æ•ˆæœ\")\n",
    "print(\"- å°æ¨¡å‹éœ€è¦æ›´å¤šæ¨£æœ¬æ‰èƒ½å­¸åˆ°è‰¯å¥½çš„å°è©±æ¨¡å¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c515a8-3728-45fa-88cc-6eb4de839839",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT è¨“ç·´é…ç½®èªªæ˜ï¼ˆé‡å°å°æ¨¡å‹å„ªåŒ–ï¼Œä¿®å¾©é‡è¤‡å•é¡Œï¼‰ï¼š\n",
      "å­¸ç¿’ç‡: 0.0001 - æé«˜ä»¥å¹«åŠ©å°æ¨¡å‹æ›´å¿«å­¸ç¿’\n",
      "è¨“ç·´è¼ªæ•¸: 3 - å¢åŠ åˆ°3è¼ªï¼Œå°æ¨¡å‹éœ€è¦æ›´å¤šè¨“ç·´\n",
      "æ‰¹æ¬¡å¤§å°: 2 - ç¨å¾®å¢åŠ \n",
      "æ¢¯åº¦ç´¯ç©: 4 - æ¸›å°‘ä»¥åŠ å¿«æ›´æ–°\n",
      "æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: 8\n",
      "æœ€å¤§åºåˆ—é•·åº¦: 256 - æ¸›å°‘ä»¥é©æ‡‰å°æ¨¡å‹\n",
      "è³‡æ–™é›†: å¢åŠ åˆ°500å€‹æ¨£æœ¬\n",
      "ç”Ÿæˆ: æ·»åŠ repetition_penalty=1.2å’Œtemperature=0.7æ¸›å°‘é‡è¤‡\n",
      "æ­£è¦åŒ–: æ·»åŠ weight_decayå’Œwarmup_stepsé˜²æ­¢éæ“¬åˆ\n"
     ]
    }
   ],
   "source": [
    "# SFT è¨“ç·´å™¨é…ç½®ï¼ˆé‡å°å°æ¨¡å‹å„ªåŒ–ï¼Œä¿®å¾©é‡è¤‡å•é¡Œï¼‰\n",
    "sft_config = SFTConfig(\n",
    "    learning_rate=1e-4,  # æé«˜å­¸ç¿’ç‡å¹«åŠ©å°æ¨¡å‹æ›´å¿«å­¸ç¿’\n",
    "    num_train_epochs=3,  # å¢åŠ è¨“ç·´è¼ªæ•¸ï¼Œå°æ¨¡å‹éœ€è¦æ›´å¤šè¨“ç·´\n",
    "    per_device_train_batch_size=2,  # ç¨å¾®å¢åŠ æ‰¹æ¬¡å¤§å°\n",
    "    gradient_accumulation_steps=4,  # æ¸›å°‘æ¢¯åº¦ç´¯ç©æ­¥é©Ÿ\n",
    "    gradient_checkpointing=False,  # å°æ¨¡å‹ä¸éœ€è¦æ¢¯åº¦æª¢æŸ¥é»\n",
    "    logging_steps=10,  # å¢åŠ è¨˜éŒ„é–“éš”\n",
    "    \n",
    "    # é—œéµä¿®å¾©ï¼šæ·»åŠ ä»¥ä¸‹åƒæ•¸ä¾†é¿å…é‡è¤‡å’Œæ”¹å–„è¨“ç·´\n",
    "    max_seq_length=256,  # æ¸›å°‘åºåˆ—é•·åº¦ä»¥é©æ‡‰å°æ¨¡å‹\n",
    "    dataset_text_field=\"messages\",  # æŒ‡å®šè³‡æ–™é›†çš„æ–‡å­—æ¬„ä½\n",
    "    packing=False,  # é—œé–‰åºåˆ—æ‰“åŒ…ï¼Œç¢ºä¿è¨“ç·´ç©©å®š\n",
    "    remove_unused_columns=False,  # ä¿ç•™æ‰€æœ‰æ¬„ä½\n",
    "    \n",
    "    # æ·»åŠ æ­£è¦åŒ–ä»¥æ¸›å°‘éæ“¬åˆ\n",
    "    weight_decay=0.01,  # æ¬Šé‡è¡°æ¸›\n",
    "    warmup_steps=20,  # é ç†±æ­¥é©Ÿ\n",
    "    \n",
    "    # è©•ä¼°é…ç½®\n",
    "    save_steps=100,  # ä¿å­˜æª¢æŸ¥é»é »ç‡\n",
    "    eval_steps=50,  # è©•ä¼°é »ç‡ï¼ˆå¦‚æœæœ‰é©—è­‰é›†ï¼‰\n",
    ")\n",
    "\n",
    "# é—œéµè¶…åƒæ•¸è§£é‡‹ï¼š\n",
    "print(\"SFT è¨“ç·´é…ç½®èªªæ˜ï¼ˆé‡å°å°æ¨¡å‹å„ªåŒ–ï¼Œä¿®å¾©é‡è¤‡å•é¡Œï¼‰ï¼š\")\n",
    "print(f\"å­¸ç¿’ç‡: {sft_config.learning_rate} - æé«˜ä»¥å¹«åŠ©å°æ¨¡å‹æ›´å¿«å­¸ç¿’\")\n",
    "print(f\"è¨“ç·´è¼ªæ•¸: {sft_config.num_train_epochs} - å¢åŠ åˆ°3è¼ªï¼Œå°æ¨¡å‹éœ€è¦æ›´å¤šè¨“ç·´\")\n",
    "print(f\"æ‰¹æ¬¡å¤§å°: {sft_config.per_device_train_batch_size} - ç¨å¾®å¢åŠ \")\n",
    "print(f\"æ¢¯åº¦ç´¯ç©: {sft_config.gradient_accumulation_steps} - æ¸›å°‘ä»¥åŠ å¿«æ›´æ–°\")\n",
    "print(f\"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps}\")\n",
    "print(f\"æœ€å¤§åºåˆ—é•·åº¦: {sft_config.max_seq_length} - æ¸›å°‘ä»¥é©æ‡‰å°æ¨¡å‹\")\n",
    "print(\"ç”Ÿæˆ: æ·»åŠ repetition_penalty=1.2å’Œtemperature=0.7æ¸›å°‘é‡è¤‡\")\n",
    "print(\"æ­£è¦åŒ–: æ·»åŠ weight_decayå’Œwarmup_stepsé˜²æ­¢éæ“¬åˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd7d132-4c69-4b12-a8ec-e6b4795faad9",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f24e27cdcde438386502eb260a6dab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1134d5fa444df6b27994b36a1102a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='544' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [544/750 02:42 < 01:01, 3.34 it/s, Epoch 2.17/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.301500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.124300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.082300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.947700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.214100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.141200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.143100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.999700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.122100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.858600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.441800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.549100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.681700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.679600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.607600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.501100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.646500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.635300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.561900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.552400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.536800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.655900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.624200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.603900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.548600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.562300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.680300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.522700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.606900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.243400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.443500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.328500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m sft_trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39msft_config,\n\u001b[1;32m      4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, \n\u001b[1;32m      5\u001b[0m     processing_class\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m \u001b[43msft_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/post-training-llms/lib/python3.10/site-packages/transformers/trainer.py:2206\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/post-training-llms/lib/python3.10/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/post-training-llms/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:872\u001b[0m, in \u001b[0;36mSFTTrainer.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_activation_offload_context:\n\u001b[0;32m--> 872\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/post-training-llms/lib/python3.10/site-packages/transformers/trainer.py:3797\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3795\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3797\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniconda3/envs/post-training-llms/lib/python3.10/site-packages/accelerate/accelerator.py:2553\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2553\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/post-training-llms/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/post-training-llms/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/post-training-llms/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset, \n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4bac1-7262-4c55-b411-6a59188157b0",
   "metadata": {},
   "source": [
    "## æ¸¬è©¦å°å‹æ¨¡å‹å’Œå°è³‡æ–™é›†çš„è¨“ç·´çµæœ\n",
    "\n",
    "**æ³¨æ„ï¼š** ä»¥ä¸‹çµæœæ˜¯é‡å°æˆ‘å€‘åœ¨ SFT è¨“ç·´ä¸­ä½¿ç”¨çš„å°å‹æ¨¡å‹å’Œè³‡æ–™é›†ï¼Œé€™æ˜¯ç”±æ–¼è¨ˆç®—è³‡æºæœ‰é™ã€‚è‹¥è¦æŸ¥çœ‹å¤§å‹æ¨¡å‹çš„å®Œæ•´è¨“ç·´çµæœï¼Œè«‹åƒé–±ä¸Šæ–¹çš„ **ã€ŒQwen3-0.6B æ¨¡å‹çš„ SFT çµæœã€** éƒ¨åˆ†ã€‚\n",
    "\n",
    "### é æœŸçµæœåˆ†æï¼š\n",
    "ç”±æ–¼æˆ‘å€‘ä½¿ç”¨çš„æ˜¯å°å‹æ¨¡å‹ï¼ˆç›¸å°è¼ƒå°‘çš„åƒæ•¸ï¼‰å’Œæœ‰é™çš„è¨“ç·´è³‡æ–™ï¼Œæ¨¡å‹çš„è¡¨ç¾å¯èƒ½æœƒæœ‰ä»¥ä¸‹ç‰¹é»ï¼š\n",
    "- **æœ‰ä¸€å®šçš„æ”¹å–„**ï¼šç›¸æ¯”æœªè¨“ç·´çš„æ¨¡å‹ï¼Œæ‡‰è©²èƒ½çœ‹åˆ°æ˜é¡¯çš„æ”¹å–„\n",
    "- **ä»æœ‰é™åˆ¶**ï¼šç”±æ–¼æ¨¡å‹è¦æ¨¡å’Œè³‡æ–™é‡çš„é™åˆ¶ï¼Œå¯èƒ½ç„¡æ³•é”åˆ°å®Œç¾çš„è¡¨ç¾\n",
    "- **å­¸ç¿’èƒ½åŠ›å±•ç¾**ï¼šå¯ä»¥è§€å¯Ÿåˆ°æ¨¡å‹é–‹å§‹å­¸æœƒå›æ‡‰æŒ‡ä»¤çš„åŸºæœ¬èƒ½åŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d678274-5768-4cea-ae20-051488e5d0f3",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (After SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "1. I am a Licensed Marriage and Family Therapist (LFTT).\n",
      "\n",
      "2. My background in counseling, specifically marriage and family therapy has made it easy for me to provide compassionate and effective solutions tailored to individual needs.\n",
      "\n",
      "3. As a result, my work is highly sought after by both mental health professionals and individuals seeking help with various issues related to relationships and emotional wellbeing.\n",
      "\n",
      "4. Through experience as a licensed professional counselor, I have gained valuable insights into the unique challenges faced when\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "3. Subtract the result from step two (step four) to get your answer in decimal form, which is 0.52846...\n",
      "Note that you can round off any calculated number by simply adding or subtracting a predetermined value called a \"decimal point.\" Therefore, if desired precision was requested for this calculation, adjust accordingly!\n",
      "For example: Given our original question:\n",
      "\n",
      "1 + 1 - 1 = 0.5\n",
      "So there's approximately 0.\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "1. Thread is a single-threaded program that runs concurrently with other threads while processes are run on different computers or operating systems independently of each other, creating multitasking capabilities similar to those found in modern multi-user computing environments such as Windows XP and its successors. Processes can be started by running their own command line interface (CLI) tool like Task Manager or Control Panel to manage them effectively without any coordination from another application or user.\n",
      "\n",
      "2. Threads have an exclusive execution period\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not USE_GPU: # move model to CPU when GPU isn't requested\n",
    "    sft_trainer.model.to(\"cpu\")\n",
    "test_model_with_questions(sft_trainer.model, tokenizer, questions, \n",
    "                          title=\"Base Model (After SFT) Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bogag1lqd7",
   "metadata": {},
   "source": [
    "## èª²ç¨‹ç¸½çµèˆ‡æ·±å…¥æ€è€ƒ\n",
    "\n",
    "### æˆ‘å€‘åœ¨é€™å€‹èª²ç¨‹ä¸­å­¸åˆ°äº†ä»€éº¼ï¼š\n",
    "\n",
    "#### 1. SFT çš„æ ¸å¿ƒæ¦‚å¿µ\n",
    "- **å®šç¾©**ï¼šç›£ç£å¼å¾®èª¿ï¼ˆSFTï¼‰æ˜¯ä¸€ç¨®å°‡åŸºç¤èªè¨€æ¨¡å‹è½‰æ›ç‚ºèƒ½å¤ éµå¾ªæŒ‡ä»¤çš„å°è©±æ¨¡å‹çš„æŠ€è¡“\n",
    "- **åŸç†**ï¼šé€šéæœ€å°åŒ–è² å°æ•¸ä¼¼ç„¶æå¤±å‡½æ•¸ï¼Œè®“æ¨¡å‹å­¸ç¿’æ¨¡ä»¿è¨“ç·´è³‡æ–™ä¸­çš„å›æ‡‰\n",
    "- **ç›®æ¨™**ï¼šä½¿æ¨¡å‹èƒ½å¤ ç†è§£ä¸¦é©ç•¶å›æ‡‰ä½¿ç”¨è€…çš„å„ç¨®æŒ‡ä»¤å’Œå•é¡Œ\n",
    "\n",
    "#### 2. SFT çš„æ•¸å­¸åŸºç¤\n",
    "- **æå¤±å‡½æ•¸**ï¼š`Loss = -log P(response | prompt)`\n",
    "- **è¨“ç·´ç›®æ¨™**ï¼šæœ€å¤§åŒ–çµ¦å®šæç¤ºä¸‹ç”Ÿæˆæ­£ç¢ºå›æ‡‰çš„æ¦‚ç‡\n",
    "- **å¯¦ç¾æ–¹å¼**ï¼šå°æ‰€æœ‰è¨“ç·´æ¨£æœ¬çš„æå¤±æ±‚å’Œä¸¦é€²è¡Œæ¢¯åº¦ä¸‹é™\n",
    "\n",
    "#### 3. è³‡æ–™å“è³ªçš„é‡è¦æ€§\n",
    "- **å“è³ªå‹æ–¼æ•¸é‡**ï¼š1000å€‹é«˜å“è³ªæ¨£æœ¬å¾€å¾€æ¯”100è¬å€‹æ··åˆå“è³ªçš„æ¨£æœ¬æ•ˆæœæ›´å¥½\n",
    "- **è³‡æ–™æ•´ç†æ–¹æ³•**ï¼š\n",
    "  - è’¸é¤¾ï¼šä½¿ç”¨è¼ƒå¤§æ¨¡å‹ç”Ÿæˆé«˜å“è³ªå›æ‡‰\n",
    "  - æœ€ä½³ k é¸æ“‡ï¼šå¾å¤šå€‹ç”Ÿæˆçµæœä¸­é¸æ“‡æœ€ä½³å›æ‡‰\n",
    "  - ç¯©é¸ï¼šæ ¹æ“šå“è³ªå’Œå¤šæ¨£æ€§ç¯©é¸å¤§è¦æ¨¡è³‡æ–™é›†\n",
    "\n",
    "#### 4. å¯¦éš›æ‡‰ç”¨å ´æ™¯\n",
    "- **æ¨¡å‹è¡Œç‚ºå•Ÿå‹•**ï¼šå°‡é è¨“ç·´æ¨¡å‹è½‰ç‚ºæŒ‡ä»¤æ¨¡å‹\n",
    "- **èƒ½åŠ›æ”¹å–„**ï¼šæå‡ç‰¹å®šä»»å‹™çš„è¡¨ç¾\n",
    "- **çŸ¥è­˜è’¸é¤¾**ï¼šå°‡å¤§æ¨¡å‹çš„èƒ½åŠ›è½‰ç§»åˆ°å°æ¨¡å‹\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "post-training-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
