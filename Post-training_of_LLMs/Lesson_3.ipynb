{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc935f2b-a58b-4110-8918-96c868047b38",
   "metadata": {},
   "source": "# L3: Supervised Fine-Tuning (SFT) 監督式微調\n\n## 課程概述\n\n在這個課程中，我們將學習監督式微調（SFT）的基本概念和實作方法。SFT是一種將基礎語言模型轉換為能夠遵循指令的對話模型的重要技術。\n\n### 主要學習目標：\n1. **理解 SFT 的基本原理**：學習如何透過模仿範例回應來訓練模型\n2. **掌握 SFT 的工作流程**：從資料準備到模型訓練的完整過程\n3. **實作 SFT 訓練**：使用真實資料集進行模型微調\n4. **比較訓練前後的效果**：觀察模型在微調前後的差異\n\n### 課程重點：\n- **SFT 的數學原理**：負對數似然損失函數的最小化\n- **資料品質的重要性**：高品質資料比大量資料更重要\n- **參數效率微調**：LoRA 等技術的應用\n- **實際應用案例**：從基礎模型到指令模型的轉換"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7248ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepLearning.AI_SelfStudy-Notes'...\n",
      "remote: Enumerating objects: 23, done.\u001b[K\n",
      "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
      "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
      "remote: Total 23 (delta 3), reused 18 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (23/23), 10.71 KiB | 10.71 MiB/s, done.\n",
      "Resolving deltas: 100% (3/3), done.\n",
      "DeepLearning.AI_SelfStudy-Notes  Post-training_of_LLMs\tREADME.md\n",
      "/kaggle/working/DeepLearning.AI_SelfStudy-Notes/DeepLearning.AI_SelfStudy-Notes\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sheep52031/DeepLearning.AI_SelfStudy-Notes.git\n",
    "!ls\n",
    "%cd DeepLearning.AI_SelfStudy-Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = 'DeepLearning.AI_SelfStudy-Notes/Post-training_of_LLMs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2428be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r Post-training_of_LLMs/requirements.txt --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfc986-9ac7-4a2d-9dd0-a76841c7f46d",
   "metadata": {},
   "source": "## 匯入必要的函式庫\n\n這個部分我們將匯入進行 SFT 訓練所需的核心函式庫："
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3304e49d-bd1e-469b-a5b4-5edb16ecf344",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": "import torch  # PyTorch 深度學習框架\nimport pandas as pd  # 資料處理和分析\nfrom datasets import load_dataset, Dataset  # HuggingFace 資料集載入\nfrom transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM  # 模型和分詞器\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig  # SFT 訓練工具"
  },
  {
   "cell_type": "markdown",
   "id": "3cc63b02-5e9a-4a83-b042-4a2386cf5976",
   "metadata": {},
   "source": "## 設定輔助函式\n\n這些輔助函式將幫助我們進行模型載入、回應生成和測試等操作。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69982ae0-755e-48cf-ba4c-3b83b091fd9a",
   "metadata": {
    "height": 557
   },
   "outputs": [],
   "source": "def generate_responses(model, tokenizer, user_message, system_message=None, \n                       max_new_tokens=100):\n    \"\"\"\n    生成模型回應的函式\n    \n    參數:\n    - model: 語言模型\n    - tokenizer: 分詞器\n    - user_message: 使用者訊息\n    - system_message: 系統訊息（可選）\n    - max_new_tokens: 生成的最大新詞元數量\n    \n    返回:\n    - response: 模型生成的回應\n    \"\"\"\n    # 使用分詞器的聊天模板格式化對話\n    messages = []\n    if system_message:\n        messages.append({\"role\": \"system\", \"content\": system_message})\n    \n    # 我們假設資料都是單輪對話\n    messages.append({\"role\": \"user\", \"content\": user_message})\n        \n    # 應用聊天模板\n    prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n        enable_thinking=False,\n    )\n\n    # 將提示轉換為模型輸入格式\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # 生成回應（建議使用 vllm、sglang 或 TensorRT 以獲得更好的效能）\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,  # 使用貪心搜索\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n    \n    # 提取生成的部分\n    input_len = inputs[\"input_ids\"].shape[1]\n    generated_ids = outputs[0][input_len:]\n    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n\n    return response"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e5b05-a493-4683-91fd-7417885efc0f",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": "def test_model_with_questions(model, tokenizer, questions, \n                              system_message=None, title=\"Model Output\"):\n    \"\"\"\n    測試模型對一系列問題的回應\n    \n    參數:\n    - model: 語言模型\n    - tokenizer: 分詞器\n    - questions: 問題列表\n    - system_message: 系統訊息（可選）\n    - title: 輸出標題\n    \"\"\"\n    print(f\"\\n=== {title} ===\")\n    for i, question in enumerate(questions, 1):\n        response = generate_responses(model, tokenizer, question, \n                                      system_message)\n        print(f\"\\nModel Input {i}:\\n{question}\\nModel Output {i}:\\n{response}\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c273931-6827-4ee1-af1a-83a99bf94bf7",
   "metadata": {
    "height": 387
   },
   "outputs": [],
   "source": "def load_model_and_tokenizer(model_name, use_gpu = False):\n    \"\"\"\n    載入模型和分詞器\n    \n    參數:\n    - model_name: 模型名稱或路徑\n    - use_gpu: 是否使用 GPU\n    \n    返回:\n    - model: 載入的模型\n    - tokenizer: 載入的分詞器\n    \"\"\"\n    # 載入基礎模型和分詞器\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n    \n    # 如果使用 GPU，將模型移動到 CUDA 設備\n    if use_gpu:\n        model.to(\"cuda\")\n    \n    # 如果分詞器沒有聊天模板，則創建一個基本的模板\n    if not tokenizer.chat_template:\n        tokenizer.chat_template = \"\"\"{% for message in messages %}\n                {% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\n                {% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\n                {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }} <|endoftext|>\n                {% endif %}\n                {% endfor %}\"\"\"\n    \n    # 分詞器配置\n    if not tokenizer.pad_token:\n        tokenizer.pad_token = tokenizer.eos_token\n        \n    return model, tokenizer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd15e1d-6ecd-4337-a5dd-1602da354f62",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": "def display_dataset(dataset):\n    \"\"\"\n    顯示資料集的前幾個範例\n    \n    參數:\n    - dataset: 要顯示的資料集\n    \"\"\"\n    # 視覺化資料集\n    rows = []\n    for i in range(3):\n        example = dataset[i]\n        # 提取使用者訊息\n        user_msg = next(m['content'] for m in example['messages']\n                        if m['role'] == 'user')\n        # 提取助手回應\n        assistant_msg = next(m['content'] for m in example['messages']\n                             if m['role'] == 'assistant')\n        rows.append({\n            'User Prompt': user_msg,\n            'Assistant Response': assistant_msg\n        })\n    \n    # 顯示為表格\n    df = pd.DataFrame(rows)\n    pd.set_option('display.max_colwidth', None)  # 避免截斷長字串\n    display(df)"
  },
  {
   "cell_type": "markdown",
   "id": "0f5ac817-43a4-43c9-88f3-9825b96b84b7",
   "metadata": {},
   "source": "## 載入基礎模型並測試簡單問題\n\n首先我們載入一個基礎模型（未經過指令微調的模型），並測試它對簡單問題的回應能力。這將幫助我們理解 SFT 前後的差異。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed78c2-ea93-4ac2-bd6f-5d4391de7c8d",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": "# 設定是否使用 GPU（在 Kaggle 環境中設為 True）\nUSE_GPU = True\n\n# 定義測試問題\nquestions = [\n    \"Give me an 1-sentence introduction of LLM.\",  # 要求簡短介紹 LLM\n    \"Calculate 1+1-1\",  # 簡單數學計算\n    \"What's the difference between thread and process?\"  # 技術概念解釋\n]"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf3d51af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612741e8945f4804b303759a2d0c923f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff117a5dee484e4681fc6d94bda230a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d464d632d67471b9241285fc4bd2051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5f173e14ff47b7bcfe76b8ef1738a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b380f74d394257a25d7b1a119cd07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b329559f9e8a4a8799475b46d28b1322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63868ecd3b114006a86f8fee414d0f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2c01183096427dbe81352980db502e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998bb9466dd74db7b946ccca33c03d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5a9fa6db364f40ba0b5516327be62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/DeepLearning.AI_SelfStudy-Notes/DeepLearning.AI_SelfStudy-Notes/Qwen3-0.6B-Base'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"Qwen/Qwen3-0.6B-Base\",\n",
    "    local_dir=\"./Qwen3-0.6B-Base\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "086a902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "local_dir = \"Qwen3-0.6B-Base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba426c74-4d93-42b3-b2c7-5791fb9bf3c5",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (Before SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ ⚙ �\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ �\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ ⚇ �\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_model_with_questions(model, tokenizer, questions, \n",
    "                          title=\"Base Model (Before SFT) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885253e0-3a9a-4b42-a36b-a8a3ddd340a1",
   "metadata": {},
   "source": "## Qwen3-0.6B 模型的 SFT 結果\n\n在這個部分，我們將檢視先前完成的 SFT 訓練結果。由於資源限制，我們不會在像 Qwen3-0.6B 這樣相對較大的模型上進行完整訓練。\n\n### 對比分析：\n- **基礎模型（SFT前）**：只會生成隨機符號，無法理解指令\n- **微調模型（SFT後）**：能夠理解並回應使用者的問題\n\n這個對比清楚地展示了 SFT 的威力 - 它能將一個只會預測下一個詞的基礎模型，轉換為能夠進行有意義對話的助手模型。"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8f6f92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b945693ced4920acd8421690368e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d8baac81c44c108d543d7db975210b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c350de832604a30a82897a896433af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65045f65aecb42d598f1022960d35d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a1f38afb2941d181cf5db0870cc3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b688ae40ef4078b9f7c0ab7bb927ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73edf0ce74d40f288e7682a2cf50df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.38G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018fde577f33455e83ea23d1e31ee108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a8643ef57f45178afe219329c3f8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/DeepLearning.AI_SelfStudy-Notes/Qwen3-0.6B-SFT'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"banghua/Qwen3-0.6B-SFT\",\n",
    "    local_dir=\"./Qwen3-0.6B-SFT\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e86f13c-c969-4c7e-8702-d074ee7a2ce6",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (After SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "LLM is a program that provides advanced legal knowledge and skills to professionals and individuals.\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "1+1-1 = 2-1 = 1\n",
      "\n",
      "So, the final answer is 1.\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "In computer science, a thread is a unit of execution that runs in a separate process. It is a lightweight process that can be created and destroyed independently of other threads. Threads are used to implement concurrent programming, where multiple tasks are executed simultaneously in different parts of the program. Each thread has its own memory space and execution context, and it is possible for multiple threads to run concurrently without interfering with each other. Threads are also known as lightweight processes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\"./Qwen3-0.6B-SFT\", USE_GPU)\n",
    "\n",
    "test_model_with_questions(model, tokenizer, questions, \n",
    "                          title=\"Base Model (After SFT) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf299ee-aa84-4c43-8d7b-f0998077e2cb",
   "metadata": {},
   "source": "## 在小型模型上進行 SFT 訓練\n\n接下來我們將實際進行 SFT 訓練的完整流程。我們將使用一個較小的模型和資料集來確保訓練過程能在有限的計算資源上執行。"
  },
  {
   "cell_type": "markdown",
   "id": "7c5cb7ea-e157-418e-84f5-34ecbed823ad",
   "metadata": {},
   "source": "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n<p> 💻 &nbsp; <b>注意：</b> 我們在小型模型 <code>HuggingFaceTB/SmolLM2-135M</code> 和較小的訓練資料集上進行 SFT，以確保完整的訓練過程能在有限的計算資源上運行。如果你在自己的機器上運行筆記本並且有 GPU 資源，可以切換到更大的模型（如 <code>Qwen/Qwen3-0.6B-Base</code>）來進行完整的 SFT 訓練並重現上述結果。</p>\n</div>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb07589-049d-432e-8001-e6e9175ad806",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": "# 選擇模型：在有 GPU 資源的情況下使用 Qwen3-0.6B-Base\n# 如果資源有限，可以改用 HuggingFaceTB/SmolLM2-135M\n#model_name = \"./models/HuggingFaceTB/SmolLM2-135M\"\nmodel_name = \"Qwen/Qwen3-0.6B-Base\"\nmodel, tokenizer = load_model_and_tokenizer(model_name, USE_GPU)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d52c3e-9c6c-43c3-bd95-92d60b9c9a8f",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": "# 載入訓練資料集\ntrain_dataset = load_dataset(\"banghua/DL-SFT-Dataset\")[\"train\"]\n\n# 如果沒有使用 GPU，則只使用前 100 個樣本進行訓練\nif not USE_GPU:\n    train_dataset = train_dataset.select(range(100))\n\n# 顯示資料集的前幾個範例\nprint(\"訓練資料集範例：\")\ndisplay_dataset(train_dataset)\n\nprint(f\"\\n資料集大小：{len(train_dataset)} 個樣本\")\nprint(\"\\n資料集特點：\")\nprint(\"- 包含多樣化的指令和回應對\")\nprint(\"- 涵蓋問答、翻譯、計算等多種任務\")\nprint(\"- 每個樣本都包含使用者提示和助手回應\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c515a8-3728-45fa-88cc-6eb4de839839",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": "# SFT 訓練器配置\nsft_config = SFTConfig(\n    learning_rate=8e-5,  # 學習率：控制模型參數更新的步長\n    num_train_epochs=1,  # 訓練輪數：設定模型訓練的完整迭代次數\n    per_device_train_batch_size=1,  # 每設備批次大小：每個設備（如 GPU）訓練時的批次大小\n    gradient_accumulation_steps=8,  # 梯度累積步驟：在執行反向傳播之前累積梯度的步驟數\n    gradient_checkpointing=False,  # 梯度檢查點：以較慢的訓練速度為代價減少記憶體使用\n    logging_steps=2,  # 記錄步驟：記錄訓練進度的頻率（每 2 步記錄一次）\n)\n\n# 關鍵超參數解釋：\nprint(\"SFT 訓練配置說明：\")\nprint(f\"學習率: {sft_config.learning_rate} - 決定模型參數更新的幅度\")\nprint(f\"訓練輪數: {sft_config.num_train_epochs} - 完整遍歷資料集的次數\")\nprint(f\"批次大小: {sft_config.per_device_train_batch_size} - 每次更新使用的樣本數\")\nprint(f\"梯度累積: {sft_config.gradient_accumulation_steps} - 有效批次大小 = 批次大小 × 梯度累積步驟\")\nprint(f\"有效批次大小: {sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7d132-4c69-4b12-a8ec-e6b4795faad9",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": "# 建立 SFT 訓練器\nsft_trainer = SFTTrainer(\n    model=model,  # 要訓練的模型\n    args=sft_config,  # 訓練配置\n    train_dataset=train_dataset,  # 訓練資料集\n    processing_class=tokenizer,  # 分詞器\n)\n\nprint(\"開始 SFT 訓練...\")\nprint(\"\\nSFT 訓練過程說明：\")\nprint(\"1. 模型將學習模仿訓練資料中的回應\")\nprint(\"2. 透過最小化負對數似然損失來訓練\")\nprint(\"3. 進度條將顯示訓練步驟和損失值\")\nprint(\"4. 訓練完成後，模型將能夠回應使用者指令\")\n\n# 執行訓練\nsft_trainer.train()"
  },
  {
   "cell_type": "markdown",
   "id": "56b4bac1-7262-4c55-b411-6a59188157b0",
   "metadata": {},
   "source": "## 測試小型模型和小資料集的訓練結果\n\n**注意：** 以下結果是針對我們在 SFT 訓練中使用的小型模型和資料集，這是由於計算資源有限。若要查看大型模型的完整訓練結果，請參閱上方的 **「Qwen3-0.6B 模型的 SFT 結果」** 部分。\n\n### 預期結果分析：\n由於我們使用的是小型模型（相對較少的參數）和有限的訓練資料，模型的表現可能會有以下特點：\n- **有一定的改善**：相比未訓練的模型，應該能看到明顯的改善\n- **仍有限制**：由於模型規模和資料量的限制，可能無法達到完美的表現\n- **學習能力展現**：可以觀察到模型開始學會回應指令的基本能力"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d678274-5768-4cea-ae20-051488e5d0f3",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": "# 如果沒有使用 GPU，將模型移到 CPU\nif not USE_GPU:\n    sft_trainer.model.to(\"cpu\")\n\nprint(\"\\n=== SFT 訓練完成！===\\n\")\nprint(\"現在來測試訓練後的模型表現...\")\n\n# 測試訓練後的模型\ntest_model_with_questions(sft_trainer.model, tokenizer, questions, \n                          title=\"訓練後的模型輸出\")\n\nprint(\"\\n=== 訓練效果分析 ===\\n\")\nprint(\"SFT 訓練的核心價值：\")\nprint(\"1. 將基礎模型轉換為指令跟隨模型\")\nprint(\"2. 透過模仿範例回應來學習對話模式\")\nprint(\"3. 使模型能夠理解並回應使用者的各種請求\")\nprint(\"\\n即使在有限的資源下，我們也能觀察到模型在指令跟隨方面的改善！\")"
  },
  {
   "cell_type": "markdown",
   "id": "bogag1lqd7",
   "source": "## 課程總結與深入思考\n\n### 我們在這個課程中學到了什麼：\n\n#### 1. SFT 的核心概念\n- **定義**：監督式微調（SFT）是一種將基礎語言模型轉換為能夠遵循指令的對話模型的技術\n- **原理**：通過最小化負對數似然損失函數，讓模型學習模仿訓練資料中的回應\n- **目標**：使模型能夠理解並適當回應使用者的各種指令和問題\n\n#### 2. SFT 的數學基礎\n- **損失函數**：`Loss = -log P(response | prompt)`\n- **訓練目標**：最大化給定提示下生成正確回應的概率\n- **實現方式**：對所有訓練樣本的損失求和並進行梯度下降\n\n#### 3. 資料品質的重要性\n- **品質勝於數量**：1000個高品質樣本往往比100萬個混合品質的樣本效果更好\n- **資料整理方法**：\n  - 蒸餾：使用較大模型生成高品質回應\n  - 最佳 k 選擇：從多個生成結果中選擇最佳回應\n  - 篩選：根據品質和多樣性篩選大規模資料集\n\n#### 4. 實際應用場景\n- **模型行為啟動**：將預訓練模型轉為指令模型\n- **能力改善**：提升特定任務的表現\n- **知識蒸餾**：將大模型的能力轉移到小模型\n\n### 運算環境選擇建議\n\n關於您提到的運算環境選擇問題，讓我為您分析：\n\n#### Kaggle GPU vs Ubuntu RTX3080 選擇：\n\n**Kaggle 優勢：**\n- 免費的 GPU 資源（每週約 30 小時）\n- 無需本地設置，即開即用\n- 預裝大部分深度學習套件\n- 方便的筆記本分享和版本控制\n\n**Kaggle 劣勢：**\n- 檔案管理相對不便\n- 終端操作受限\n- 時間限制（每次最多 12 小時）\n- 網路限制可能影響大模型下載\n\n**Ubuntu RTX3080 優勢：**\n- 完全控制權，無時間限制\n- 更好的檔案管理和終端操作\n- 可以進行長時間訓練\n- 本地儲存，無需重複下載\n\n**Ubuntu RTX3080 劣勢：**\n- 需要自己設置環境\n- 電費和硬體損耗\n- 需要維護和更新\n\n#### 具體建議：\n\n1. **對於本課程的 SFT 訓練**：\n   - 如果只是學習和實驗，Kaggle 完全夠用\n   - RTX3080 10GB 對於 Qwen3-0.6B 模型是充足的\n   - 可以兩者結合使用：Kaggle 做初步實驗，Ubuntu 做正式訓練\n\n2. **選擇依據**：\n   - **學習目的**：優先選擇 Kaggle\n   - **生產目的**：建議使用 Ubuntu RTX3080\n   - **大模型訓練**：Ubuntu RTX3080 更適合\n\n3. **最佳實踐**：\n   - 在 Kaggle 上快速原型和測試\n   - 在本地 Ubuntu 上進行完整訓練\n   - 使用 Git 同步代碼和配置\n\n### 下一步學習建議\n\n1. **深入理解 SFT 變體**：\n   - 研究 LoRA、QLoRA 等參數效率微調方法\n   - 了解指令工程和提示設計\n\n2. **探索進階技術**：\n   - 直接偏好優化（DPO）\n   - 強化學習人類反饋（RLHF）\n   - 多輪對話訓練\n\n3. **實踐項目**：\n   - 嘗試在不同領域的資料集上進行 SFT\n   - 比較不同超參數設置的效果\n   - 評估模型在各種任務上的表現\n\n監督式微調是現代 AI 系統的核心技術之一，掌握它將為您在 AI 領域的發展打下堅實基礎！",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}