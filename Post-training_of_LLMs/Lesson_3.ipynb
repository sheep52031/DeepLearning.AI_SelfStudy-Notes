{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc935f2b-a58b-4110-8918-96c868047b38",
   "metadata": {},
   "source": "# L3: Supervised Fine-Tuning (SFT) ç›£ç£å¼å¾®èª¿\n\n## èª²ç¨‹æ¦‚è¿°\n\nåœ¨é€™å€‹èª²ç¨‹ä¸­ï¼Œæˆ‘å€‘å°‡å­¸ç¿’ç›£ç£å¼å¾®èª¿ï¼ˆSFTï¼‰çš„åŸºæœ¬æ¦‚å¿µå’Œå¯¦ä½œæ–¹æ³•ã€‚SFTæ˜¯ä¸€ç¨®å°‡åŸºç¤èªè¨€æ¨¡å‹è½‰æ›ç‚ºèƒ½å¤ éµå¾ªæŒ‡ä»¤çš„å°è©±æ¨¡å‹çš„é‡è¦æŠ€è¡“ã€‚\n\n### ä¸»è¦å­¸ç¿’ç›®æ¨™ï¼š\n1. **ç†è§£ SFT çš„åŸºæœ¬åŸç†**ï¼šå­¸ç¿’å¦‚ä½•é€éæ¨¡ä»¿ç¯„ä¾‹å›æ‡‰ä¾†è¨“ç·´æ¨¡å‹\n2. **æŒæ¡ SFT çš„å·¥ä½œæµç¨‹**ï¼šå¾è³‡æ–™æº–å‚™åˆ°æ¨¡å‹è¨“ç·´çš„å®Œæ•´éç¨‹\n3. **å¯¦ä½œ SFT è¨“ç·´**ï¼šä½¿ç”¨çœŸå¯¦è³‡æ–™é›†é€²è¡Œæ¨¡å‹å¾®èª¿\n4. **æ¯”è¼ƒè¨“ç·´å‰å¾Œçš„æ•ˆæœ**ï¼šè§€å¯Ÿæ¨¡å‹åœ¨å¾®èª¿å‰å¾Œçš„å·®ç•°\n\n### èª²ç¨‹é‡é»ï¼š\n- **SFT çš„æ•¸å­¸åŸç†**ï¼šè² å°æ•¸ä¼¼ç„¶æå¤±å‡½æ•¸çš„æœ€å°åŒ–\n- **è³‡æ–™å“è³ªçš„é‡è¦æ€§**ï¼šé«˜å“è³ªè³‡æ–™æ¯”å¤§é‡è³‡æ–™æ›´é‡è¦\n- **åƒæ•¸æ•ˆç‡å¾®èª¿**ï¼šLoRA ç­‰æŠ€è¡“çš„æ‡‰ç”¨\n- **å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹**ï¼šå¾åŸºç¤æ¨¡å‹åˆ°æŒ‡ä»¤æ¨¡å‹çš„è½‰æ›"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7248ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepLearning.AI_SelfStudy-Notes'...\n",
      "remote: Enumerating objects: 23, done.\u001b[K\n",
      "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
      "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
      "remote: Total 23 (delta 3), reused 18 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (23/23), 10.71 KiB | 10.71 MiB/s, done.\n",
      "Resolving deltas: 100% (3/3), done.\n",
      "DeepLearning.AI_SelfStudy-Notes  Post-training_of_LLMs\tREADME.md\n",
      "/kaggle/working/DeepLearning.AI_SelfStudy-Notes/DeepLearning.AI_SelfStudy-Notes\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sheep52031/DeepLearning.AI_SelfStudy-Notes.git\n",
    "!ls\n",
    "%cd DeepLearning.AI_SelfStudy-Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = 'DeepLearning.AI_SelfStudy-Notes/Post-training_of_LLMs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2428be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r Post-training_of_LLMs/requirements.txt --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfc986-9ac7-4a2d-9dd0-a76841c7f46d",
   "metadata": {},
   "source": "## åŒ¯å…¥å¿…è¦çš„å‡½å¼åº«\n\né€™å€‹éƒ¨åˆ†æˆ‘å€‘å°‡åŒ¯å…¥é€²è¡Œ SFT è¨“ç·´æ‰€éœ€çš„æ ¸å¿ƒå‡½å¼åº«ï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3304e49d-bd1e-469b-a5b4-5edb16ecf344",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": "import torch  # PyTorch æ·±åº¦å­¸ç¿’æ¡†æ¶\nimport pandas as pd  # è³‡æ–™è™•ç†å’Œåˆ†æ\nfrom datasets import load_dataset, Dataset  # HuggingFace è³‡æ–™é›†è¼‰å…¥\nfrom transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM  # æ¨¡å‹å’Œåˆ†è©å™¨\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig  # SFT è¨“ç·´å·¥å…·"
  },
  {
   "cell_type": "markdown",
   "id": "3cc63b02-5e9a-4a83-b042-4a2386cf5976",
   "metadata": {},
   "source": "## è¨­å®šè¼”åŠ©å‡½å¼\n\né€™äº›è¼”åŠ©å‡½å¼å°‡å¹«åŠ©æˆ‘å€‘é€²è¡Œæ¨¡å‹è¼‰å…¥ã€å›æ‡‰ç”Ÿæˆå’Œæ¸¬è©¦ç­‰æ“ä½œã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69982ae0-755e-48cf-ba4c-3b83b091fd9a",
   "metadata": {
    "height": 557
   },
   "outputs": [],
   "source": "def generate_responses(model, tokenizer, user_message, system_message=None, \n                       max_new_tokens=100):\n    \"\"\"\n    ç”Ÿæˆæ¨¡å‹å›æ‡‰çš„å‡½å¼\n    \n    åƒæ•¸:\n    - model: èªè¨€æ¨¡å‹\n    - tokenizer: åˆ†è©å™¨\n    - user_message: ä½¿ç”¨è€…è¨Šæ¯\n    - system_message: ç³»çµ±è¨Šæ¯ï¼ˆå¯é¸ï¼‰\n    - max_new_tokens: ç”Ÿæˆçš„æœ€å¤§æ–°è©å…ƒæ•¸é‡\n    \n    è¿”å›:\n    - response: æ¨¡å‹ç”Ÿæˆçš„å›æ‡‰\n    \"\"\"\n    # ä½¿ç”¨åˆ†è©å™¨çš„èŠå¤©æ¨¡æ¿æ ¼å¼åŒ–å°è©±\n    messages = []\n    if system_message:\n        messages.append({\"role\": \"system\", \"content\": system_message})\n    \n    # æˆ‘å€‘å‡è¨­è³‡æ–™éƒ½æ˜¯å–®è¼ªå°è©±\n    messages.append({\"role\": \"user\", \"content\": user_message})\n        \n    # æ‡‰ç”¨èŠå¤©æ¨¡æ¿\n    prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n        enable_thinking=False,\n    )\n\n    # å°‡æç¤ºè½‰æ›ç‚ºæ¨¡å‹è¼¸å…¥æ ¼å¼\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # ç”Ÿæˆå›æ‡‰ï¼ˆå»ºè­°ä½¿ç”¨ vllmã€sglang æˆ– TensorRT ä»¥ç²å¾—æ›´å¥½çš„æ•ˆèƒ½ï¼‰\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,  # ä½¿ç”¨è²ªå¿ƒæœç´¢\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n    \n    # æå–ç”Ÿæˆçš„éƒ¨åˆ†\n    input_len = inputs[\"input_ids\"].shape[1]\n    generated_ids = outputs[0][input_len:]\n    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n\n    return response"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e5b05-a493-4683-91fd-7417885efc0f",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": "def test_model_with_questions(model, tokenizer, questions, \n                              system_message=None, title=\"Model Output\"):\n    \"\"\"\n    æ¸¬è©¦æ¨¡å‹å°ä¸€ç³»åˆ—å•é¡Œçš„å›æ‡‰\n    \n    åƒæ•¸:\n    - model: èªè¨€æ¨¡å‹\n    - tokenizer: åˆ†è©å™¨\n    - questions: å•é¡Œåˆ—è¡¨\n    - system_message: ç³»çµ±è¨Šæ¯ï¼ˆå¯é¸ï¼‰\n    - title: è¼¸å‡ºæ¨™é¡Œ\n    \"\"\"\n    print(f\"\\n=== {title} ===\")\n    for i, question in enumerate(questions, 1):\n        response = generate_responses(model, tokenizer, question, \n                                      system_message)\n        print(f\"\\nModel Input {i}:\\n{question}\\nModel Output {i}:\\n{response}\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c273931-6827-4ee1-af1a-83a99bf94bf7",
   "metadata": {
    "height": 387
   },
   "outputs": [],
   "source": "def load_model_and_tokenizer(model_name, use_gpu = False):\n    \"\"\"\n    è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨\n    \n    åƒæ•¸:\n    - model_name: æ¨¡å‹åç¨±æˆ–è·¯å¾‘\n    - use_gpu: æ˜¯å¦ä½¿ç”¨ GPU\n    \n    è¿”å›:\n    - model: è¼‰å…¥çš„æ¨¡å‹\n    - tokenizer: è¼‰å…¥çš„åˆ†è©å™¨\n    \"\"\"\n    # è¼‰å…¥åŸºç¤æ¨¡å‹å’Œåˆ†è©å™¨\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n    \n    # å¦‚æœä½¿ç”¨ GPUï¼Œå°‡æ¨¡å‹ç§»å‹•åˆ° CUDA è¨­å‚™\n    if use_gpu:\n        model.to(\"cuda\")\n    \n    # å¦‚æœåˆ†è©å™¨æ²’æœ‰èŠå¤©æ¨¡æ¿ï¼Œå‰‡å‰µå»ºä¸€å€‹åŸºæœ¬çš„æ¨¡æ¿\n    if not tokenizer.chat_template:\n        tokenizer.chat_template = \"\"\"{% for message in messages %}\n                {% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\n                {% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\n                {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }} <|endoftext|>\n                {% endif %}\n                {% endfor %}\"\"\"\n    \n    # åˆ†è©å™¨é…ç½®\n    if not tokenizer.pad_token:\n        tokenizer.pad_token = tokenizer.eos_token\n        \n    return model, tokenizer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd15e1d-6ecd-4337-a5dd-1602da354f62",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": "def display_dataset(dataset):\n    \"\"\"\n    é¡¯ç¤ºè³‡æ–™é›†çš„å‰å¹¾å€‹ç¯„ä¾‹\n    \n    åƒæ•¸:\n    - dataset: è¦é¡¯ç¤ºçš„è³‡æ–™é›†\n    \"\"\"\n    # è¦–è¦ºåŒ–è³‡æ–™é›†\n    rows = []\n    for i in range(3):\n        example = dataset[i]\n        # æå–ä½¿ç”¨è€…è¨Šæ¯\n        user_msg = next(m['content'] for m in example['messages']\n                        if m['role'] == 'user')\n        # æå–åŠ©æ‰‹å›æ‡‰\n        assistant_msg = next(m['content'] for m in example['messages']\n                             if m['role'] == 'assistant')\n        rows.append({\n            'User Prompt': user_msg,\n            'Assistant Response': assistant_msg\n        })\n    \n    # é¡¯ç¤ºç‚ºè¡¨æ ¼\n    df = pd.DataFrame(rows)\n    pd.set_option('display.max_colwidth', None)  # é¿å…æˆªæ–·é•·å­—ä¸²\n    display(df)"
  },
  {
   "cell_type": "markdown",
   "id": "0f5ac817-43a4-43c9-88f3-9825b96b84b7",
   "metadata": {},
   "source": "## è¼‰å…¥åŸºç¤æ¨¡å‹ä¸¦æ¸¬è©¦ç°¡å–®å•é¡Œ\n\né¦–å…ˆæˆ‘å€‘è¼‰å…¥ä¸€å€‹åŸºç¤æ¨¡å‹ï¼ˆæœªç¶“éæŒ‡ä»¤å¾®èª¿çš„æ¨¡å‹ï¼‰ï¼Œä¸¦æ¸¬è©¦å®ƒå°ç°¡å–®å•é¡Œçš„å›æ‡‰èƒ½åŠ›ã€‚é€™å°‡å¹«åŠ©æˆ‘å€‘ç†è§£ SFT å‰å¾Œçš„å·®ç•°ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed78c2-ea93-4ac2-bd6f-5d4391de7c8d",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": "# è¨­å®šæ˜¯å¦ä½¿ç”¨ GPUï¼ˆåœ¨ Kaggle ç’°å¢ƒä¸­è¨­ç‚º Trueï¼‰\nUSE_GPU = True\n\n# å®šç¾©æ¸¬è©¦å•é¡Œ\nquestions = [\n    \"Give me an 1-sentence introduction of LLM.\",  # è¦æ±‚ç°¡çŸ­ä»‹ç´¹ LLM\n    \"Calculate 1+1-1\",  # ç°¡å–®æ•¸å­¸è¨ˆç®—\n    \"What's the difference between thread and process?\"  # æŠ€è¡“æ¦‚å¿µè§£é‡‹\n]"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf3d51af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612741e8945f4804b303759a2d0c923f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff117a5dee484e4681fc6d94bda230a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d464d632d67471b9241285fc4bd2051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5f173e14ff47b7bcfe76b8ef1738a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b380f74d394257a25d7b1a119cd07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b329559f9e8a4a8799475b46d28b1322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63868ecd3b114006a86f8fee414d0f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2c01183096427dbe81352980db502e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998bb9466dd74db7b946ccca33c03d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5a9fa6db364f40ba0b5516327be62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/DeepLearning.AI_SelfStudy-Notes/DeepLearning.AI_SelfStudy-Notes/Qwen3-0.6B-Base'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"Qwen/Qwen3-0.6B-Base\",\n",
    "    local_dir=\"./Qwen3-0.6B-Base\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "086a902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "local_dir = \"Qwen3-0.6B-Base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba426c74-4d93-42b3-b2c7-5791fb9bf3c5",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (Before SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ ï¿½\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ ï¿½\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ ï¿½\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_model_with_questions(model, tokenizer, questions, \n",
    "                          title=\"Base Model (Before SFT) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885253e0-3a9a-4b42-a36b-a8a3ddd340a1",
   "metadata": {},
   "source": "## Qwen3-0.6B æ¨¡å‹çš„ SFT çµæœ\n\nåœ¨é€™å€‹éƒ¨åˆ†ï¼Œæˆ‘å€‘å°‡æª¢è¦–å…ˆå‰å®Œæˆçš„ SFT è¨“ç·´çµæœã€‚ç”±æ–¼è³‡æºé™åˆ¶ï¼Œæˆ‘å€‘ä¸æœƒåœ¨åƒ Qwen3-0.6B é€™æ¨£ç›¸å°è¼ƒå¤§çš„æ¨¡å‹ä¸Šé€²è¡Œå®Œæ•´è¨“ç·´ã€‚\n\n### å°æ¯”åˆ†æï¼š\n- **åŸºç¤æ¨¡å‹ï¼ˆSFTå‰ï¼‰**ï¼šåªæœƒç”Ÿæˆéš¨æ©Ÿç¬¦è™Ÿï¼Œç„¡æ³•ç†è§£æŒ‡ä»¤\n- **å¾®èª¿æ¨¡å‹ï¼ˆSFTå¾Œï¼‰**ï¼šèƒ½å¤ ç†è§£ä¸¦å›æ‡‰ä½¿ç”¨è€…çš„å•é¡Œ\n\né€™å€‹å°æ¯”æ¸…æ¥šåœ°å±•ç¤ºäº† SFT çš„å¨åŠ› - å®ƒèƒ½å°‡ä¸€å€‹åªæœƒé æ¸¬ä¸‹ä¸€å€‹è©çš„åŸºç¤æ¨¡å‹ï¼Œè½‰æ›ç‚ºèƒ½å¤ é€²è¡Œæœ‰æ„ç¾©å°è©±çš„åŠ©æ‰‹æ¨¡å‹ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8f6f92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b945693ced4920acd8421690368e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d8baac81c44c108d543d7db975210b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c350de832604a30a82897a896433af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65045f65aecb42d598f1022960d35d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a1f38afb2941d181cf5db0870cc3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b688ae40ef4078b9f7c0ab7bb927ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73edf0ce74d40f288e7682a2cf50df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.38G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018fde577f33455e83ea23d1e31ee108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a8643ef57f45178afe219329c3f8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/DeepLearning.AI_SelfStudy-Notes/Qwen3-0.6B-SFT'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"banghua/Qwen3-0.6B-SFT\",\n",
    "    local_dir=\"./Qwen3-0.6B-SFT\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e86f13c-c969-4c7e-8702-d074ee7a2ce6",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (After SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "LLM is a program that provides advanced legal knowledge and skills to professionals and individuals.\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "1+1-1 = 2-1 = 1\n",
      "\n",
      "So, the final answer is 1.\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "In computer science, a thread is a unit of execution that runs in a separate process. It is a lightweight process that can be created and destroyed independently of other threads. Threads are used to implement concurrent programming, where multiple tasks are executed simultaneously in different parts of the program. Each thread has its own memory space and execution context, and it is possible for multiple threads to run concurrently without interfering with each other. Threads are also known as lightweight processes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\"./Qwen3-0.6B-SFT\", USE_GPU)\n",
    "\n",
    "test_model_with_questions(model, tokenizer, questions, \n",
    "                          title=\"Base Model (After SFT) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf299ee-aa84-4c43-8d7b-f0998077e2cb",
   "metadata": {},
   "source": "## åœ¨å°å‹æ¨¡å‹ä¸Šé€²è¡Œ SFT è¨“ç·´\n\næ¥ä¸‹ä¾†æˆ‘å€‘å°‡å¯¦éš›é€²è¡Œ SFT è¨“ç·´çš„å®Œæ•´æµç¨‹ã€‚æˆ‘å€‘å°‡ä½¿ç”¨ä¸€å€‹è¼ƒå°çš„æ¨¡å‹å’Œè³‡æ–™é›†ä¾†ç¢ºä¿è¨“ç·´éç¨‹èƒ½åœ¨æœ‰é™çš„è¨ˆç®—è³‡æºä¸ŠåŸ·è¡Œã€‚"
  },
  {
   "cell_type": "markdown",
   "id": "7c5cb7ea-e157-418e-84f5-34ecbed823ad",
   "metadata": {},
   "source": "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n<p> ğŸ’» &nbsp; <b>æ³¨æ„ï¼š</b> æˆ‘å€‘åœ¨å°å‹æ¨¡å‹ <code>HuggingFaceTB/SmolLM2-135M</code> å’Œè¼ƒå°çš„è¨“ç·´è³‡æ–™é›†ä¸Šé€²è¡Œ SFTï¼Œä»¥ç¢ºä¿å®Œæ•´çš„è¨“ç·´éç¨‹èƒ½åœ¨æœ‰é™çš„è¨ˆç®—è³‡æºä¸Šé‹è¡Œã€‚å¦‚æœä½ åœ¨è‡ªå·±çš„æ©Ÿå™¨ä¸Šé‹è¡Œç­†è¨˜æœ¬ä¸¦ä¸”æœ‰ GPU è³‡æºï¼Œå¯ä»¥åˆ‡æ›åˆ°æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚ <code>Qwen/Qwen3-0.6B-Base</code>ï¼‰ä¾†é€²è¡Œå®Œæ•´çš„ SFT è¨“ç·´ä¸¦é‡ç¾ä¸Šè¿°çµæœã€‚</p>\n</div>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb07589-049d-432e-8001-e6e9175ad806",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": "# é¸æ“‡æ¨¡å‹ï¼šåœ¨æœ‰ GPU è³‡æºçš„æƒ…æ³ä¸‹ä½¿ç”¨ Qwen3-0.6B-Base\n# å¦‚æœè³‡æºæœ‰é™ï¼Œå¯ä»¥æ”¹ç”¨ HuggingFaceTB/SmolLM2-135M\n#model_name = \"./models/HuggingFaceTB/SmolLM2-135M\"\nmodel_name = \"Qwen/Qwen3-0.6B-Base\"\nmodel, tokenizer = load_model_and_tokenizer(model_name, USE_GPU)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d52c3e-9c6c-43c3-bd95-92d60b9c9a8f",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": "# è¼‰å…¥è¨“ç·´è³‡æ–™é›†\ntrain_dataset = load_dataset(\"banghua/DL-SFT-Dataset\")[\"train\"]\n\n# å¦‚æœæ²’æœ‰ä½¿ç”¨ GPUï¼Œå‰‡åªä½¿ç”¨å‰ 100 å€‹æ¨£æœ¬é€²è¡Œè¨“ç·´\nif not USE_GPU:\n    train_dataset = train_dataset.select(range(100))\n\n# é¡¯ç¤ºè³‡æ–™é›†çš„å‰å¹¾å€‹ç¯„ä¾‹\nprint(\"è¨“ç·´è³‡æ–™é›†ç¯„ä¾‹ï¼š\")\ndisplay_dataset(train_dataset)\n\nprint(f\"\\nè³‡æ–™é›†å¤§å°ï¼š{len(train_dataset)} å€‹æ¨£æœ¬\")\nprint(\"\\nè³‡æ–™é›†ç‰¹é»ï¼š\")\nprint(\"- åŒ…å«å¤šæ¨£åŒ–çš„æŒ‡ä»¤å’Œå›æ‡‰å°\")\nprint(\"- æ¶µè“‹å•ç­”ã€ç¿»è­¯ã€è¨ˆç®—ç­‰å¤šç¨®ä»»å‹™\")\nprint(\"- æ¯å€‹æ¨£æœ¬éƒ½åŒ…å«ä½¿ç”¨è€…æç¤ºå’ŒåŠ©æ‰‹å›æ‡‰\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c515a8-3728-45fa-88cc-6eb4de839839",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": "# SFT è¨“ç·´å™¨é…ç½®\nsft_config = SFTConfig(\n    learning_rate=8e-5,  # å­¸ç¿’ç‡ï¼šæ§åˆ¶æ¨¡å‹åƒæ•¸æ›´æ–°çš„æ­¥é•·\n    num_train_epochs=1,  # è¨“ç·´è¼ªæ•¸ï¼šè¨­å®šæ¨¡å‹è¨“ç·´çš„å®Œæ•´è¿­ä»£æ¬¡æ•¸\n    per_device_train_batch_size=1,  # æ¯è¨­å‚™æ‰¹æ¬¡å¤§å°ï¼šæ¯å€‹è¨­å‚™ï¼ˆå¦‚ GPUï¼‰è¨“ç·´æ™‚çš„æ‰¹æ¬¡å¤§å°\n    gradient_accumulation_steps=8,  # æ¢¯åº¦ç´¯ç©æ­¥é©Ÿï¼šåœ¨åŸ·è¡Œåå‘å‚³æ’­ä¹‹å‰ç´¯ç©æ¢¯åº¦çš„æ­¥é©Ÿæ•¸\n    gradient_checkpointing=False,  # æ¢¯åº¦æª¢æŸ¥é»ï¼šä»¥è¼ƒæ…¢çš„è¨“ç·´é€Ÿåº¦ç‚ºä»£åƒ¹æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨\n    logging_steps=2,  # è¨˜éŒ„æ­¥é©Ÿï¼šè¨˜éŒ„è¨“ç·´é€²åº¦çš„é »ç‡ï¼ˆæ¯ 2 æ­¥è¨˜éŒ„ä¸€æ¬¡ï¼‰\n)\n\n# é—œéµè¶…åƒæ•¸è§£é‡‹ï¼š\nprint(\"SFT è¨“ç·´é…ç½®èªªæ˜ï¼š\")\nprint(f\"å­¸ç¿’ç‡: {sft_config.learning_rate} - æ±ºå®šæ¨¡å‹åƒæ•¸æ›´æ–°çš„å¹…åº¦\")\nprint(f\"è¨“ç·´è¼ªæ•¸: {sft_config.num_train_epochs} - å®Œæ•´éæ­·è³‡æ–™é›†çš„æ¬¡æ•¸\")\nprint(f\"æ‰¹æ¬¡å¤§å°: {sft_config.per_device_train_batch_size} - æ¯æ¬¡æ›´æ–°ä½¿ç”¨çš„æ¨£æœ¬æ•¸\")\nprint(f\"æ¢¯åº¦ç´¯ç©: {sft_config.gradient_accumulation_steps} - æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = æ‰¹æ¬¡å¤§å° Ã— æ¢¯åº¦ç´¯ç©æ­¥é©Ÿ\")\nprint(f\"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7d132-4c69-4b12-a8ec-e6b4795faad9",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": "# å»ºç«‹ SFT è¨“ç·´å™¨\nsft_trainer = SFTTrainer(\n    model=model,  # è¦è¨“ç·´çš„æ¨¡å‹\n    args=sft_config,  # è¨“ç·´é…ç½®\n    train_dataset=train_dataset,  # è¨“ç·´è³‡æ–™é›†\n    processing_class=tokenizer,  # åˆ†è©å™¨\n)\n\nprint(\"é–‹å§‹ SFT è¨“ç·´...\")\nprint(\"\\nSFT è¨“ç·´éç¨‹èªªæ˜ï¼š\")\nprint(\"1. æ¨¡å‹å°‡å­¸ç¿’æ¨¡ä»¿è¨“ç·´è³‡æ–™ä¸­çš„å›æ‡‰\")\nprint(\"2. é€éæœ€å°åŒ–è² å°æ•¸ä¼¼ç„¶æå¤±ä¾†è¨“ç·´\")\nprint(\"3. é€²åº¦æ¢å°‡é¡¯ç¤ºè¨“ç·´æ­¥é©Ÿå’Œæå¤±å€¼\")\nprint(\"4. è¨“ç·´å®Œæˆå¾Œï¼Œæ¨¡å‹å°‡èƒ½å¤ å›æ‡‰ä½¿ç”¨è€…æŒ‡ä»¤\")\n\n# åŸ·è¡Œè¨“ç·´\nsft_trainer.train()"
  },
  {
   "cell_type": "markdown",
   "id": "56b4bac1-7262-4c55-b411-6a59188157b0",
   "metadata": {},
   "source": "## æ¸¬è©¦å°å‹æ¨¡å‹å’Œå°è³‡æ–™é›†çš„è¨“ç·´çµæœ\n\n**æ³¨æ„ï¼š** ä»¥ä¸‹çµæœæ˜¯é‡å°æˆ‘å€‘åœ¨ SFT è¨“ç·´ä¸­ä½¿ç”¨çš„å°å‹æ¨¡å‹å’Œè³‡æ–™é›†ï¼Œé€™æ˜¯ç”±æ–¼è¨ˆç®—è³‡æºæœ‰é™ã€‚è‹¥è¦æŸ¥çœ‹å¤§å‹æ¨¡å‹çš„å®Œæ•´è¨“ç·´çµæœï¼Œè«‹åƒé–±ä¸Šæ–¹çš„ **ã€ŒQwen3-0.6B æ¨¡å‹çš„ SFT çµæœã€** éƒ¨åˆ†ã€‚\n\n### é æœŸçµæœåˆ†æï¼š\nç”±æ–¼æˆ‘å€‘ä½¿ç”¨çš„æ˜¯å°å‹æ¨¡å‹ï¼ˆç›¸å°è¼ƒå°‘çš„åƒæ•¸ï¼‰å’Œæœ‰é™çš„è¨“ç·´è³‡æ–™ï¼Œæ¨¡å‹çš„è¡¨ç¾å¯èƒ½æœƒæœ‰ä»¥ä¸‹ç‰¹é»ï¼š\n- **æœ‰ä¸€å®šçš„æ”¹å–„**ï¼šç›¸æ¯”æœªè¨“ç·´çš„æ¨¡å‹ï¼Œæ‡‰è©²èƒ½çœ‹åˆ°æ˜é¡¯çš„æ”¹å–„\n- **ä»æœ‰é™åˆ¶**ï¼šç”±æ–¼æ¨¡å‹è¦æ¨¡å’Œè³‡æ–™é‡çš„é™åˆ¶ï¼Œå¯èƒ½ç„¡æ³•é”åˆ°å®Œç¾çš„è¡¨ç¾\n- **å­¸ç¿’èƒ½åŠ›å±•ç¾**ï¼šå¯ä»¥è§€å¯Ÿåˆ°æ¨¡å‹é–‹å§‹å­¸æœƒå›æ‡‰æŒ‡ä»¤çš„åŸºæœ¬èƒ½åŠ›"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d678274-5768-4cea-ae20-051488e5d0f3",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": "# å¦‚æœæ²’æœ‰ä½¿ç”¨ GPUï¼Œå°‡æ¨¡å‹ç§»åˆ° CPU\nif not USE_GPU:\n    sft_trainer.model.to(\"cpu\")\n\nprint(\"\\n=== SFT è¨“ç·´å®Œæˆï¼===\\n\")\nprint(\"ç¾åœ¨ä¾†æ¸¬è©¦è¨“ç·´å¾Œçš„æ¨¡å‹è¡¨ç¾...\")\n\n# æ¸¬è©¦è¨“ç·´å¾Œçš„æ¨¡å‹\ntest_model_with_questions(sft_trainer.model, tokenizer, questions, \n                          title=\"è¨“ç·´å¾Œçš„æ¨¡å‹è¼¸å‡º\")\n\nprint(\"\\n=== è¨“ç·´æ•ˆæœåˆ†æ ===\\n\")\nprint(\"SFT è¨“ç·´çš„æ ¸å¿ƒåƒ¹å€¼ï¼š\")\nprint(\"1. å°‡åŸºç¤æ¨¡å‹è½‰æ›ç‚ºæŒ‡ä»¤è·Ÿéš¨æ¨¡å‹\")\nprint(\"2. é€éæ¨¡ä»¿ç¯„ä¾‹å›æ‡‰ä¾†å­¸ç¿’å°è©±æ¨¡å¼\")\nprint(\"3. ä½¿æ¨¡å‹èƒ½å¤ ç†è§£ä¸¦å›æ‡‰ä½¿ç”¨è€…çš„å„ç¨®è«‹æ±‚\")\nprint(\"\\nå³ä½¿åœ¨æœ‰é™çš„è³‡æºä¸‹ï¼Œæˆ‘å€‘ä¹Ÿèƒ½è§€å¯Ÿåˆ°æ¨¡å‹åœ¨æŒ‡ä»¤è·Ÿéš¨æ–¹é¢çš„æ”¹å–„ï¼\")"
  },
  {
   "cell_type": "markdown",
   "id": "bogag1lqd7",
   "source": "## èª²ç¨‹ç¸½çµèˆ‡æ·±å…¥æ€è€ƒ\n\n### æˆ‘å€‘åœ¨é€™å€‹èª²ç¨‹ä¸­å­¸åˆ°äº†ä»€éº¼ï¼š\n\n#### 1. SFT çš„æ ¸å¿ƒæ¦‚å¿µ\n- **å®šç¾©**ï¼šç›£ç£å¼å¾®èª¿ï¼ˆSFTï¼‰æ˜¯ä¸€ç¨®å°‡åŸºç¤èªè¨€æ¨¡å‹è½‰æ›ç‚ºèƒ½å¤ éµå¾ªæŒ‡ä»¤çš„å°è©±æ¨¡å‹çš„æŠ€è¡“\n- **åŸç†**ï¼šé€šéæœ€å°åŒ–è² å°æ•¸ä¼¼ç„¶æå¤±å‡½æ•¸ï¼Œè®“æ¨¡å‹å­¸ç¿’æ¨¡ä»¿è¨“ç·´è³‡æ–™ä¸­çš„å›æ‡‰\n- **ç›®æ¨™**ï¼šä½¿æ¨¡å‹èƒ½å¤ ç†è§£ä¸¦é©ç•¶å›æ‡‰ä½¿ç”¨è€…çš„å„ç¨®æŒ‡ä»¤å’Œå•é¡Œ\n\n#### 2. SFT çš„æ•¸å­¸åŸºç¤\n- **æå¤±å‡½æ•¸**ï¼š`Loss = -log P(response | prompt)`\n- **è¨“ç·´ç›®æ¨™**ï¼šæœ€å¤§åŒ–çµ¦å®šæç¤ºä¸‹ç”Ÿæˆæ­£ç¢ºå›æ‡‰çš„æ¦‚ç‡\n- **å¯¦ç¾æ–¹å¼**ï¼šå°æ‰€æœ‰è¨“ç·´æ¨£æœ¬çš„æå¤±æ±‚å’Œä¸¦é€²è¡Œæ¢¯åº¦ä¸‹é™\n\n#### 3. è³‡æ–™å“è³ªçš„é‡è¦æ€§\n- **å“è³ªå‹æ–¼æ•¸é‡**ï¼š1000å€‹é«˜å“è³ªæ¨£æœ¬å¾€å¾€æ¯”100è¬å€‹æ··åˆå“è³ªçš„æ¨£æœ¬æ•ˆæœæ›´å¥½\n- **è³‡æ–™æ•´ç†æ–¹æ³•**ï¼š\n  - è’¸é¤¾ï¼šä½¿ç”¨è¼ƒå¤§æ¨¡å‹ç”Ÿæˆé«˜å“è³ªå›æ‡‰\n  - æœ€ä½³ k é¸æ“‡ï¼šå¾å¤šå€‹ç”Ÿæˆçµæœä¸­é¸æ“‡æœ€ä½³å›æ‡‰\n  - ç¯©é¸ï¼šæ ¹æ“šå“è³ªå’Œå¤šæ¨£æ€§ç¯©é¸å¤§è¦æ¨¡è³‡æ–™é›†\n\n#### 4. å¯¦éš›æ‡‰ç”¨å ´æ™¯\n- **æ¨¡å‹è¡Œç‚ºå•Ÿå‹•**ï¼šå°‡é è¨“ç·´æ¨¡å‹è½‰ç‚ºæŒ‡ä»¤æ¨¡å‹\n- **èƒ½åŠ›æ”¹å–„**ï¼šæå‡ç‰¹å®šä»»å‹™çš„è¡¨ç¾\n- **çŸ¥è­˜è’¸é¤¾**ï¼šå°‡å¤§æ¨¡å‹çš„èƒ½åŠ›è½‰ç§»åˆ°å°æ¨¡å‹\n\n### é‹ç®—ç’°å¢ƒé¸æ“‡å»ºè­°\n\né—œæ–¼æ‚¨æåˆ°çš„é‹ç®—ç’°å¢ƒé¸æ“‡å•é¡Œï¼Œè®“æˆ‘ç‚ºæ‚¨åˆ†æï¼š\n\n#### Kaggle GPU vs Ubuntu RTX3080 é¸æ“‡ï¼š\n\n**Kaggle å„ªå‹¢ï¼š**\n- å…è²»çš„ GPU è³‡æºï¼ˆæ¯é€±ç´„ 30 å°æ™‚ï¼‰\n- ç„¡éœ€æœ¬åœ°è¨­ç½®ï¼Œå³é–‹å³ç”¨\n- é è£å¤§éƒ¨åˆ†æ·±åº¦å­¸ç¿’å¥—ä»¶\n- æ–¹ä¾¿çš„ç­†è¨˜æœ¬åˆ†äº«å’Œç‰ˆæœ¬æ§åˆ¶\n\n**Kaggle åŠ£å‹¢ï¼š**\n- æª”æ¡ˆç®¡ç†ç›¸å°ä¸ä¾¿\n- çµ‚ç«¯æ“ä½œå—é™\n- æ™‚é–“é™åˆ¶ï¼ˆæ¯æ¬¡æœ€å¤š 12 å°æ™‚ï¼‰\n- ç¶²è·¯é™åˆ¶å¯èƒ½å½±éŸ¿å¤§æ¨¡å‹ä¸‹è¼‰\n\n**Ubuntu RTX3080 å„ªå‹¢ï¼š**\n- å®Œå…¨æ§åˆ¶æ¬Šï¼Œç„¡æ™‚é–“é™åˆ¶\n- æ›´å¥½çš„æª”æ¡ˆç®¡ç†å’Œçµ‚ç«¯æ“ä½œ\n- å¯ä»¥é€²è¡Œé•·æ™‚é–“è¨“ç·´\n- æœ¬åœ°å„²å­˜ï¼Œç„¡éœ€é‡è¤‡ä¸‹è¼‰\n\n**Ubuntu RTX3080 åŠ£å‹¢ï¼š**\n- éœ€è¦è‡ªå·±è¨­ç½®ç’°å¢ƒ\n- é›»è²»å’Œç¡¬é«”æè€—\n- éœ€è¦ç¶­è­·å’Œæ›´æ–°\n\n#### å…·é«”å»ºè­°ï¼š\n\n1. **å°æ–¼æœ¬èª²ç¨‹çš„ SFT è¨“ç·´**ï¼š\n   - å¦‚æœåªæ˜¯å­¸ç¿’å’Œå¯¦é©—ï¼ŒKaggle å®Œå…¨å¤ ç”¨\n   - RTX3080 10GB å°æ–¼ Qwen3-0.6B æ¨¡å‹æ˜¯å……è¶³çš„\n   - å¯ä»¥å…©è€…çµåˆä½¿ç”¨ï¼šKaggle åšåˆæ­¥å¯¦é©—ï¼ŒUbuntu åšæ­£å¼è¨“ç·´\n\n2. **é¸æ“‡ä¾æ“š**ï¼š\n   - **å­¸ç¿’ç›®çš„**ï¼šå„ªå…ˆé¸æ“‡ Kaggle\n   - **ç”Ÿç”¢ç›®çš„**ï¼šå»ºè­°ä½¿ç”¨ Ubuntu RTX3080\n   - **å¤§æ¨¡å‹è¨“ç·´**ï¼šUbuntu RTX3080 æ›´é©åˆ\n\n3. **æœ€ä½³å¯¦è¸**ï¼š\n   - åœ¨ Kaggle ä¸Šå¿«é€ŸåŸå‹å’Œæ¸¬è©¦\n   - åœ¨æœ¬åœ° Ubuntu ä¸Šé€²è¡Œå®Œæ•´è¨“ç·´\n   - ä½¿ç”¨ Git åŒæ­¥ä»£ç¢¼å’Œé…ç½®\n\n### ä¸‹ä¸€æ­¥å­¸ç¿’å»ºè­°\n\n1. **æ·±å…¥ç†è§£ SFT è®Šé«”**ï¼š\n   - ç ”ç©¶ LoRAã€QLoRA ç­‰åƒæ•¸æ•ˆç‡å¾®èª¿æ–¹æ³•\n   - äº†è§£æŒ‡ä»¤å·¥ç¨‹å’Œæç¤ºè¨­è¨ˆ\n\n2. **æ¢ç´¢é€²éšæŠ€è¡“**ï¼š\n   - ç›´æ¥åå¥½å„ªåŒ–ï¼ˆDPOï¼‰\n   - å¼·åŒ–å­¸ç¿’äººé¡åé¥‹ï¼ˆRLHFï¼‰\n   - å¤šè¼ªå°è©±è¨“ç·´\n\n3. **å¯¦è¸é …ç›®**ï¼š\n   - å˜—è©¦åœ¨ä¸åŒé ˜åŸŸçš„è³‡æ–™é›†ä¸Šé€²è¡Œ SFT\n   - æ¯”è¼ƒä¸åŒè¶…åƒæ•¸è¨­ç½®çš„æ•ˆæœ\n   - è©•ä¼°æ¨¡å‹åœ¨å„ç¨®ä»»å‹™ä¸Šçš„è¡¨ç¾\n\nç›£ç£å¼å¾®èª¿æ˜¯ç¾ä»£ AI ç³»çµ±çš„æ ¸å¿ƒæŠ€è¡“ä¹‹ä¸€ï¼ŒæŒæ¡å®ƒå°‡ç‚ºæ‚¨åœ¨ AI é ˜åŸŸçš„ç™¼å±•æ‰“ä¸‹å …å¯¦åŸºç¤ï¼",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}