{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc935f2b-a58b-4110-8918-96c868047b38",
   "metadata": {},
   "source": [
    "# L3: Supervised Fine-Tuning (SFT) ç›£ç£å¼å¾®èª¿\n",
    "\n",
    "## èª²ç¨‹æ¦‚è¿°\n",
    "\n",
    "åœ¨é€™å€‹èª²ç¨‹ä¸­ï¼Œæˆ‘å€‘å°‡å­¸ç¿’ç›£ç£å¼å¾®èª¿ï¼ˆSFTï¼‰çš„åŸºæœ¬æ¦‚å¿µå’Œå¯¦ä½œæ–¹æ³•ã€‚SFTæ˜¯ä¸€ç¨®å°‡åŸºç¤èªè¨€æ¨¡å‹è½‰æ›ç‚ºèƒ½å¤ éµå¾ªæŒ‡ä»¤çš„å°è©±æ¨¡å‹çš„é‡è¦æŠ€è¡“ã€‚\n",
    "\n",
    "### ä¸»è¦å­¸ç¿’ç›®æ¨™ï¼š\n",
    "1. **ç†è§£ SFT çš„åŸºæœ¬åŸç†**ï¼šå­¸ç¿’å¦‚ä½•é€éæ¨¡ä»¿ç¯„ä¾‹å›æ‡‰ä¾†è¨“ç·´æ¨¡å‹\n",
    "2. **æŒæ¡ SFT çš„å·¥ä½œæµç¨‹**ï¼šå¾è³‡æ–™æº–å‚™åˆ°æ¨¡å‹è¨“ç·´çš„å®Œæ•´éç¨‹\n",
    "3. **å¯¦ä½œ SFT è¨“ç·´**ï¼šä½¿ç”¨çœŸå¯¦è³‡æ–™é›†é€²è¡Œæ¨¡å‹å¾®èª¿\n",
    "4. **æ¯”è¼ƒè¨“ç·´å‰å¾Œçš„æ•ˆæœ**ï¼šè§€å¯Ÿæ¨¡å‹åœ¨å¾®èª¿å‰å¾Œçš„å·®ç•°\n",
    "\n",
    "### èª²ç¨‹é‡é»ï¼š\n",
    "- **SFT çš„æ•¸å­¸åŸç†**ï¼šè² å°æ•¸ä¼¼ç„¶æå¤±å‡½æ•¸çš„æœ€å°åŒ–\n",
    "- **è³‡æ–™å“è³ªçš„é‡è¦æ€§**ï¼šé«˜å“è³ªè³‡æ–™æ¯”å¤§é‡è³‡æ–™æ›´é‡è¦\n",
    "- **åƒæ•¸æ•ˆç‡å¾®èª¿**ï¼šLoRA ç­‰æŠ€è¡“çš„æ‡‰ç”¨\n",
    "- **å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹**ï¼šå¾åŸºç¤æ¨¡å‹åˆ°æŒ‡ä»¤æ¨¡å‹çš„è½‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7248ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/sheep52031/DeepLearning.AI_SelfStudy-Notes.git\n",
    "# !ls\n",
    "# %cd DeepLearning.AI_SelfStudy-Notes\n",
    "\n",
    "import os\n",
    "base_dir = 'DeepLearning.AI_SelfStudy-Notes/Post-training_of_LLMs'\n",
    "\n",
    "# !pip install -r Post-training_of_LLMs/requirements.txt --no-deps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfc986-9ac7-4a2d-9dd0-a76841c7f46d",
   "metadata": {},
   "source": [
    "## åŒ¯å…¥å¿…è¦çš„å‡½å¼åº«\n",
    "\n",
    "é€™å€‹éƒ¨åˆ†æˆ‘å€‘å°‡åŒ¯å…¥é€²è¡Œ SFT è¨“ç·´æ‰€éœ€çš„æ ¸å¿ƒå‡½å¼åº«ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3304e49d-bd1e-469b-a5b4-5edb16ecf344",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "import torch  # PyTorch æ·±åº¦å­¸ç¿’æ¡†æ¶\n",
    "import pandas as pd  # è³‡æ–™è™•ç†å’Œåˆ†æ\n",
    "from datasets import load_dataset, Dataset  # HuggingFace è³‡æ–™é›†è¼‰å…¥\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM  # æ¨¡å‹å’Œåˆ†è©å™¨\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig  # SFT è¨“ç·´å·¥å…·"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc63b02-5e9a-4a83-b042-4a2386cf5976",
   "metadata": {},
   "source": [
    "## è¨­å®šè¼”åŠ©å‡½å¼\n",
    "\n",
    "é€™äº›è¼”åŠ©å‡½å¼å°‡å¹«åŠ©æˆ‘å€‘é€²è¡Œæ¨¡å‹è¼‰å…¥ã€å›æ‡‰ç”Ÿæˆå’Œæ¸¬è©¦ç­‰æ“ä½œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69982ae0-755e-48cf-ba4c-3b83b091fd9a",
   "metadata": {
    "height": 557
   },
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, user_message, system_message=None, \n",
    "                       max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæ¨¡å‹å›æ‡‰çš„å‡½å¼ï¼ˆä¿®å¾©é‡è¤‡å•é¡Œç‰ˆæœ¬ï¼‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    - model: èªè¨€æ¨¡å‹\n",
    "    - tokenizer: åˆ†è©å™¨\n",
    "    - user_message: ä½¿ç”¨è€…è¨Šæ¯\n",
    "    - system_message: ç³»çµ±è¨Šæ¯ï¼ˆå¯é¸ï¼‰\n",
    "    - max_new_tokens: ç”Ÿæˆçš„æœ€å¤§æ–°è©å…ƒæ•¸é‡\n",
    "    \n",
    "    è¿”å›:\n",
    "    - response: æ¨¡å‹ç”Ÿæˆçš„å›æ‡‰\n",
    "    \"\"\"\n",
    "    # ä½¿ç”¨åˆ†è©å™¨çš„èŠå¤©æ¨¡æ¿æ ¼å¼åŒ–å°è©±\n",
    "    messages = []\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    \n",
    "    # æˆ‘å€‘å‡è¨­è³‡æ–™éƒ½æ˜¯å–®è¼ªå°è©±\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "    # æ‡‰ç”¨èŠå¤©æ¨¡æ¿\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "\n",
    "    # å°‡æç¤ºè½‰æ›ç‚ºæ¨¡å‹è¼¸å…¥æ ¼å¼\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # ç”Ÿæˆå›æ‡‰ï¼ˆæ·»åŠ é‡è¤‡æ‡²ç½°å’Œæº«åº¦æ§åˆ¶ï¼‰\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,  # æ”¹ç‚ºTrueä»¥å¢åŠ å¤šæ¨£æ€§\n",
    "            temperature=0.7,  # æ·»åŠ æº«åº¦æ§åˆ¶\n",
    "            top_p=0.9,  # æ·»åŠ top-pæ¡æ¨£\n",
    "            repetition_penalty=1.2,  # é—œéµï¼šæ·»åŠ é‡è¤‡æ‡²ç½°\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # æå–ç”Ÿæˆçš„éƒ¨åˆ†\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    generated_ids = outputs[0][input_len:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234e5b05-a493-4683-91fd-7417885efc0f",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "def test_model_with_questions(model, tokenizer, questions, \n",
    "                              system_message=None, title=\"Model Output\"):\n",
    "    \"\"\"\n",
    "    æ¸¬è©¦æ¨¡å‹å°ä¸€ç³»åˆ—å•é¡Œçš„å›æ‡‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    - model: èªè¨€æ¨¡å‹\n",
    "    - tokenizer: åˆ†è©å™¨\n",
    "    - questions: å•é¡Œåˆ—è¡¨\n",
    "    - system_message: ç³»çµ±è¨Šæ¯ï¼ˆå¯é¸ï¼‰\n",
    "    - title: è¼¸å‡ºæ¨™é¡Œ\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        response = generate_responses(model, tokenizer, question, \n",
    "                                      system_message)\n",
    "        print(f\"\\nModel Input {i}:\\n{question}\\nModel Output {i}:\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c273931-6827-4ee1-af1a-83a99bf94bf7",
   "metadata": {
    "height": 387
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, use_gpu = False):\n",
    "    \"\"\"\n",
    "    è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨ï¼ˆä¿®å¾©ç‰ˆï¼‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    - model_name: æ¨¡å‹åç¨±æˆ–è·¯å¾‘\n",
    "    - use_gpu: æ˜¯å¦ä½¿ç”¨ GPU\n",
    "    \n",
    "    è¿”å›:\n",
    "    - model: è¼‰å…¥çš„æ¨¡å‹\n",
    "    - tokenizer: è¼‰å…¥çš„åˆ†è©å™¨\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    import torch\n",
    "    \n",
    "    # æ¸…ç† GPU è¨˜æ†¶é«”\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # è¼‰å…¥åˆ†è©å™¨\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # è¼‰å…¥æ¨¡å‹\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        try:\n",
    "            model.to(\"cuda\")\n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            print(f\"âš ï¸  GPU è¨˜æ†¶é«”ä¸è¶³ï¼Œå°‡ä½¿ç”¨ CPU: {e}\")\n",
    "            model.to(\"cpu\")\n",
    "            use_gpu = False\n",
    "    \n",
    "    # ä¿®å¾©åˆ†è©å™¨é…ç½® - é€™æ˜¯é—œéµä¿®å¾©\n",
    "    if not tokenizer.chat_template:\n",
    "        tokenizer.chat_template = \"\"\"{% for message in messages %}{% if message['role'] == 'system' %}System: {{ message['content'] }}\\n{% elif message['role'] == 'user' %}User: {{ message['content'] }}\\nAssistant:{% elif message['role'] == 'assistant' %} {{ message['content'] }}{% if not loop.last %}\\n{% endif %}{% endif %}{% endfor %}{% if add_generation_prompt %} {% endif %}\"\"\"\n",
    "    \n",
    "    # é‡è¦ï¼šæ­£ç¢ºè¨­ç½® tokenizer çš„ç‰¹æ®Š token\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # ç¢ºä¿ tokenizer æœ‰æ­£ç¢ºçš„ pad_token_id\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    print(f\"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œä½¿ç”¨è¨­å‚™: {'GPU' if use_gpu else 'CPU'}\")\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        print(f\"GPU è¨˜æ†¶é«”ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f}GB\")\n",
    "        \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd15e1d-6ecd-4337-a5dd-1602da354f62",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "def display_dataset(dataset):\n",
    "    \"\"\"\n",
    "    é¡¯ç¤ºè³‡æ–™é›†çš„å‰å¹¾å€‹ç¯„ä¾‹\n",
    "    \n",
    "    åƒæ•¸:\n",
    "    - dataset: è¦é¡¯ç¤ºçš„è³‡æ–™é›†\n",
    "    \"\"\"\n",
    "    # è¦–è¦ºåŒ–è³‡æ–™é›†\n",
    "    rows = []\n",
    "    for i in range(3):\n",
    "        example = dataset[i]\n",
    "        # æå–ä½¿ç”¨è€…è¨Šæ¯\n",
    "        user_msg = next(m['content'] for m in example['messages']\n",
    "                        if m['role'] == 'user')\n",
    "        # æå–åŠ©æ‰‹å›æ‡‰\n",
    "        assistant_msg = next(m['content'] for m in example['messages']\n",
    "                             if m['role'] == 'assistant')\n",
    "        rows.append({\n",
    "            'User Prompt': user_msg,\n",
    "            'Assistant Response': assistant_msg\n",
    "        })\n",
    "    \n",
    "    # é¡¯ç¤ºç‚ºè¡¨æ ¼\n",
    "    df = pd.DataFrame(rows)\n",
    "    pd.set_option('display.max_colwidth', None)  # é¿å…æˆªæ–·é•·å­—ä¸²\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ac817-43a4-43c9-88f3-9825b96b84b7",
   "metadata": {},
   "source": [
    "## è¼‰å…¥åŸºç¤æ¨¡å‹ä¸¦æ¸¬è©¦ç°¡å–®å•é¡Œ\n",
    "\n",
    "é¦–å…ˆæˆ‘å€‘è¼‰å…¥ä¸€å€‹åŸºç¤æ¨¡å‹ï¼ˆæœªç¶“éæŒ‡ä»¤å¾®èª¿çš„æ¨¡å‹ï¼‰ï¼Œä¸¦æ¸¬è©¦å®ƒå°ç°¡å–®å•é¡Œçš„å›æ‡‰èƒ½åŠ›ã€‚é€™å°‡å¹«åŠ©æˆ‘å€‘ç†è§£ SFT å‰å¾Œçš„å·®ç•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fed78c2-ea93-4ac2-bd6f-5d4391de7c8d",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# è¨­å®šæ˜¯å¦ä½¿ç”¨ GPUï¼ˆåœ¨ Kaggle ç’°å¢ƒä¸­è¨­ç‚º Trueï¼‰\n",
    "USE_GPU = True\n",
    "\n",
    "# å®šç¾©æ¸¬è©¦å•é¡Œ\n",
    "questions = [\n",
    "    \"Give me an 1-sentence introduction of LLM.\",  # è¦æ±‚ç°¡çŸ­ä»‹ç´¹ LLM\n",
    "    \"Calculate 1+1-1\",  # ç°¡å–®æ•¸å­¸è¨ˆç®—\n",
    "    \"What's the difference between thread and process?\"  # æŠ€è¡“æ¦‚å¿µè§£é‡‹\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf3d51af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaren/miniconda3/envs/post-training-llms/lib/python3.10/site-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ec086db6f3490987c2a20ffe8cf691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/jaren/DeepLearning.AI_SelfStudy-Notes/Post-training_of_LLMs/Qwen3-0.6B-Base'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¾ HuggingFace Hub ä¸‹è¼‰ Qwen3-0.6B-Base æ¨¡å‹\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# ä¸‹è¼‰æ¨¡å‹åˆ°æœ¬åœ°ç›®éŒ„\n",
    "snapshot_download(\n",
    "    repo_id=\"Qwen/Qwen3-0.6B-Base\", # æŒ‡å®šè¦ä¸‹è¼‰çš„æ¨¡å‹ï¼ˆQwen3 0.6B åŸºç¤ç‰ˆæœ¬ï¼‰\n",
    "    local_dir=\"./Qwen3-0.6B-Base\", # è¨­å®šæœ¬åœ°å„²å­˜è·¯å¾‘\n",
    "    local_dir_use_symlinks=False  # é—œé–‰ç¬¦è™Ÿé€£çµï¼Œç¢ºä¿æª”æ¡ˆå®Œæ•´è¤‡è£½\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086a902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ Transformers åº«ä¸­çš„è‡ªå‹•åˆ†è©å™¨å’Œå› æœèªè¨€æ¨¡å‹\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# è¨­å®šæ¨¡å‹ç›®éŒ„è·¯å¾‘\n",
    "local_dir = \"Qwen3-0.6B-Base\"\n",
    "\n",
    "# è¼‰å…¥åˆ†è©å™¨ï¼ˆè² è²¬å°‡æ–‡å­—è½‰æ›ç‚ºæ¨¡å‹å¯ç†è§£çš„æ•¸å­—åºåˆ—ï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir, trust_remote_code=True) # ä¿¡ä»»é ç«¯ç¨‹å¼ç¢¼ï¼Œå…è¨±åŸ·è¡Œæ¨¡å‹è‡ªå®šç¾©çš„ç¨‹å¼ç¢¼\n",
    "\n",
    "# è¼‰å…¥èªè¨€æ¨¡å‹ï¼ˆè² è²¬ç”Ÿæˆæ–‡å­—å›æ‡‰ï¼‰\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir, trust_remote_code=True) # ä¿¡ä»»é ç«¯ç¨‹å¼ç¢¼ï¼Œç¢ºä¿æ¨¡å‹èƒ½æ­£å¸¸è¼‰å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba426c74-4d93-42b3-b2c7-5791fb9bf3c5",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (Before SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "â‹…\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "âšˆ âš‡\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "Ù…ÙØ§ÙˆØ¶Ø§Øª\n",
      "å“ªä¸ªæ˜¯æ­£ç¡®å•è¯ï¼Œä¸ºä»€ä¹ˆï¼Ÿ\n",
      "### Thread å’Œ Process çš„åŒºåˆ«\n",
      "\n",
      "**Thread**: åœ¨è®¡ç®—æœºç§‘å­¦ä¸­ï¼Œç‰¹åˆ«æ˜¯æ“ä½œç³»ç»Ÿé¢†åŸŸï¼Œthread æ˜¯ä¸€ç§è½»é‡çº§çš„æ‰§è¡Œå•å…ƒã€‚å®ƒå…è®¸ç¨‹åºåŒæ—¶è¿è¡Œå¤šä¸ªçº¿ç¨‹ï¼ˆå³å¹¶è¡Œä»»åŠ¡ï¼‰ï¼Œæ¯ä¸ªçº¿ç¨‹å¯ä»¥ç‹¬ç«‹åœ°è·å–CPUèµ„æºï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„æ—¶é—´ç‚¹ä¸Šå®Œæˆè®¡ç®—æˆ–I/Oæ“ä½œã€‚\n",
      "\n",
      "- **ç‰¹ç‚¹**:\n",
      "  - æ›´å°ã€æ›´é«˜æ•ˆï¼šç”±äºæ˜¯é€šè¿‡å…±äº«å†…å­˜å’Œä¿¡å·æ¥åŒæ­¥é€šä¿¡è€Œéç›´æ¥\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_model_with_questions(model, tokenizer, questions, \n",
    "                          title=\"Base Model (Before SFT) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885253e0-3a9a-4b42-a36b-a8a3ddd340a1",
   "metadata": {},
   "source": [
    "## Qwen3-0.6B æ¨¡å‹çš„ SFT çµæœ\n",
    "\n",
    "åœ¨é€™å€‹éƒ¨åˆ†ï¼Œæˆ‘å€‘å°‡æª¢è¦–å…ˆå‰å®Œæˆçš„ SFT è¨“ç·´çµæœã€‚ç”±æ–¼è³‡æºé™åˆ¶ï¼Œæˆ‘å€‘ä¸æœƒåœ¨åƒ Qwen3-0.6B é€™æ¨£ç›¸å°è¼ƒå¤§çš„æ¨¡å‹ä¸Šé€²è¡Œå®Œæ•´è¨“ç·´ã€‚\n",
    "\n",
    "### å°æ¯”åˆ†æï¼š\n",
    "- **åŸºç¤æ¨¡å‹ï¼ˆSFTå‰ï¼‰**ï¼šåªæœƒç”Ÿæˆéš¨æ©Ÿç¬¦è™Ÿï¼Œç„¡æ³•ç†è§£æŒ‡ä»¤\n",
    "- **å¾®èª¿æ¨¡å‹ï¼ˆSFTå¾Œï¼‰**ï¼šèƒ½å¤ ç†è§£ä¸¦å›æ‡‰ä½¿ç”¨è€…çš„å•é¡Œ\n",
    "\n",
    "é€™å€‹å°æ¯”æ¸…æ¥šåœ°å±•ç¤ºäº† SFT çš„å¨åŠ› - å®ƒèƒ½å°‡ä¸€å€‹åªæœƒé æ¸¬ä¸‹ä¸€å€‹è©çš„åŸºç¤æ¨¡å‹ï¼Œè½‰æ›ç‚ºèƒ½å¤ é€²è¡Œæœ‰æ„ç¾©å°è©±çš„åŠ©æ‰‹æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8f6f92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaff4ddb25794541b6116d9ca5eea1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/jaren/DeepLearning.AI_SelfStudy-Notes/Post-training_of_LLMs/Qwen3-0.6B-SFT'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"banghua/Qwen3-0.6B-SFT\",\n",
    "    local_dir=\"./Qwen3-0.6B-SFT\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e86f13c-c969-4c7e-8702-d074ee7a2ce6",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œä½¿ç”¨è¨­å‚™: GPU\n",
      "GPU è¨˜æ†¶é«”ä½¿ç”¨: 2.22GB / 9.77GB\n",
      "\n",
      "=== Base Model (After SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "LLM is a program that offers advanced degrees in law and provides students with practical experience through real-world case studies.\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "First, we need to calculate the value of each expression in the given equation:\n",
      "\n",
      "1. The value of \"1 + 1 - 1\" is (1 + 1) - 1 = 2 - 1 = 1.\n",
      "2. The value of \"1 - 1 + 1\" is (1 - 1) + 1 = 0 + 1 = 1.\n",
      "\n",
      "Now, we can substitute these values into the original equation and simplify:\n",
      "\n",
      "(1\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "The main difference between a thread and a process is that a thread is a unit of execution in a program, while a process is an instance of a running program. Threads are created inside processes to allow for concurrent execution of different parts of a program or applications.\n",
      "\n",
      "For example:\n",
      "- A single-threaded application (e.g., a web browser) consists of only one thread.\n",
      "- An application that can run multiple threads is called multithreaded software (e.g., Java).\n",
      "- Each time you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\"./Qwen3-0.6B-SFT\", USE_GPU)\n",
    "\n",
    "test_model_with_questions(model, tokenizer, questions, \n",
    "                          title=\"Base Model (After SFT) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf299ee-aa84-4c43-8d7b-f0998077e2cb",
   "metadata": {},
   "source": [
    "## åœ¨å°å‹æ¨¡å‹ä¸Šé€²è¡Œ SFT è¨“ç·´\n",
    "\n",
    "æ¥ä¸‹ä¾†æˆ‘å€‘å°‡å¯¦éš›é€²è¡Œ SFT è¨“ç·´çš„å®Œæ•´æµç¨‹ã€‚æˆ‘å€‘å°‡ä½¿ç”¨ä¸€å€‹è¼ƒå°çš„æ¨¡å‹å’Œè³‡æ–™é›†ä¾†ç¢ºä¿è¨“ç·´éç¨‹èƒ½åœ¨æœ‰é™çš„è¨ˆç®—è³‡æºä¸ŠåŸ·è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5cb7ea-e157-418e-84f5-34ecbed823ad",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ğŸ’» &nbsp; <b>æ³¨æ„ï¼š</b> æˆ‘å€‘åœ¨å°å‹æ¨¡å‹ <code>HuggingFaceTB/SmolLM2-135M</code> å’Œè¼ƒå°çš„è¨“ç·´è³‡æ–™é›†ä¸Šé€²è¡Œ SFTï¼Œä»¥ç¢ºä¿å®Œæ•´çš„è¨“ç·´éç¨‹èƒ½åœ¨æœ‰é™çš„è¨ˆç®—è³‡æºä¸Šé‹è¡Œã€‚å¦‚æœä½ åœ¨è‡ªå·±çš„æ©Ÿå™¨ä¸Šé‹è¡Œç­†è¨˜æœ¬ä¸¦ä¸”æœ‰ GPU è³‡æºï¼Œå¯ä»¥åˆ‡æ›åˆ°æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚ <code>Qwen/Qwen3-0.6B-Base</code>ï¼‰ä¾†é€²è¡Œå®Œæ•´çš„ SFT è¨“ç·´ä¸¦é‡ç¾ä¸Šè¿°çµæœã€‚</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdb07589-049d-432e-8001-e6e9175ad806",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afa382a3da24b1f81a856b0cd3aefc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œä½¿ç”¨è¨­å‚™: CPU\n"
     ]
    }
   ],
   "source": [
    "# ä¸‹è¼‰ HuggingFaceTB/SmolLM2-135M æ¨¡å‹\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# ä¸‹è¼‰å°æ¨¡å‹åˆ°æœ¬åœ°ç›®éŒ„\n",
    "snapshot_download(\n",
    "    repo_id=\"HuggingFaceTB/SmolLM2-135M\",\n",
    "    local_dir=\"./models/HuggingFaceTB/SmolLM2-135M\",\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨å°æ¨¡å‹ä¸¦è¨­å®šç‚ºCPUæ¨¡å¼ä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ\n",
    "model_name = \"./models/HuggingFaceTB/SmolLM2-135M\"\n",
    "USE_GPU = False  # æ”¹ç‚ºFalseä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, USE_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20d52c3e-9c6c-43c3-bd95-92d60b9c9a8f",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨“ç·´è³‡æ–™é›†ç¯„ä¾‹ï¼š\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Prompt</th>\n",
       "      <th>Assistant Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- The left child should have a value less than the parent node's value, and the right child should have a value greater than the parent node's value.</td>\n",
       "      <td>This statement is correct. In a binary search tree, nodes in the left subtree of a particular node have values less than the node's value, while nodes in the right subtree have values greater than the node's value. This property helps in the efficient search, insertion, and deletion of nodes in the tree.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To pass three levels must be the plan.\\nThen tackle Two, when that is done.\\nOf 100 that start, at the end will be 20.\\nFinQuiz is a website that helps you prepare.\\nUse it to be stress-free, and not lose your hair.\\nThen, take the exam with a smile on your face.\\nBe confident that you will gain your place.\\nSo make this the goal to which you aspire. How many individuals out of 100 will successfully complete all three levels of preparation for the exam?</td>\n",
       "      <td>Based on the given information, out of 100 individuals who start, only 20 will make it to the end. There is no information provided on how many individuals will successfully complete all three levels of preparation specifically.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you translate the text material into Spanish or any other language?: He really is, you know.\\nThings a hero should show.\\nHe loves me more than a zillion things.\\nHe loves me when I sing my jolly folktale rhymes.\\nHe's good, not just good, in fact he's great!\\nBut because he's my best mate!\\nWOW !!! I love it!!!!</td>\n",
       "      <td>Â¿Puede traducir el texto a espaÃ±ol o a cualquier otro idioma?: \\nRealmente lo es, ya sabes.\\nCosas que un hÃ©roe deberÃ­a demostrar.\\nMe quiere mÃ¡s que un millÃ³n de cosas.\\nMe quiere cuando canto mis alegres rimas de cuentos populares.\\nEs bueno, no solo bueno, Â¡de hecho es genial!\\nÂ¡Pero porque es mi mejor amigo!\\nÂ¡WOW! Â¡Me encanta!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                 User Prompt  \\\n",
       "0                                                                                                                                                                                                                                                                                                                      - The left child should have a value less than the parent node's value, and the right child should have a value greater than the parent node's value.   \n",
       "1  To pass three levels must be the plan.\\nThen tackle Two, when that is done.\\nOf 100 that start, at the end will be 20.\\nFinQuiz is a website that helps you prepare.\\nUse it to be stress-free, and not lose your hair.\\nThen, take the exam with a smile on your face.\\nBe confident that you will gain your place.\\nSo make this the goal to which you aspire. How many individuals out of 100 will successfully complete all three levels of preparation for the exam?   \n",
       "2                                                                                                                                             Can you translate the text material into Spanish or any other language?: He really is, you know.\\nThings a hero should show.\\nHe loves me more than a zillion things.\\nHe loves me when I sing my jolly folktale rhymes.\\nHe's good, not just good, in fact he's great!\\nBut because he's my best mate!\\nWOW !!! I love it!!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                              Assistant Response  \n",
       "0                              This statement is correct. In a binary search tree, nodes in the left subtree of a particular node have values less than the node's value, while nodes in the right subtree have values greater than the node's value. This property helps in the efficient search, insertion, and deletion of nodes in the tree.  \n",
       "1                                                                                                           Based on the given information, out of 100 individuals who start, only 20 will make it to the end. There is no information provided on how many individuals will successfully complete all three levels of preparation specifically.  \n",
       "2  Â¿Puede traducir el texto a espaÃ±ol o a cualquier otro idioma?: \\nRealmente lo es, ya sabes.\\nCosas que un hÃ©roe deberÃ­a demostrar.\\nMe quiere mÃ¡s que un millÃ³n de cosas.\\nMe quiere cuando canto mis alegres rimas de cuentos populares.\\nEs bueno, no solo bueno, Â¡de hecho es genial!\\nÂ¡Pero porque es mi mejor amigo!\\nÂ¡WOW! Â¡Me encanta!  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è³‡æ–™é›†å¤§å°ï¼š2000 å€‹æ¨£æœ¬\n",
      "\n",
      "è³‡æ–™é›†ç‰¹é»ï¼š\n",
      "- åŒ…å«å¤šæ¨£åŒ–çš„æŒ‡ä»¤å’Œå›æ‡‰å°\n",
      "- æ¶µè“‹å•ç­”ã€ç¿»è­¯ã€è¨ˆç®—ç­‰å¤šç¨®ä»»å‹™\n",
      "- æ¯å€‹æ¨£æœ¬éƒ½åŒ…å«ä½¿ç”¨è€…æç¤ºå’ŒåŠ©æ‰‹å›æ‡‰\n",
      "- å¢åŠ åˆ°500å€‹æ¨£æœ¬ä»¥æ”¹å–„å°æ¨¡å‹è¨“ç·´æ•ˆæœ\n",
      "- å°æ¨¡å‹éœ€è¦æ›´å¤šæ¨£æœ¬æ‰èƒ½å­¸åˆ°è‰¯å¥½çš„å°è©±æ¨¡å¼\n"
     ]
    }
   ],
   "source": [
    "# è¼‰å…¥è¨“ç·´è³‡æ–™é›†\n",
    "train_dataset = load_dataset(\"banghua/DL-SFT-Dataset\")[\"train\"]\n",
    "\n",
    "# å¢åŠ è³‡æ–™é›†å¤§å°ä»¥æ”¹å–„å°æ¨¡å‹çš„è¨“ç·´æ•ˆæœï¼ˆå¾100å¢åŠ åˆ°500ï¼‰\n",
    "train_dataset = train_dataset.select(range(2000))\n",
    "\n",
    "# é¡¯ç¤ºè³‡æ–™é›†çš„å‰å¹¾å€‹ç¯„ä¾‹\n",
    "print(\"è¨“ç·´è³‡æ–™é›†ç¯„ä¾‹ï¼š\")\n",
    "display_dataset(train_dataset)\n",
    "\n",
    "print(f\"\\nè³‡æ–™é›†å¤§å°ï¼š{len(train_dataset)} å€‹æ¨£æœ¬\")\n",
    "print(\"\\nè³‡æ–™é›†ç‰¹é»ï¼š\")\n",
    "print(\"- åŒ…å«å¤šæ¨£åŒ–çš„æŒ‡ä»¤å’Œå›æ‡‰å°\")\n",
    "print(\"- æ¶µè“‹å•ç­”ã€ç¿»è­¯ã€è¨ˆç®—ç­‰å¤šç¨®ä»»å‹™\") \n",
    "print(\"- æ¯å€‹æ¨£æœ¬éƒ½åŒ…å«ä½¿ç”¨è€…æç¤ºå’ŒåŠ©æ‰‹å›æ‡‰\")\n",
    "print(\"- å¢åŠ åˆ°500å€‹æ¨£æœ¬ä»¥æ”¹å–„å°æ¨¡å‹è¨“ç·´æ•ˆæœ\")\n",
    "print(\"- å°æ¨¡å‹éœ€è¦æ›´å¤šæ¨£æœ¬æ‰èƒ½å­¸åˆ°è‰¯å¥½çš„å°è©±æ¨¡å¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01c515a8-3728-45fa-88cc-6eb4de839839",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT è¨“ç·´é…ç½®èªªæ˜ï¼ˆé‡å°å°æ¨¡å‹å„ªåŒ–ï¼Œä¿®å¾©é‡è¤‡å•é¡Œï¼‰ï¼š\n",
      "å­¸ç¿’ç‡: 0.0001 - æé«˜ä»¥å¹«åŠ©å°æ¨¡å‹æ›´å¿«å­¸ç¿’\n",
      "è¨“ç·´è¼ªæ•¸: 3 - å¢åŠ åˆ°3è¼ªï¼Œå°æ¨¡å‹éœ€è¦æ›´å¤šè¨“ç·´\n",
      "æ‰¹æ¬¡å¤§å°: 2 - ç¨å¾®å¢åŠ \n",
      "æ¢¯åº¦ç´¯ç©: 4 - æ¸›å°‘ä»¥åŠ å¿«æ›´æ–°\n",
      "æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: 8\n",
      "æœ€å¤§åºåˆ—é•·åº¦: 256 - æ¸›å°‘ä»¥é©æ‡‰å°æ¨¡å‹\n",
      "ç”Ÿæˆ: æ·»åŠ repetition_penalty=1.2å’Œtemperature=0.7æ¸›å°‘é‡è¤‡\n",
      "æ­£è¦åŒ–: æ·»åŠ weight_decayå’Œwarmup_stepsé˜²æ­¢éæ“¬åˆ\n"
     ]
    }
   ],
   "source": [
    "# SFT è¨“ç·´å™¨é…ç½®ï¼ˆé‡å°å°æ¨¡å‹å„ªåŒ–ï¼Œä¿®å¾©é‡è¤‡å•é¡Œï¼‰\n",
    "sft_config = SFTConfig(\n",
    "    learning_rate=1e-4,  # æé«˜å­¸ç¿’ç‡å¹«åŠ©å°æ¨¡å‹æ›´å¿«å­¸ç¿’\n",
    "    num_train_epochs=3,  # å¢åŠ è¨“ç·´è¼ªæ•¸ï¼Œå°æ¨¡å‹éœ€è¦æ›´å¤šè¨“ç·´\n",
    "    per_device_train_batch_size=2,  # ç¨å¾®å¢åŠ æ‰¹æ¬¡å¤§å°\n",
    "    gradient_accumulation_steps=4,  # æ¸›å°‘æ¢¯åº¦ç´¯ç©æ­¥é©Ÿ\n",
    "    gradient_checkpointing=False,  # å°æ¨¡å‹ä¸éœ€è¦æ¢¯åº¦æª¢æŸ¥é»\n",
    "    logging_steps=50,  # å¢åŠ è¨˜éŒ„é–“éš”\n",
    "    \n",
    "    # é—œéµä¿®å¾©ï¼šæ·»åŠ ä»¥ä¸‹åƒæ•¸ä¾†é¿å…é‡è¤‡å’Œæ”¹å–„è¨“ç·´\n",
    "    max_seq_length=256,  # æ¸›å°‘åºåˆ—é•·åº¦ä»¥é©æ‡‰å°æ¨¡å‹\n",
    "    dataset_text_field=\"messages\",  # æŒ‡å®šè³‡æ–™é›†çš„æ–‡å­—æ¬„ä½\n",
    "    packing=False,  # é—œé–‰åºåˆ—æ‰“åŒ…ï¼Œç¢ºä¿è¨“ç·´ç©©å®š\n",
    "    remove_unused_columns=False,  # ä¿ç•™æ‰€æœ‰æ¬„ä½\n",
    "    \n",
    "    # æ·»åŠ æ­£è¦åŒ–ä»¥æ¸›å°‘éæ“¬åˆ\n",
    "    weight_decay=0.01,  # æ¬Šé‡è¡°æ¸›\n",
    "    warmup_steps=20,  # é ç†±æ­¥é©Ÿ\n",
    "    \n",
    "    # è©•ä¼°é…ç½®\n",
    "    save_steps=100,  # ä¿å­˜æª¢æŸ¥é»é »ç‡\n",
    "    eval_steps=50,  # è©•ä¼°é »ç‡ï¼ˆå¦‚æœæœ‰é©—è­‰é›†ï¼‰\n",
    ")\n",
    "\n",
    "# é—œéµè¶…åƒæ•¸è§£é‡‹ï¼š\n",
    "print(\"SFT è¨“ç·´é…ç½®èªªæ˜ï¼ˆé‡å°å°æ¨¡å‹å„ªåŒ–ï¼Œä¿®å¾©é‡è¤‡å•é¡Œï¼‰ï¼š\")\n",
    "print(f\"å­¸ç¿’ç‡: {sft_config.learning_rate} - æé«˜ä»¥å¹«åŠ©å°æ¨¡å‹æ›´å¿«å­¸ç¿’\")\n",
    "print(f\"è¨“ç·´è¼ªæ•¸: {sft_config.num_train_epochs} - å¢åŠ åˆ°3è¼ªï¼Œå°æ¨¡å‹éœ€è¦æ›´å¤šè¨“ç·´\")\n",
    "print(f\"æ‰¹æ¬¡å¤§å°: {sft_config.per_device_train_batch_size} - ç¨å¾®å¢åŠ \")\n",
    "print(f\"æ¢¯åº¦ç´¯ç©: {sft_config.gradient_accumulation_steps} - æ¸›å°‘ä»¥åŠ å¿«æ›´æ–°\")\n",
    "print(f\"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps}\")\n",
    "print(f\"æœ€å¤§åºåˆ—é•·åº¦: {sft_config.max_seq_length} - æ¸›å°‘ä»¥é©æ‡‰å°æ¨¡å‹\")\n",
    "print(\"ç”Ÿæˆ: æ·»åŠ repetition_penalty=1.2å’Œtemperature=0.7æ¸›å°‘é‡è¤‡\")\n",
    "print(\"æ­£è¦åŒ–: æ·»åŠ weight_decayå’Œwarmup_stepsé˜²æ­¢éæ“¬åˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cd7d132-4c69-4b12-a8ec-e6b4795faad9",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 03:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.567900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.522600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.601300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.680900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.238400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.223300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.222600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.984900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.914100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.919500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=1.2521834920247397, metrics={'train_runtime': 228.1798, 'train_samples_per_second': 26.295, 'train_steps_per_second': 3.287, 'total_flos': 727475406605568.0, 'train_loss': 1.2521834920247397})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset, \n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4bac1-7262-4c55-b411-6a59188157b0",
   "metadata": {},
   "source": [
    "## æ¸¬è©¦å°å‹æ¨¡å‹å’Œå°è³‡æ–™é›†çš„è¨“ç·´çµæœ\n",
    "\n",
    "**æ³¨æ„ï¼š** ä»¥ä¸‹çµæœæ˜¯é‡å°æˆ‘å€‘åœ¨ SFT è¨“ç·´ä¸­ä½¿ç”¨çš„å°å‹æ¨¡å‹å’Œè³‡æ–™é›†ï¼Œé€™æ˜¯ç”±æ–¼è¨ˆç®—è³‡æºæœ‰é™ã€‚è‹¥è¦æŸ¥çœ‹å¤§å‹æ¨¡å‹çš„å®Œæ•´è¨“ç·´çµæœï¼Œè«‹åƒé–±ä¸Šæ–¹çš„ **ã€ŒQwen3-0.6B æ¨¡å‹çš„ SFT çµæœã€** éƒ¨åˆ†ã€‚\n",
    "\n",
    "### é æœŸçµæœåˆ†æï¼š\n",
    "ç”±æ–¼æˆ‘å€‘ä½¿ç”¨çš„æ˜¯å°å‹æ¨¡å‹ï¼ˆç›¸å°è¼ƒå°‘çš„åƒæ•¸ï¼‰å’Œæœ‰é™çš„è¨“ç·´è³‡æ–™ï¼Œæ¨¡å‹çš„è¡¨ç¾å¯èƒ½æœƒæœ‰ä»¥ä¸‹ç‰¹é»ï¼š\n",
    "- **æœ‰ä¸€å®šçš„æ”¹å–„**ï¼šç›¸æ¯”æœªè¨“ç·´çš„æ¨¡å‹ï¼Œæ‡‰è©²èƒ½çœ‹åˆ°æ˜é¡¯çš„æ”¹å–„\n",
    "- **ä»æœ‰é™åˆ¶**ï¼šç”±æ–¼æ¨¡å‹è¦æ¨¡å’Œè³‡æ–™é‡çš„é™åˆ¶ï¼Œå¯èƒ½ç„¡æ³•é”åˆ°å®Œç¾çš„è¡¨ç¾\n",
    "- **å­¸ç¿’èƒ½åŠ›å±•ç¾**ï¼šå¯ä»¥è§€å¯Ÿåˆ°æ¨¡å‹é–‹å§‹å­¸æœƒå›æ‡‰æŒ‡ä»¤çš„åŸºæœ¬èƒ½åŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d678274-5768-4cea-ae20-051488e5d0f3",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (After SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "1. I am a Licensed Marriage and Family Therapist (LFTT).\n",
      "\n",
      "2. My background in counseling, specifically marriage and family therapy has made it easy for me to provide compassionate and effective solutions tailored to individual needs.\n",
      "\n",
      "3. As a result, my work is highly sought after by both mental health professionals and individuals seeking help with various issues related to relationships and emotional wellbeing.\n",
      "\n",
      "4. Through experience as a licensed professional counselor, I have gained valuable insights into the unique challenges faced when\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "3. Subtract the result from step two (step four) to get your answer in decimal form, which is 0.52846...\n",
      "Note that you can round off any calculated number by simply adding or subtracting a predetermined value called a \"decimal point.\" Therefore, if desired precision was requested for this calculation, adjust accordingly!\n",
      "For example: Given our original question:\n",
      "\n",
      "1 + 1 - 1 = 0.5\n",
      "So there's approximately 0.\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "1. Thread is a single-threaded program that runs concurrently with other threads while processes are run on different computers or operating systems independently of each other, creating multitasking capabilities similar to those found in modern multi-user computing environments such as Windows XP and its successors. Processes can be started by running their own command line interface (CLI) tool like Task Manager or Control Panel to manage them effectively without any coordination from another application or user.\n",
      "\n",
      "2. Threads have an exclusive execution period\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not USE_GPU: # move model to CPU when GPU isn't requested\n",
    "    sft_trainer.model.to(\"cpu\")\n",
    "test_model_with_questions(sft_trainer.model, tokenizer, questions, \n",
    "                          title=\"Base Model (After SFT) Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bogag1lqd7",
   "metadata": {},
   "source": [
    "## èª²ç¨‹ç¸½çµèˆ‡æ·±å…¥æ€è€ƒ\n",
    "\n",
    "### æˆ‘å€‘åœ¨é€™å€‹èª²ç¨‹ä¸­å­¸åˆ°äº†ä»€éº¼ï¼š\n",
    "\n",
    "#### 1. SFT çš„æ ¸å¿ƒæ¦‚å¿µ\n",
    "- **å®šç¾©**ï¼šç›£ç£å¼å¾®èª¿ï¼ˆSFTï¼‰æ˜¯ä¸€ç¨®å°‡åŸºç¤èªè¨€æ¨¡å‹è½‰æ›ç‚ºèƒ½å¤ éµå¾ªæŒ‡ä»¤çš„å°è©±æ¨¡å‹çš„æŠ€è¡“\n",
    "- **åŸç†**ï¼šé€šéæœ€å°åŒ–è² å°æ•¸ä¼¼ç„¶æå¤±å‡½æ•¸ï¼Œè®“æ¨¡å‹å­¸ç¿’æ¨¡ä»¿è¨“ç·´è³‡æ–™ä¸­çš„å›æ‡‰\n",
    "- **ç›®æ¨™**ï¼šä½¿æ¨¡å‹èƒ½å¤ ç†è§£ä¸¦é©ç•¶å›æ‡‰ä½¿ç”¨è€…çš„å„ç¨®æŒ‡ä»¤å’Œå•é¡Œ\n",
    "\n",
    "#### 2. SFT çš„æ•¸å­¸åŸºç¤\n",
    "- **æå¤±å‡½æ•¸**ï¼š`Loss = -log P(response | prompt)`\n",
    "- **è¨“ç·´ç›®æ¨™**ï¼šæœ€å¤§åŒ–çµ¦å®šæç¤ºä¸‹ç”Ÿæˆæ­£ç¢ºå›æ‡‰çš„æ¦‚ç‡\n",
    "- **å¯¦ç¾æ–¹å¼**ï¼šå°æ‰€æœ‰è¨“ç·´æ¨£æœ¬çš„æå¤±æ±‚å’Œä¸¦é€²è¡Œæ¢¯åº¦ä¸‹é™\n",
    "\n",
    "#### 3. è³‡æ–™å“è³ªçš„é‡è¦æ€§\n",
    "- **å“è³ªå‹æ–¼æ•¸é‡**ï¼š1000å€‹é«˜å“è³ªæ¨£æœ¬å¾€å¾€æ¯”100è¬å€‹æ··åˆå“è³ªçš„æ¨£æœ¬æ•ˆæœæ›´å¥½\n",
    "- **è³‡æ–™æ•´ç†æ–¹æ³•**ï¼š\n",
    "  - è’¸é¤¾ï¼šä½¿ç”¨è¼ƒå¤§æ¨¡å‹ç”Ÿæˆé«˜å“è³ªå›æ‡‰\n",
    "  - æœ€ä½³ k é¸æ“‡ï¼šå¾å¤šå€‹ç”Ÿæˆçµæœä¸­é¸æ“‡æœ€ä½³å›æ‡‰\n",
    "  - ç¯©é¸ï¼šæ ¹æ“šå“è³ªå’Œå¤šæ¨£æ€§ç¯©é¸å¤§è¦æ¨¡è³‡æ–™é›†\n",
    "\n",
    "#### 4. å¯¦éš›æ‡‰ç”¨å ´æ™¯\n",
    "- **æ¨¡å‹è¡Œç‚ºå•Ÿå‹•**ï¼šå°‡é è¨“ç·´æ¨¡å‹è½‰ç‚ºæŒ‡ä»¤æ¨¡å‹\n",
    "- **èƒ½åŠ›æ”¹å–„**ï¼šæå‡ç‰¹å®šä»»å‹™çš„è¡¨ç¾\n",
    "- **çŸ¥è­˜è’¸é¤¾**ï¼šå°‡å¤§æ¨¡å‹çš„èƒ½åŠ›è½‰ç§»åˆ°å°æ¨¡å‹\n",
    "\n",
    "### æ·±å…¥æ€è€ƒï¼šSFT è¨“ç·´é›£é¡Œè¨ºæ–·\n",
    "\n",
    "#### ç‚ºä½•è¨“ç·´æå¤±ä¸¦éå–®èª¿éæ¸›ï¼Ÿ\n",
    "\n",
    "è§€å¯Ÿåˆ°è¨“ç·´éç¨‹ä¸­æå¤±å€¼è®ŠåŒ–ï¼š\n",
    "```\n",
    "1000    0.604300\n",
    "1050    0.615200  â† ä¸Šå‡\n",
    "1100    0.621700  â† ç¹¼çºŒä¸Šå‡\n",
    "```\n",
    "\n",
    "**åŸå› åˆ†æï¼š**\n",
    "1. **å­¸ç¿’ç‡éé«˜**ï¼šå°è‡´åœ¨æå¤±å‡½æ•¸çš„æœ€å°å€¼é™„è¿‘éœ‡ç›ª\n",
    "2. **éæ“¬åˆé–‹å§‹**ï¼šæ¨¡å‹é–‹å§‹è¨˜æ†¶è¨“ç·´è³‡æ–™ï¼Œæ±åŒ–èƒ½åŠ›ä¸‹é™\n",
    "3. **æ‰¹æ¬¡é–“è®Šç•°**ï¼šä¸åŒæ‰¹æ¬¡çš„è³‡æ–™åˆ†ä½ˆå·®ç•°å°è‡´æå¤±æ³¢å‹•\n",
    "4. **è³‡æ–™å“è³ªä¸ä¸€è‡´**ï¼šè¨“ç·´è³‡æ–™ä¸­å­˜åœ¨å“è³ªå·®ç•°è¼ƒå¤§çš„æ¨£æœ¬\n",
    "\n",
    "#### å¦‚ä½•åˆ¤æ–· SFT è¨“ç·´å¤±æ•—çš„æ ¹æœ¬åŸå› ï¼Ÿ\n",
    "\n",
    "##### 1. è³‡æ–™å•é¡Œè¨ºæ–·æŒ‡æ¨™\n",
    "\n",
    "**é‡åŒ–æŒ‡æ¨™ï¼š**\n",
    "- **è³‡æ–™é‡æ˜¯å¦å……è¶³**\n",
    "  - å°æ¨¡å‹ï¼ˆ<1Bï¼‰ï¼šè‡³å°‘éœ€è¦ 10K-50K é«˜å“è³ªæ¨£æœ¬\n",
    "  - ä¸­å‹æ¨¡å‹ï¼ˆ1B-7Bï¼‰ï¼šéœ€è¦ 100K-500K æ¨£æœ¬\n",
    "  - å¤§å‹æ¨¡å‹ï¼ˆ>7Bï¼‰ï¼šéœ€è¦ 1M+ æ¨£æœ¬\n",
    "\n",
    "- **è³‡æ–™å“è³ªè©•ä¼°**\n",
    "  ```python\n",
    "  # å»ºè­°çš„å“è³ªæª¢æŸ¥æŒ‡æ¨™\n",
    "  - å¹³å‡å›æ‡‰é•·åº¦ vs é æœŸé•·åº¦\n",
    "  - é‡è¤‡æ¨£æœ¬æ¯”ä¾‹ï¼ˆ<5%ï¼‰\n",
    "  - ä¸ç›¸é—œå›æ‡‰æ¯”ä¾‹ï¼ˆ<2%ï¼‰\n",
    "  - èªè¨€éŒ¯èª¤æ¯”ä¾‹ï¼ˆ<1%ï¼‰\n",
    "  ```\n",
    "\n",
    "**è¨ºæ–·æ–¹æ³•ï¼š**\n",
    "- **äººå·¥æŠ½æ¨£è©•ä¼°**ï¼šéš¨æ©Ÿé¸å– 100-200 å€‹æ¨£æœ¬é€²è¡Œäººå·¥è©•åˆ†\n",
    "- **è‡ªå‹•åŒ–å“è³ªæª¢æŸ¥**ï¼šä½¿ç”¨è¼ƒå¤§æ¨¡å‹è©•ä¼°å›æ‡‰å“è³ª\n",
    "- **å¤šæ¨£æ€§åˆ†æ**ï¼šè¨ˆç®—æŒ‡ä»¤é¡å‹åˆ†ä½ˆå’Œèªç¾©å¤šæ¨£æ€§\n",
    "\n",
    "##### 2. æ¨¡å‹èƒ½åŠ›æ¥µé™è¨ºæ–·æŒ‡æ¨™\n",
    "\n",
    "**é‡åŒ–æŒ‡æ¨™ï¼š**\n",
    "- **æ¨¡å‹å®¹é‡åˆ†æ**\n",
    "  ```python\n",
    "  åƒæ•¸é‡ vs ä»»å‹™è¤‡é›œåº¦æ¯”ä¾‹\n",
    "  - 135M æ¨¡å‹ï¼šé©åˆç°¡å–®å•ç­”ã€åŸºæœ¬å°è©±\n",
    "  - 0.6B æ¨¡å‹ï¼šé©åˆå¤šè¼ªå°è©±ã€åŸºæœ¬æ¨ç†\n",
    "  - 7B+ æ¨¡å‹ï¼šé©åˆè¤‡é›œæ¨ç†ã€å°ˆæ¥­çŸ¥è­˜\n",
    "  ```\n",
    "\n",
    "- **å­¸ç¿’æ›²ç·šåˆ†æ**\n",
    "  - è¨“ç·´æå¤±æŒçºŒä¸‹é™ä½†é©—è­‰æå¤±åœæ»¯ â†’ éæ“¬åˆ\n",
    "  - è¨“ç·´å’Œé©—è­‰æå¤±éƒ½åœæ»¯ â†’ æ¨¡å‹å®¹é‡ä¸è¶³\n",
    "  - æå¤±ä¸‹é™ä½†å“è³ªä¸æå‡ â†’ è©•ä¼°æŒ‡æ¨™å•é¡Œ\n",
    "\n",
    "**å¯¦ç”¨è¨ºæ–·æµç¨‹ï¼š**\n",
    "\n",
    "1. **æ•¸æ“šè¨ºæ–·å„ªå…ˆ**ï¼ˆ80%çš„å•é¡Œä¾†è‡ªè³‡æ–™ï¼‰\n",
    "   ```python\n",
    "   # å¿«é€Ÿè¨ºæ–·è…³æœ¬\n",
    "   def diagnose_data_quality(dataset):\n",
    "       # æª¢æŸ¥é‡è¤‡ç‡\n",
    "       # æª¢æŸ¥å¹³å‡é•·åº¦\n",
    "       # æª¢æŸ¥æ ¼å¼ä¸€è‡´æ€§\n",
    "       # äººå·¥æŠ½æ¨£è©•ä¼°\n",
    "   ```\n",
    "\n",
    "2. **æ¨¡å‹èƒ½åŠ›æ¸¬è©¦**\n",
    "   ```python\n",
    "   # ç°¡å–®åŸºæº–æ¸¬è©¦\n",
    "   simple_tasks = [\n",
    "       \"è¨ˆç®— 2+2\",\n",
    "       \"ç¿»è­¯ 'hello' åˆ°ä¸­æ–‡\", \n",
    "       \"åˆ—å‡º3ç¨®æ°´æœ\"\n",
    "   ]\n",
    "   # å¦‚æœç°¡å–®ä»»å‹™éƒ½å¤±æ•— â†’ æ¨¡å‹æˆ–è¨“ç·´æœ‰æ ¹æœ¬å•é¡Œ\n",
    "   ```\n",
    "\n",
    "3. **å°æ¯”åŸºæº–**\n",
    "   - åŒæ¨£æ¨¡å‹ + é«˜å“è³ªå°è³‡æ–™é›† vs å¤§é‡æ··åˆè³‡æ–™é›†\n",
    "   - ä¸åŒå¤§å°æ¨¡å‹ + ç›¸åŒè³‡æ–™é›†\n",
    "\n",
    "##### 3. å¯¦æˆ°ç¶“é©—æ³•å‰‡\n",
    "\n",
    "**è³‡æ–™å•é¡Œçš„ä¿¡è™Ÿï¼š**\n",
    "- æ¨¡å‹è¼¸å‡ºæ ¼å¼ä¸ä¸€è‡´\n",
    "- ç¶“å¸¸ç”¢ç”Ÿç„¡é—œå›æ‡‰\n",
    "- ç‰¹å®šé¡å‹å•é¡Œè¡¨ç¾æ¥µå·®\n",
    "- å¢åŠ è³‡æ–™é‡å¾Œæ•ˆæœé¡¯è‘—æå‡\n",
    "\n",
    "**æ¨¡å‹é™åˆ¶çš„ä¿¡è™Ÿï¼š**\n",
    "- ç°¡å–®ä»»å‹™ä¹Ÿç„¡æ³•å®Œæˆ\n",
    "- å¢åŠ è³‡æ–™é‡æ•ˆæœä¸æ˜é¡¯\n",
    "- æ›´å¤§æ¨¡å‹åœ¨ç›¸åŒè³‡æ–™ä¸Šè¡¨ç¾æ˜é¡¯æ›´å¥½\n",
    "- è¨“ç·´æå¤±ä¸‹é™ä½†å¯¦éš›è¡¨ç¾ç„¡æ”¹å–„\n",
    "\n",
    "**å»ºè­°çš„å„ªåŒ–é †åºï¼š**\n",
    "1. å…ˆå„ªåŒ–è³‡æ–™å“è³ªï¼ˆæŠ•å…¥ç”¢å‡ºæ¯”æœ€é«˜ï¼‰\n",
    "2. èª¿æ•´è¨“ç·´è¶…åƒæ•¸\n",
    "3. è€ƒæ…®å¢åŠ æ¨¡å‹å¤§å°\n",
    "4. ä½¿ç”¨é€²éšæŠ€è¡“ï¼ˆå¦‚ LoRAã€é‡åŒ–ç­‰ï¼‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "post-training-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
