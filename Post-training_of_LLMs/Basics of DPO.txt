1
00:00:02,460 --> 00:00:03,503
In this lesson

2
00:00:02,460 --> 00:00:03,503
本課程將講解直接偏好優化（DPO），這是一種用於微調大型語言模型的對比學習方法。DPO 利用正負樣本來調整模型行為，例如改變其身份或提升能力。課程將詳細說明 DPO..

3
00:00:03,503 --> 00:00:06,506
you'll learn basic concepts about direct preference optimization,

4
00:00:03,503 --> 00:00:06,506
你將學習關於直接偏好優化的基本概念，

5
00:00:07,173 --> 00:00:10,176
including the method, common use cases,

6
00:00:07,173 --> 00:00:10,176
包括該方法、常見使用案例，

7
00:00:10,176 --> 00:00:13,138
and principles for high-quality data curation in DPO.

8
00:00:10,176 --> 00:00:13,138
以及 DPO 中高品質數據整理的原則。

9
00:00:13,847 --> 00:00:14,305
All right.

10
00:00:13,847 --> 00:00:14,305
好的。

11
00:00:14,305 --> 00:00:15,098
Let's go.

12
00:00:14,305 --> 00:00:15,098
我們開始吧。

13
00:00:15,098 --> 00:00:15,807
Let's take a look at the detail formulation of DPO.

14
00:00:15,098 --> 00:00:15,807
讓我們來看看 DPO 的詳細公式。

15
00:00:15,807 --> 00:00:17,976
Let's take a look at the detail formulation of DPO.

16
00:00:15,807 --> 00:00:17,976
讓我們來看看 DPO 的詳細公式。

17
00:00:18,101 --> 00:00:22,022
So usually DPO can be considered as a contrastive learning method

18
00:00:18,101 --> 00:00:22,022
通常 DPO 可以被視為一種對比學習方法

19
00:00:22,355 --> 00:00:25,358
from both positive and negative responses.

20
00:00:22,355 --> 00:00:25,358
從正面與負面的回應中學習。

21
00:00:25,442 --> 00:00:30,442
So, like SFT, we can start from any LLM which usually is recommended to be

22
00:00:25,442 --> 00:00:30,442
因此，如同監督式微調(SFT)，我們可以從任何 LLM 開始，通常建議使用

23
00:00:31,072 --> 00:00:36,072
an instruct LLM where the model can already answer some basic questions to the user.

24
00:00:31,072 --> 00:00:36,072
已具備基礎問答能力的指令型 LLM，這類模型已能回答使用者一些基本問題。

25
00:00:36,536 --> 00:00:38,496
Let's say the user ask who are you?

26
00:00:36,536 --> 00:00:38,496
假設使用者詢問「你是誰？」

27
00:00:38,496 --> 00:00:40,790
And assistant says I'm Llama.

28
00:00:38,496 --> 00:00:40,790
而我會說我是 Llama。

29
00:00:40,790 --> 00:00:44,878
And in such scenario, we'd like to change the model identity

30
00:00:40,790 --> 00:00:44,878
在這種情況下，我們希望改變模型的身份

31
00:00:45,211 --> 00:00:49,299
by curating some comparison data prepared by the labeler.

32
00:00:45,211 --> 00:00:49,299
透過整理一些由標註員準備的比較數據。

33
00:00:49,883 --> 00:00:52,886
Such labeler can be is a human labeler

34
00:00:49,883 --> 00:00:52,886
這些標註員可以是人類標註員

35
00:00:52,969 --> 00:00:56,806
or even some model-based labeler that curates the dataset for us.

36
00:00:52,969 --> 00:00:56,806
或者甚至是由某個模型標記器來為我們整理資料集。

37
00:00:57,307 --> 00:01:00,810
So in this case, the user might ask, tell me your identity

38
00:00:57,307 --> 00:01:00,810
因此，在這種情況下，使用者可能會詢問：告訴我你的身份

39
00:01:01,352 --> 00:01:05,397
and we need to prepare at least two responses for DPO to work.

40
00:01:01,352 --> 00:01:05,397
而我們需要準備至少兩個回應才能讓 DPO 運作。

41
00:01:05,690 --> 00:01:09,986
So we can prepare one response saying I'm Athene,

42
00:01:05,690 --> 00:01:09,986
所以我們可以準備一個回應說我是雅典娜，

43
00:01:10,445 --> 00:01:15,445
and the other response saying I'm Llama. Where I'm Athene, is label as a preferred

44
00:01:10,445 --> 00:01:15,445
而另一個回答說我是 Llama。其中我說我是 Athene 的回答被標記為偏好回應

45
00:01:15,533 --> 00:01:19,204
response, and Llama is labeled as a less preferred response.

46
00:01:15,533 --> 00:01:19,204
而 Llama 則被標記為較不偏好的回應

47
00:01:19,370 --> 00:01:24,370
In this way, we try to encourage the model to say I'm Athene over I'm Llama

48
00:01:19,370 --> 00:01:24,370
透過這種方式，我們試圖鼓勵模型說出「我是 Athene」而非「我是 Llama」

49
00:01:25,168 --> 00:01:27,712
when responding to the identity related question.

50
00:01:25,168 --> 00:01:27,712
在回應與身份相關的問題時。

51
00:01:27,712 --> 00:01:32,425
After collecting such comparison data, you are ready to perform DPO

52
00:01:27,712 --> 00:01:32,425
收集完這類比較數據後，您就可以準備進行 DPO

53
00:01:32,425 --> 00:01:35,845
on top of this language model using the prepared data.

54
00:01:32,425 --> 00:01:35,845
在此語言模型的基礎上，使用準備好的數據。

55
00:01:36,262 --> 00:01:38,598
With such a loss function. We would dive deep into this loss function

56
00:01:36,262 --> 00:01:38,598
有了這樣的損失函數。我們將深入探討這個損失函數

57
00:01:38,598 --> 00:01:40,183
With such a loss function. We would dive deep into this loss function

58
00:01:38,598 --> 00:01:40,183
有了這樣的損失函數。我們將深入探討這個損失函數

59
00:01:40,725 --> 00:01:45,063
soon in this lesson After performing DPO on top of the language model,

60
00:01:40,725 --> 00:01:45,063
在本課程稍後部分 當我們在語言模型上執行 DPO 後

61
00:01:45,438 --> 00:01:48,858
we'll get a fine-tuned LLM that hopefully hello

62
00:01:45,438 --> 00:01:48,858
我們將獲得一個經過微調的 LLM，希望它能

63
00:01:48,858 --> 00:01:51,986
from both the path to and negative samples curated here.

64
00:01:48,858 --> 00:01:51,986
從這裡策劃的正樣本和負樣本中學習。

65
00:01:52,403 --> 00:01:55,615
So in this case, it will try to imitate the preferred samples.

66
00:01:52,403 --> 00:01:55,615
在這種情況下，它會試圖模仿偏好的樣本。

67
00:01:55,740 --> 00:01:56,157
And if the user asks further who are you?

68
00:01:55,740 --> 00:01:56,157
如果使用者進一步詢問你是誰？

69
00:01:56,157 --> 00:01:58,284
And if the user asks further who are you?

70
00:01:56,157 --> 00:01:58,284
如果使用者進一步詢問你是誰？

71
00:01:58,451 --> 00:02:00,954
And hopefully the assistant will answer

72
00:01:58,451 --> 00:02:00,954
希望助手能回答

73
00:02:00,954 --> 00:02:03,248
I'm Athene rather than I'm Llama.

74
00:02:00,954 --> 00:02:03,248
我是 Athene 而非 Llama。

75
00:02:03,248 --> 00:02:08,247
In this way, we get to change the identity of the model using this DPO approach.

76
00:02:03,248 --> 00:02:08,247
透過這種方式，我們得以運用 DPO 方法來改變模型的身份識別。

77
00:02:08,669 --> 00:02:13,258
Let's take a closer look at the loss function and what DPO is really doing.

78
00:02:08,669 --> 00:02:13,258
讓我們更深入探討損失函數以及 DPO 實際運作的原理。

79
00:02:13,550 --> 00:02:17,220
So the DPO is considered minimizing the contrastive loss,

80
00:02:13,550 --> 00:02:17,220
因此，DPO 被視為是在最小化對比損失，

81
00:02:17,595 --> 00:02:21,724
which penalizes negative response and encourages positive response.

82
00:02:17,595 --> 00:02:21,724
這會懲罰負面回應並鼓勵正面回應。

83
00:02:22,058 --> 00:02:26,187
And DPO loss is actually a cross-entropy loss on the reward difference

84
00:02:22,058 --> 00:02:26,187
而 DPO 損失函數實際上是基於獎勵差異的交叉熵損失

85
00:02:26,521 --> 00:02:30,567
of a re-parametrized from reward model, which will dive deeper here.

86
00:02:26,521 --> 00:02:30,567
這是由獎勵模型重新參數化而來，我們將在此深入探討。

87
00:02:30,859 --> 00:02:33,111
So let's take a look at this DPO loss, which is a negative log

88
00:02:30,859 --> 00:02:33,111
讓我們來看看這個 DPO 損失函數，它是一個負對數

89
00:02:33,111 --> 00:02:34,654
So let's take a look at this DPO loss, which is a negative log

90
00:02:33,111 --> 00:02:34,654
讓我們來看看這個 DPO 損失函數，它是一個負對數

91
00:02:34,863 --> 00:02:39,075
of sigmoid function of some log difference where a sigma is actually

92
00:02:34,863 --> 00:02:39,075
的 sigmoid 函數，其中 sigma 實際上就是

93
00:02:39,075 --> 00:02:43,246
a sigmoid function, and beta is a very important hyperparameter

94
00:02:39,075 --> 00:02:43,246
sigmoid 函數，而 beta 是一個非常重要的超參數

95
00:02:43,538 --> 00:02:47,000
that we can tune during the training process of DPO. Where the higher beta

96
00:02:43,538 --> 00:02:47,000
我們可以在 DPO 訓練過程中調整這個參數。beta 值越高

97
00:02:47,000 --> 00:02:47,834
that we can tune during the training process of DPO. Where the higher beta

98
00:02:47,000 --> 00:02:47,834
我們可以在 DPO 訓練過程中調整這個數值。其中 beta 值越高

99
00:02:47,834 --> 00:02:48,168
that we can tune during the training process of DPO. Where the higher beta

100
00:02:47,834 --> 00:02:48,168
我們可以在 DPO 訓練過程中調整這個數值。其中 beta 值越高

101
00:02:48,251 --> 00:02:52,172
is the more important this log difference could be.

102
00:02:48,251 --> 00:02:52,172
這個對數差異就越重要。

103
00:02:52,463 --> 00:02:56,342
And inside this big parenthesis we have two log differences

104
00:02:52,463 --> 00:02:56,342
而在這個大括號內，我們有兩個對數差異

105
00:02:56,676 --> 00:03:00,221
which focuses on positive sample and negative sample.

106
00:02:56,676 --> 00:03:00,221
著重於正樣本與負樣本。

107
00:03:00,513 --> 00:03:01,472
Let's take a look at top first.

108
00:03:00,513 --> 00:03:01,472
讓我們先看看頂部。

109
00:03:01,472 --> 00:03:05,143
First, we have a log of the ratio of two probabilities.

110
00:03:01,472 --> 00:03:05,143
首先，我們有兩個概率比值的對數。

111
00:03:05,560 --> 00:03:09,230
The numerator, which is pi zetha is a fine-tuned model.

112
00:03:05,560 --> 00:03:09,230
分子部分，即 pi zetha，是一個經過微調的模型。

113
00:03:09,314 --> 00:03:12,233
So here we're looking at for the fine-tuned model,

114
00:03:09,314 --> 00:03:12,233
我們現在要檢視的是經過微調後的模型，

115
00:03:12,233 --> 00:03:16,237
what's the probability of the positive response given the prompt here.

116
00:03:12,233 --> 00:03:16,237
在給定提示下產生正面回應的機率為何。

117
00:03:16,321 --> 00:03:16,821
And the denominator is a reference model

118
00:03:16,321 --> 00:03:16,821
而分母部分則是一個參考模型

119
00:03:16,821 --> 00:03:17,780
And the denominator is a reference model

120
00:03:16,821 --> 00:03:17,780
而分母部分則是一個參考模型

121
00:03:17,780 --> 00:03:18,656
And the denominator is a reference model

122
00:03:17,780 --> 00:03:18,656
而分母是一個參考模型

123
00:03:18,781 --> 00:03:22,118
which is a copy of the original model with weight fixed there.

124
00:03:18,781 --> 00:03:22,118
它是原始模型的複本，其權重在此固定。

125
00:03:22,118 --> 00:03:22,702
which is a copy of the original model with weight fixed there.

126
00:03:22,118 --> 00:03:22,702
它是原始模型的複本，其權重在此固定。

127
00:03:23,036 --> 00:03:24,662
And this is not tunable.

128
00:03:23,036 --> 00:03:24,662
而這是不可調整的。

129
00:03:24,662 --> 00:03:28,499
And we only look at what's the probability of the original model

130
00:03:24,662 --> 00:03:28,499
而我們只看原始模型

131
00:03:28,875 --> 00:03:32,253
in generating those positive response given the prompt.

132
00:03:28,875 --> 00:03:32,253
在給定提示下生成那些正面回應的機率為何。

133
00:03:32,545 --> 00:03:35,632
And similarly for the negative sample we also have the log

134
00:03:32,545 --> 00:03:35,632
同樣地，對於負面樣本我們也有對數

135
00:03:35,632 --> 00:03:38,635
ratio where pi zetha is your fine-tuned model.

136
00:03:35,632 --> 00:03:38,635
比例，其中 pi zetha 是您微調後的模型。

137
00:03:38,885 --> 00:03:41,429
And zetha is a ways you like to tune here.

138
00:03:38,885 --> 00:03:41,429
而 zetha 是你喜歡在這裡調整的方式。

139
00:03:41,429 --> 00:03:42,430
And Pi reference

140
00:03:41,429 --> 00:03:42,430
而 Pi 參考

141
00:03:42,430 --> 00:03:46,559
is a fixed reference model which can be a copy of the original model.

142
00:03:42,430 --> 00:03:46,559
是一個固定的參考模型，可以是原始模型的副本。

143
00:03:46,684 --> 00:03:49,395
Essentially, this log ratio term

144
00:03:46,684 --> 00:03:49,395
基本上，這個對數比率項

145
00:03:49,395 --> 00:03:52,982
can be viewed as a reparameterization of a reward model.

146
00:03:49,395 --> 00:03:52,982
可視為獎勵模型的一種重新參數化

147
00:03:53,191 --> 00:03:56,152
And if you look at this as a reward model,

148
00:03:53,191 --> 00:03:56,152
若將其視為獎勵模型

149
00:03:56,152 --> 00:03:59,697
then this DPO loss is actually a sigmoid function

150
00:03:56,152 --> 00:03:59,697
那麼這個 DPO 損失函數實際上就是

151
00:04:00,114 --> 00:04:03,785
of a reward difference between a positive sample and negative sample.

152
00:04:00,114 --> 00:04:03,785
正樣本與負樣本間獎勵差異的 sigmoid 函數

153
00:04:04,035 --> 00:04:07,330
And essentially DPO this is trying to maximize reward

154
00:04:04,035 --> 00:04:07,330
而基本上，DPO 的目標就是試圖最大化獎勵

155
00:04:07,664 --> 00:04:11,876
for the positive sample and minimize reward for the negative sample.

156
00:04:07,664 --> 00:04:11,876
針對正向樣本給予獎勵，並對負向樣本最小化獎勵。

157
00:04:12,126 --> 00:04:16,589
For details on why such log ratio can be viewed as a reparameterization

158
00:04:12,126 --> 00:04:16,589
關於為何此對數比率可視為一種重新參數化的詳細說明

159
00:04:16,589 --> 00:04:18,007
of such reward model,

160
00:04:16,589 --> 00:04:18,007
此類獎勵模型的

161
00:04:18,007 --> 00:04:22,303
I recommend you to read our original DPO paper and find the details there.

162
00:04:18,007 --> 00:04:22,303
我建議你閱讀我們原始的 DPO 論文，詳細內容都在裡面。

163
00:04:22,595 --> 00:04:25,556
So there are some best use cases for DPO as well,

164
00:04:22,595 --> 00:04:25,556
所以 DPO 也有一些最佳使用情境，

165
00:04:25,890 --> 00:04:29,394
where the first most important use case will be changing

166
00:04:25,890 --> 00:04:29,394
其中最重要的第一個使用情境就是改變

167
00:04:29,394 --> 00:04:30,228
model behavior.

168
00:04:29,394 --> 00:04:30,228
模型行為。

169
00:04:30,228 --> 00:04:33,690
Usually DPO is really good when you want to make small modifications

170
00:04:30,228 --> 00:04:33,690
通常當你想要進行微調時，DPO 的效果非常好

171
00:04:34,023 --> 00:04:35,608
of the model responses.

172
00:04:34,023 --> 00:04:35,608
模型回應的

173
00:04:35,608 --> 00:04:38,069
This includes changing the model identity

174
00:04:35,608 --> 00:04:38,069
這包括改變模型的身份

175
00:04:38,069 --> 00:04:38,903
or making the model better in multilingual responses or instruction,

176
00:04:38,069 --> 00:04:38,903
或是讓模型在多語言回應或指令方面表現更佳，

177
00:04:38,903 --> 00:04:39,404
or making the model better in multilingual responses or instruction,

178
00:04:38,903 --> 00:04:39,404
或提升模型在多語言回應或指令方面的表現，

179
00:04:39,404 --> 00:04:42,073
or making the model better in multilingual responses or instruction,

180
00:04:39,404 --> 00:04:42,073
或提升模型在多語言回應或指令方面的表現，

181
00:04:42,073 --> 00:04:46,494
falling capability, or change some safety related responses of the model.

182
00:04:42,073 --> 00:04:46,494
效能下降，或改變模型某些與安全性相關的回應。

183
00:04:46,661 --> 00:04:51,541
The second use case is about improving model capabilities. So usually, DPO,

184
00:04:46,661 --> 00:04:51,541
第二個使用案例是關於提升模型能力。通常來說，DPO

185
00:04:51,541 --> 00:04:56,087
when done right, can be better than SFT in improving model capabilities

186
00:04:51,541 --> 00:04:56,087
只要方法得當，在提升模型能力方面甚至能勝過監督式微調(SFT)

187
00:04:56,337 --> 00:04:58,172
due to its contrastive nature of seeing both good samples

188
00:04:56,337 --> 00:04:58,172
由於其對比性質能同時觀察優良樣本

189
00:04:58,172 --> 00:04:59,841
due to its contrastive nature of seeing both good samples

190
00:04:58,172 --> 00:04:59,841
由於其對比性質能同時觀察優良樣本

191
00:04:59,966 --> 00:05:03,469
and bad samples, especially when you can make DPO align

192
00:04:59,966 --> 00:05:03,469
以及不良樣本，特別是當你能讓 DPO 對齊時

193
00:05:03,928 --> 00:05:07,098
it can be even better for improving capabilities.

194
00:05:03,928 --> 00:05:07,098
它能讓能力提升得更好。

195
00:05:07,390 --> 00:05:08,558
The author DPO

196
00:05:07,390 --> 00:05:08,558
作者 DPO

197
00:05:08,558 --> 00:05:11,894
So here are a few principles of data curation for DPO.

198
00:05:08,558 --> 00:05:11,894
以下是 DPO 數據整理的幾個原則。

199
00:05:11,936 --> 00:05:15,398
There are a few common methods for a high-quality DPO data curation.

200
00:05:11,936 --> 00:05:15,398
有幾種常見的方法可以進行高品質的 DPO 數據整理。

201
00:05:15,481 --> 00:05:19,652
The first one can be a correction method where one can usually generate responses

202
00:05:15,481 --> 00:05:19,652
第一種可以是校正方法，通常能從原始模型生成回應

203
00:05:19,652 --> 00:05:23,072
from the original model, takes that response as an active sample,

204
00:05:19,652 --> 00:05:23,072
將該回應作為主動樣本，

205
00:05:23,156 --> 00:05:26,326
and you make some enhancements to make it a positive response.

206
00:05:23,156 --> 00:05:26,326
並進行一些增強使其成為正向回應。

207
00:05:26,367 --> 00:05:28,411
One simplest example in this case,

208
00:05:26,367 --> 00:05:28,411
這種情況下最簡單的例子，

209
00:05:28,411 --> 00:05:31,998
will be changing identity of the model where you can start from

210
00:05:28,411 --> 00:05:31,998
將會改變模型的識別特徵，你可以從

211
00:05:32,123 --> 00:05:35,168
a negative example generated by the current model itself,

212
00:05:32,123 --> 00:05:35,168
當前模型自身產生的負面範例開始，

213
00:05:35,335 --> 00:05:39,088
and the model might say I'm Llama for a question like who are you?

214
00:05:35,335 --> 00:05:39,088
而模型可能會在回答「你是誰？」這類問題時說「我是 Llama」

215
00:05:39,464 --> 00:05:41,758
And you can make changes directly

216
00:05:39,464 --> 00:05:41,758
你可以直接進行調整

217
00:05:41,758 --> 00:05:44,761
and replace this Llama with any model identity you want.

218
00:05:41,758 --> 00:05:44,761
並將這個 Llama 替換成你想要的任何模型身份。

219
00:05:45,053 --> 00:05:49,057
And in this case, we want the model to say I'm Athene for the same question.

220
00:05:45,053 --> 00:05:49,057
而在這個案例中，我們希望模型對相同問題回答「我是 Athene」。

221
00:05:49,515 --> 00:05:51,476
So we make that response as positive.

222
00:05:49,515 --> 00:05:51,476
因此我們將該回應設為正面範例。

223
00:05:51,476 --> 00:05:55,271
In this way, you can automatically create large scale, high quality

224
00:05:51,476 --> 00:05:55,271
透過這種方式，你就能自動建立大規模且高品質的

225
00:05:55,646 --> 00:05:59,609
contrastive data for training of DPO using this correction-based method.

226
00:05:55,646 --> 00:05:59,609
使用這種基於校正的方法來訓練 DPO 的對比數據。

227
00:05:59,734 --> 00:06:03,613
And the second method can be considered as a special case of online

228
00:05:59,734 --> 00:06:03,613
而第二種方法可以被視為線上

229
00:06:03,613 --> 00:06:07,533
or on policy DPO. Where you want to generate a positive

230
00:06:03,613 --> 00:06:07,533
或策略內 DPO 的特殊案例。這種情況下你需要從模型自身的分佈中

231
00:06:07,700 --> 00:06:11,412
and negative examples both from your model's of own distribution.

232
00:06:07,700 --> 00:06:11,412
同時生成正面與負面的範例。

233
00:06:11,412 --> 00:06:15,458
It's actually you can generate multiple responses from the current model

234
00:06:11,412 --> 00:06:15,458
其實你可以從現有模型中產生多種回應

235
00:06:15,458 --> 00:06:19,379
you want to tune for the same prompt, and then you can collect the best response

236
00:06:15,458 --> 00:06:19,379
針對同一個提示進行調整，然後你可以收集最佳的回應

237
00:06:19,587 --> 00:06:22,632
as positive sample and worst response as negative.

238
00:06:19,587 --> 00:06:22,632
作為正面樣本，最差回應則作為負面樣本

239
00:06:22,715 --> 00:06:25,093
You later determine which response is better.

240
00:06:22,715 --> 00:06:25,093
之後你再決定哪個回應比較好

241
00:06:25,093 --> 00:06:26,594
Which response is worse.

242
00:06:25,093 --> 00:06:26,594
哪個回應比較差

243
00:06:26,594 --> 00:06:30,139
You can use some reward function or human judgment to do this job.

244
00:06:26,594 --> 00:06:30,139
你可以使用一些獎勵函數或人工判斷來完成這項工作

245
00:06:30,181 --> 00:06:31,224
And the second thing one might want to pay attention to

246
00:06:30,181 --> 00:06:31,224
而第二件你可能會想注意的事

247
00:06:31,224 --> 00:06:31,599
And the second thing one might want to pay attention to

248
00:06:31,224 --> 00:06:31,599
而第二件你可能會想注意的事

249
00:06:31,599 --> 00:06:32,975
And the second thing one might want to pay attention to

250
00:06:31,599 --> 00:06:32,975
而第二件你可能會想注意的事

251
00:06:33,309 --> 00:06:37,480
is to avoid overfitting during DPO. Because DPO is essentially

252
00:06:33,309 --> 00:06:37,480
 ...

253
00:06:37,480 --> 00:06:41,567
doing some reward learning, it can easily overfit to some shortcut.

254
00:06:37,480 --> 00:06:41,567
 ...

255
00:06:41,901 --> 00:06:44,904
One of the preferred answers might have some shortcuts to learn

256
00:06:41,901 --> 00:06:44,904
 ...

257
00:06:45,029 --> 00:06:46,656
compared with non-preferred answers.

258
00:06:45,029 --> 00:06:46,656
 ...

259
00:06:46,656 --> 00:06:49,409
So one example here would be when the positive sample

260
00:06:46,656 --> 00:06:49,409
 ...

261
00:06:49,409 --> 00:06:53,079
always contains a few special words, while negative samples do not,

262
00:06:49,409 --> 00:06:53,079
 ...

263
00:06:53,246 --> 00:06:54,664
then training on this dataset

264
00:06:53,246 --> 00:06:54,664
 ...

265
00:06:54,664 --> 00:06:55,248
then training on this dataset

266
00:06:54,664 --> 00:06:55,248
 ...

267
00:06:55,248 --> 00:06:55,957
then training on this dataset

268
00:06:55,248 --> 00:06:55,957
 ...

269
00:06:55,957 --> 00:06:57,542
can be very fragile and it might require

270
00:06:55,957 --> 00:06:57,542
 ...

271
00:06:57,542 --> 00:06:57,959
can be very fragile and it might require

272
00:06:57,542 --> 00:06:57,959
 ...

273
00:06:57,959 --> 00:06:58,793
can be very fragile and it might require

274
00:06:57,959 --> 00:06:58,793
 ...

275
00:06:58,793 --> 00:07:01,796
much more hyperparameter tuning to get DPO working here.

276
00:06:58,793 --> 00:07:01,796
 ...

277
00:07:01,796 --> 00:07:05,842
So in this lesson, we have gone through the details about DPO training

278
00:07:01,796 --> 00:07:05,842
 ...

279
00:07:06,092 --> 00:07:08,636
and some principles about DPO data curation.

280
00:07:06,092 --> 00:07:08,636
 ...

281
00:07:08,636 --> 00:07:09,762
In the next lesson,

282
00:07:08,636 --> 00:07:09,762
 ...

283
00:07:09,762 --> 00:07:14,517
we'll dive deep into a coding practice about DPO that changes the model identity.

284
00:07:09,762 --> 00:07:14,517
 ...

285
00:07:14,725 --> 00:07:15,726
Excited to see you there!

286
00:07:14,725 --> 00:07:15,726
 ...

