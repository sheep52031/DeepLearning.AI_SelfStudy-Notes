1
00:00:03,670 --> 00:00:06,756
In this lesson you will learn basic concepts about supervised

2
00:00:03,670 --> 00:00:06,756
在本課程中，你將學習關於監督式微調的基本概念

3
00:00:06,756 --> 00:00:10,176
fine-tuning, including the method, common use cases,

4
00:00:06,756 --> 00:00:10,176
微調，包括方法、常見使用情境，

5
00:00:10,552 --> 00:00:13,680
and principles for high quality data curation in SFT.

6
00:00:10,552 --> 00:00:13,680
以及監督式微調（SFT）中的高品質資料整理原則。

7
00:00:14,264 --> 00:00:15,557
Let's dive in.

8
00:00:14,264 --> 00:00:15,557
讓我們開始深入探討。

9
00:00:15,557 --> 00:00:20,557
So unit SFT can be considered as imitating example responses.

10
00:00:15,557 --> 00:00:20,557
因此，監督式微調(SFT)可以被視為模仿範例回應的過程。

11
00:00:21,396 --> 00:00:24,107
You can start from any language model you want, which can predict a response.

12
00:00:21,396 --> 00:00:24,107
你可以從任何你想要的語言模型開始，只要該模型能夠預測回應。

13
00:00:24,107 --> 00:00:25,608
You can start from any language model you want, which can predict a response.

14
00:00:24,107 --> 00:00:25,608
你可以從任何你想要的語言模型開始，只要該模型能夠預測回應。

15
00:00:25,775 --> 00:00:27,610
Given a prompt.

16
00:00:25,775 --> 00:00:27,610
給定一個提示詞。

17
00:00:27,610 --> 00:00:30,905
It can be a base model where when a user ask a question,

18
00:00:27,610 --> 00:00:30,905
它可能是一個基礎模型，當使用者提出問題時，

19
00:00:31,239 --> 00:00:35,035
the base model might just predict the most likely token in the next word.

20
00:00:31,239 --> 00:00:35,035
基礎模型可能只會預測下一個最可能的詞彙。

21
00:00:35,160 --> 00:00:38,455
So it might just follow and predict very similar question.

22
00:00:35,160 --> 00:00:38,455
因此它可能只是跟隨並預測非常相似的問題。

23
00:00:38,580 --> 00:00:40,248
Instead of answering the question.

24
00:00:38,580 --> 00:00:40,248
而不是回答問題。

25
00:00:40,248 --> 00:00:43,460
In order to perform SFT on those base model

26
00:00:40,248 --> 00:00:43,460
為了對這些基礎模型進行監督式微調(SFT)

27
00:00:43,585 --> 00:00:46,463
you'll need to create some labeled data

28
00:00:43,585 --> 00:00:46,463
你需要準備一些標記好的數據

29
00:00:46,921 --> 00:00:47,630
in the format of user questions and ideal assessment responses

30
00:00:46,921 --> 00:00:47,630
格式需包含使用者提問與理想的評估回應

31
00:00:47,630 --> 00:00:51,760
in the format of user questions and ideal assessment responses

32
00:00:47,630 --> 00:00:51,760
格式需包含使用者提問與理想的評估回應

33
00:00:52,135 --> 00:00:56,139
The data might be in the format of tell me about your identity.

34
00:00:52,135 --> 00:00:56,139
資料可能會以「告訴我你的身份」這樣的格式呈現。

35
00:00:56,389 --> 00:00:56,681
And the assistant will respond saying, I'm Llama or any model, that you want

36
00:00:56,389 --> 00:00:56,681
而助手會回應說：「我是 Llama」或任何你想要的模型

37
00:00:56,681 --> 00:01:01,352
And the assistant will respond saying, I'm Llama or any model, that you want

38
00:00:56,681 --> 00:01:01,352
而助手會回應說：「我是 Llama」或任何你想要的模型

39
00:01:01,352 --> 00:01:01,644
And the assistant will respond saying, I'm Llama or any model, that you want

40
00:01:01,352 --> 00:01:01,644
而助手會回應說：「我是 Llama」或任何你想要的模型

41
00:01:01,644 --> 00:01:02,604
It to be.

42
00:01:01,644 --> 00:01:02,604
是的。

43
00:01:02,604 --> 00:01:04,522
The user might also ask, how are you?

44
00:01:02,604 --> 00:01:04,522
使用者也可能會問，你好嗎？

45
00:01:04,522 --> 00:01:08,234
And assistant can say, I'm doing great. By preparing a large dataset

46
00:01:04,522 --> 00:01:08,234
而助理可以回答，我很好。透過準備大量

47
00:01:08,234 --> 00:01:10,028
of such labeled data,

48
00:01:08,234 --> 00:01:10,028
這樣的標記數據，

49
00:01:10,028 --> 00:01:11,279
we are ready for doing SFT

50
00:01:10,028 --> 00:01:11,279
我們已準備好進行監督式微調(SFT)

51
00:01:11,279 --> 00:01:12,530
we are ready for doing SFT

52
00:01:11,279 --> 00:01:12,530
我們已準備好進行監督式微調(SFT)

53
00:01:12,530 --> 00:01:16,534
and imitating those example responses provided in the labeled data.

54
00:01:12,530 --> 00:01:16,534
並模仿標記資料中提供的那些範例回應

55
00:01:16,826 --> 00:01:19,871
The way SFT works is by minimizing

56
00:01:16,826 --> 00:01:19,871
SFT 的運作方式是透過最小化

57
00:01:19,871 --> 00:01:22,874
the negative log likelihood for the response

58
00:01:19,871 --> 00:01:22,874
回應的負對數似然

59
00:01:22,999 --> 00:01:25,502
given the prompt. And when you take the sum over all labeled data

60
00:01:22,999 --> 00:01:25,502
給定提示。當你對所有標記數據進行加總時

61
00:01:25,502 --> 00:01:26,002
given the prompt. And when you take the sum over all labeled data

62
00:01:25,502 --> 00:01:26,002
給定提示。當你對所有標記數據進行加總時

63
00:01:26,002 --> 00:01:28,004
given the prompt. And when you take the sum over all labeled data

64
00:01:26,002 --> 00:01:28,004
給定提示。當你對所有標記數據進行加總時

65
00:01:28,296 --> 00:01:31,633
in this case, we'll go deeper into this loss function

66
00:01:28,296 --> 00:01:31,633
在這種情況下，我們將更深入地探討這個損失函數

67
00:01:31,800 --> 00:01:32,926
in the next slide.

68
00:01:31,800 --> 00:01:32,926
在下一張投影片中。

69
00:01:32,926 --> 00:01:34,636
After performing a SFT

70
00:01:32,926 --> 00:01:34,636
執行監督式微調後

71
00:01:34,636 --> 00:01:39,015
on based model, what you'll look at a fine-tune model or an instruct model,

72
00:01:34,636 --> 00:01:39,015
基於基礎模型，你將會看到一個微調模型或是指令模型，

73
00:01:39,099 --> 00:01:43,353
which is able to respond to any user query properly if done correctly.

74
00:01:39,099 --> 00:01:43,353
如果操作得當，就能夠妥善回應任何使用者查詢。

75
00:01:43,561 --> 00:01:46,606
So let's take a closer look at the formula here.

76
00:01:43,561 --> 00:01:46,606
讓我們更仔細地看看這個公式。

77
00:01:46,815 --> 00:01:51,486
You'll hear a SFT and I minimizing the negative log likelihood

78
00:01:46,815 --> 00:01:51,486
你會聽到 SFT 和我最小化負對數似然

79
00:01:51,820 --> 00:01:55,115
for the responses, where minimizing the negative log likelihood

80
00:01:51,820 --> 00:01:55,115
針對回應的部分，這裡的最小化負對數似然

81
00:01:55,406 --> 00:01:59,202
is equivalent to maximum likelihood and use across actual loss

82
00:01:55,406 --> 00:01:59,202
等同於最大似然估計，並應用於實際損失計算

83
00:01:59,202 --> 00:01:59,661
here.

84
00:01:59,202 --> 00:01:59,661
此處。

85
00:01:59,661 --> 00:02:02,664
So for any data of index I

86
00:01:59,661 --> 00:02:02,664
因此，對於任何索引為 I 的數據

87
00:02:03,039 --> 00:02:06,626
where the isolator is just a specific prompt response pairs,

88
00:02:03,039 --> 00:02:06,626
其中隔離器僅指特定的提示-回應配對，

89
00:02:07,168 --> 00:02:10,088
the loss for SFT will be the negative

90
00:02:07,168 --> 00:02:10,088
監督式微調（SFT）的損失函數將是負值

91
00:02:10,088 --> 00:02:14,217
of that log probability of the response given the prompt.

92
00:02:10,088 --> 00:02:14,217
給定提示下回應的對數概率

93
00:02:14,592 --> 00:02:16,803
Where it can be further written as the negative log likelihood,

94
00:02:14,592 --> 00:02:16,803
可進一步表示為負對數似然

95
00:02:16,803 --> 00:02:19,681
Where it can be further written as the negative log likelihood,

96
00:02:16,803 --> 00:02:19,681
可進一步表示為負對數似然

97
00:02:19,889 --> 00:02:23,935
where the likelihood is a product of the probability for the tokens

98
00:02:19,889 --> 00:02:23,935
其中可能性是基於所有先前標記（包括提示標記）

99
00:02:23,935 --> 00:02:28,189
in the responses given all the prior tokens, including the prompt tokens.

100
00:02:23,935 --> 00:02:28,189
給定回應中標記機率的乘積。

101
00:02:28,523 --> 00:02:32,569
So in this way, we train the model to maximize the possibility

102
00:02:28,523 --> 00:02:32,569
因此，我們透過這種方式訓練模型來最大化

103
00:02:32,902 --> 00:02:36,239
of outputting your provided response given the prompt.

104
00:02:32,902 --> 00:02:36,239
在給定提示下輸出所提供回應的可能性。

105
00:02:36,906 --> 00:02:41,119
That's why SFT is trying to imitate those example responses

106
00:02:36,906 --> 00:02:41,119
這就是為什麼監督式微調(SFT)要試著模仿那些範例回應

107
00:02:41,119 --> 00:02:41,578
here.

108
00:02:41,119 --> 00:02:41,578
此處。

109
00:02:41,578 --> 00:02:45,498
So there are a few best use cases or most appropriate use cases

110
00:02:41,578 --> 00:02:45,498
所以有幾個最適合或最恰當的使用情境

111
00:02:45,748 --> 00:02:47,083
for supervised fine-tuning.

112
00:02:45,748 --> 00:02:47,083
適用於監督式微調

113
00:02:47,083 --> 00:02:51,588
The first one is when wants to jump start a new model behavior.

114
00:02:47,083 --> 00:02:51,588
第一個情境是當需要快速啟動新模型行為時

115
00:02:51,880 --> 00:02:54,591
So it might be the case where you want to turn

116
00:02:51,880 --> 00:02:54,591
你可能會想要將

117
00:02:54,591 --> 00:02:58,845
a pre-trained language model to an instruct model, or the case

118
00:02:54,591 --> 00:02:58,845
預訓練的語言模型轉換成指令模型，或是將

119
00:02:58,845 --> 00:03:02,640
where you want to turn a non-reasoning model into a reasoning model.

120
00:02:58,845 --> 00:03:02,640
不具備推理能力的模型轉變為具備推理能力的模型。

121
00:03:02,891 --> 00:03:06,811
Or there might be a specific scenario where you want the model

122
00:03:02,891 --> 00:03:06,811
也可能有特定情境需要讓模型

123
00:03:06,811 --> 00:03:11,065
to use certain tools without providing the tool descriptions in the prompt,

124
00:03:06,811 --> 00:03:11,065
在不需於提示中提供工具描述的情況下使用特定工具，

125
00:03:11,649 --> 00:03:15,862
and the model would just assume that it already has access to the tools.

126
00:03:11,649 --> 00:03:15,862
而模型會直接假設它已經擁有這些工具的存取權限。

127
00:03:16,112 --> 00:03:18,072
And call the tools in the responses.

128
00:03:16,112 --> 00:03:18,072
並在回應中調用這些工具。

129
00:03:18,072 --> 00:03:21,159
In those cases, SFT will be very ideal

130
00:03:18,072 --> 00:03:21,159
在這些情況下，監督式微調（SFT）會是非常理想的選擇

131
00:03:21,451 --> 00:03:24,329
for jumpstarting such model behaviors.

132
00:03:21,451 --> 00:03:24,329
用來快速啟動這類模型行為

133
00:03:24,329 --> 00:03:27,624
And a second use case is to improve certain model capabilities.

134
00:03:24,329 --> 00:03:27,624
第二個使用案例是提升特定模型能力

135
00:03:27,874 --> 00:03:32,837
And one scenario I'd like to highlight here is to distill capabilities

136
00:03:27,874 --> 00:03:32,837
這裡我想特別強調的一個情境是能力蒸餾

137
00:03:33,171 --> 00:03:36,466
for a smaller model by training on a high quality

138
00:03:33,171 --> 00:03:36,466
透過高品質訓練來縮小模型規模

139
00:03:36,466 --> 00:03:39,469
synthetic data generated by a larger model.

140
00:03:36,466 --> 00:03:39,469
這些訓練資料是由較大型模型生成的合成數據

141
00:03:39,677 --> 00:03:42,263
So in this case, you're essentially distilling a larger model's capability

142
00:03:39,677 --> 00:03:42,263
因此這種情況下，你基本上是在萃取大型模型的能力

143
00:03:42,263 --> 00:03:43,473
So in this case, you're essentially distilling a larger model's capability

144
00:03:42,263 --> 00:03:43,473
因此這種情況下，你基本上是在萃取大型模型的能力

145
00:03:43,640 --> 00:03:46,476
into a small model using supervised fitting.

146
00:03:43,640 --> 00:03:46,476
透過監督式微調將知識注入小型模型

147
00:03:46,476 --> 00:03:50,188
So there are some principles of recommended ways to do supervised

148
00:03:46,476 --> 00:03:50,188
以下是進行監督式微調時

149
00:03:50,230 --> 00:03:51,981
fine-tuning data curation.

150
00:03:50,230 --> 00:03:51,981
資料整理的推薦原則

151
00:03:51,981 --> 00:03:52,857
So the common methods

152
00:03:51,981 --> 00:03:52,857
常見方法包括

153
00:03:52,857 --> 00:03:56,694
for high-quality SFT data curation include the following few examples:

154
00:03:52,857 --> 00:03:56,694
高品質監督式微調(SFT)資料整理的幾個範例包括：

155
00:03:56,903 --> 00:03:59,906
The first one, is distillation as we discussed before,

156
00:03:56,903 --> 00:03:59,906
首先，如同我們先前討論過的蒸餾法，

157
00:04:00,073 --> 00:04:04,327
one can generate those responses from a stronger and larger instruct model,

158
00:04:00,073 --> 00:04:04,327
可以從更強大、更大型的指令模型中生成這些回應，

159
00:04:04,369 --> 00:04:04,994
and let a smaller model to imitate those generated responses.

160
00:04:04,369 --> 00:04:04,994
然後讓較小的模型模仿這些生成的回應。

161
00:04:04,994 --> 00:04:08,081
and let a smaller model to imitate those generated responses.

162
00:04:04,994 --> 00:04:08,081
然後讓較小的模型模仿這些生成的回應。

163
00:04:08,289 --> 00:04:12,001
The second one, can be a Best of k or rejection sampling,

164
00:04:08,289 --> 00:04:12,001
第二種方法可以是「最佳 k 選取」或拒絕抽樣

165
00:04:12,126 --> 00:04:16,047
where one can generate multiple responses from the same original model

166
00:04:12,126 --> 00:04:16,047
你可以從同一個原始模型生成多個回應

167
00:04:16,255 --> 00:04:19,300
that you want to train on, and you can select the best among them

168
00:04:16,255 --> 00:04:19,300
這些模型是你想要訓練的，然後從中選出最佳的回應

169
00:04:19,634 --> 00:04:23,805
using either a reward function or some other automatic method.

170
00:04:19,634 --> 00:04:23,805
使用獎勵函數或其他自動化方法

171
00:04:24,055 --> 00:04:27,850
In this way, one can get the best response and try

172
00:04:24,055 --> 00:04:27,850
如此一來，就能獲得最佳回應並嘗試

173
00:04:27,850 --> 00:04:31,562
imitating those best responses generated by the model itself.

174
00:04:27,850 --> 00:04:31,562
模仿模型本身生成的最佳回應

175
00:04:31,646 --> 00:04:34,399
And the third case would be a filtering idea

176
00:04:31,646 --> 00:04:34,399
第三種情況則是採用篩選概念

177
00:04:34,399 --> 00:04:36,401
where you can start from a very large scale

178
00:04:34,399 --> 00:04:36,401
你可以從非常龐大的規模開始

179
00:04:36,401 --> 00:04:40,571
SFT dataset collected from HuggingFace or from your internal database.

180
00:04:36,401 --> 00:04:40,571
從 HuggingFace 或內部資料庫收集的 SFT 資料集

181
00:04:40,738 --> 00:04:43,700
Then you filter them according to both the quality

182
00:04:40,738 --> 00:04:43,700
接著你根據回應的品質

183
00:04:43,700 --> 00:04:46,703
of the responses and the diversity of the prompts

184
00:04:43,700 --> 00:04:46,703
以及提示的多樣性來篩選它們

185
00:04:46,786 --> 00:04:50,623
to get a smaller-scale SFT dataset that's have a higher quality

186
00:04:46,786 --> 00:04:50,623
以獲得一個規模較小但品質更高

187
00:04:50,915 --> 00:04:52,166
and diverse enough.

188
00:04:50,915 --> 00:04:52,166
且足夠多元的 SFT 資料集。

189
00:04:52,166 --> 00:04:54,294
Besides the common methods mentioned here,

190
00:04:52,166 --> 00:04:54,294
除了這裡提到的常見方法之外，

191
00:04:54,294 --> 00:04:58,047
I'd also like to highlight that usually in a SFT data curation,

192
00:04:54,294 --> 00:04:58,047
我還想強調的是，通常在監督式微調（SFT）的數據整理過程中，

193
00:04:58,423 --> 00:05:02,719
the quality is much more important than quantity for improving capabilities.

194
00:04:58,423 --> 00:05:02,719
提升模型能力的關鍵在於數據品質而非數量。

195
00:05:02,885 --> 00:05:07,640
If you have a 1000 really high quality and diverse data that can usually outperform

196
00:05:02,885 --> 00:05:07,640
如果你擁有 1000 筆真正高品質且多樣化的數據，其效果通常會勝過

197
00:05:07,640 --> 00:05:08,766
the SFT results of 1 million mixed quality data.

198
00:05:07,640 --> 00:05:08,766
100 萬筆混合品質資料的監督微調結果

199
00:05:08,766 --> 00:05:10,560
the SFT results of 1 million mixed quality data.

200
00:05:08,766 --> 00:05:10,560
100 萬筆混合品質資料的監督微調結果

201
00:05:10,727 --> 00:05:13,646
The rationale behind this is that SFT

202
00:05:10,727 --> 00:05:13,646
這背後的基本原理在於監督微調

203
00:05:13,646 --> 00:05:17,150
usually requires imitating all the data provided by you.

204
00:05:13,646 --> 00:05:17,150
通常需要模仿您提供的所有資料

205
00:05:17,275 --> 00:05:21,154
If there are some really bad responses in the mixed quality data, the model

206
00:05:17,275 --> 00:05:21,154
若混合品質的資料中含有一些極差的回應，模型

207
00:05:21,154 --> 00:05:25,908
will be forced to imitate such response and thus degrading the performance.

208
00:05:21,154 --> 00:05:25,908
將被迫模仿此類回應，從而導致效能下降。

209
00:05:26,242 --> 00:05:30,413
So data quality here can be really important for the success of SFT.

210
00:05:26,242 --> 00:05:30,413
因此，資料品質對於監督式微調(SFT)的成功至關重要。

211
00:05:30,705 --> 00:05:34,542
Lastly, I like to highlight one orthogonal direction

212
00:05:30,705 --> 00:05:34,542
最後，我想強調一個正交方向

213
00:05:34,584 --> 00:05:38,004
in model tuning that's completely parallel

214
00:05:34,584 --> 00:05:38,004
在模型調校中，這完全是平行

215
00:05:38,004 --> 00:05:41,883
and orthogonal to any post-training methods there will be choices

216
00:05:38,004 --> 00:05:41,883
且與任何後訓練方法正交的選擇

217
00:05:41,883 --> 00:05:45,053
of full fine-tuning versus parameter efficient fine-tuning.

218
00:05:41,883 --> 00:05:45,053
包括全微調與參數高效微調

219
00:05:45,136 --> 00:05:46,971
Where in full fine-tuning

220
00:05:45,136 --> 00:05:46,971
其中全微調

221
00:05:46,971 --> 00:05:49,974
let's say we have a layer of the neural networks

222
00:05:46,971 --> 00:05:49,974
假設我們有一個神經網路的層

223
00:05:50,350 --> 00:05:53,353
where ash is actually the latest output.

224
00:05:50,350 --> 00:05:53,353
其中 ash 實際上是該層的最新輸出

225
00:05:53,353 --> 00:05:56,814
W is actually the original weight of that layer, and x is

226
00:05:53,353 --> 00:05:56,814
W 實際上是該層的原始權重，而 x 則是

227
00:05:56,814 --> 00:05:57,065
the layer's input.

228
00:05:56,814 --> 00:05:57,065
該層的輸入。

229
00:05:57,065 --> 00:05:58,024
the layer's input.

230
00:05:57,065 --> 00:05:58,024
該層的輸入。

231
00:05:58,024 --> 00:05:59,567
What people do for fine-tuning,

232
00:05:58,024 --> 00:05:59,567
人們在進行微調時，

233
00:05:59,567 --> 00:06:04,113
we usually add some delta with delta w where this delta w

234
00:05:59,567 --> 00:06:04,113
通常會加入一些增量 delta w，而這個 delta w

235
00:06:04,113 --> 00:06:06,032
is from gradient descent.

236
00:06:04,113 --> 00:06:06,032
來自梯度下降法。

237
00:06:06,032 --> 00:06:09,494
And that the other ways has the exact same size as the original weights.

238
00:06:06,032 --> 00:06:09,494
而其他方法則保持與原始權重完全相同的尺寸。

239
00:06:09,577 --> 00:06:12,580
So in this way you have to introduce an additional

240
00:06:09,577 --> 00:06:12,580
因此，這種方式必須引入一個額外的

241
00:06:12,580 --> 00:06:15,583
d by D measures in order to do the model updates.

242
00:06:12,580 --> 00:06:15,583
d 乘 D 的測量值才能進行模型更新。

243
00:06:15,666 --> 00:06:18,503
There's an alternative method called parameter efficient fine

244
00:06:15,666 --> 00:06:18,503
另有一種稱為參數高效微調的替代方法

245
00:06:18,503 --> 00:06:22,048
tuning where we still have original layer output ash

246
00:06:18,503 --> 00:06:22,048
調整過程中，我們仍保有原始層的輸出結果

247
00:06:22,548 --> 00:06:26,803
layer input x and the original weights of that layer w.

248
00:06:22,548 --> 00:06:26,803
層輸入 x 和該層的原始權重 w

249
00:06:27,011 --> 00:06:30,556
But instead of directly adding a delta weights just

250
00:06:27,011 --> 00:06:30,556
但我們不是直接添加與原始權重 w 相同大小的

251
00:06:30,556 --> 00:06:34,268
of the same size as an original weight w, we can actually add

252
00:06:30,556 --> 00:06:34,268
增量權重，實際上可以添加

253
00:06:34,352 --> 00:06:39,107
another multiplication of two matrices that are smaller, which is b.

254
00:06:34,352 --> 00:06:39,107
另一個較小的矩陣相乘運算，也就是 b。

255
00:06:39,273 --> 00:06:42,819
Multiply a where b is a d by our dimensional matrix,

256
00:06:39,273 --> 00:06:42,819
將 a 乘以 b，其中 b 是一個 d 維度的矩陣，

257
00:06:43,069 --> 00:06:47,365
and a is r by d dimensional matrix, where R is usually much smaller than d.

258
00:06:43,069 --> 00:06:47,365
而 a 是一個 r 乘以 d 維度的矩陣，通常 r 會比 d 小很多。

259
00:06:47,532 --> 00:06:52,161
In this case, your effective numbers of parameters to update

260
00:06:47,532 --> 00:06:52,161
在這種情況下，你需要更新的有效參數數量

261
00:06:52,495 --> 00:06:55,498
is only the total number of parameters in B and A,

262
00:06:52,495 --> 00:06:55,498
僅是 B 和 A 中的參數總數，

263
00:06:56,124 --> 00:06:59,961
and that can be much smaller than the size of the original weights. In

264
00:06:56,124 --> 00:06:59,961
這可能遠小於原始權重的大小。

265
00:06:59,961 --> 00:07:03,923
this way, you are saving a lot of memory during such calculation,

266
00:06:59,961 --> 00:07:03,923
如此一來，你在這類計算過程中能節省大量記憶體，

267
00:07:04,340 --> 00:07:07,218
and also make this more efficient to compute.

268
00:07:04,340 --> 00:07:07,218
同時也使運算更有效率。

269
00:07:07,218 --> 00:07:09,887
So I'd like to mention here that both full fine-tuning on the left

270
00:07:07,218 --> 00:07:09,887
我想在此說明的是，左側的完整微調

271
00:07:09,887 --> 00:07:11,097
So I'd like to mention here that both full fine-tuning on the left

272
00:07:09,887 --> 00:07:11,097
我想在此說明的是，左側的完整微調

273
00:07:11,264 --> 00:07:15,351
and parameter efficient fine-tuning on the right can be used in combination

274
00:07:11,264 --> 00:07:15,351
以及右側的參數高效微調都可以結合使用

275
00:07:15,560 --> 00:07:18,563
with any of the training methods we'll be discussing here,

276
00:07:15,560 --> 00:07:18,563
與我們即將討論的任何訓練方法

277
00:07:18,563 --> 00:07:20,231
including supervised fine-tuning,

278
00:07:18,563 --> 00:07:20,231
包括監督式微調

279
00:07:20,231 --> 00:07:23,109
direct preference optimization, and online reinforced learning.

280
00:07:20,231 --> 00:07:23,109
直接偏好優化與線上強化學習

281
00:07:23,109 --> 00:07:26,237
So it's up to your choice whether you want to go with full fine

282
00:07:23,109 --> 00:07:26,237
因此您可以自行選擇要採用完整微調

283
00:07:26,237 --> 00:07:29,991
tuning or parameter efficient fine-tuning in any of the method here.

284
00:07:26,237 --> 00:07:29,991
或是任何一種參數高效微調方法

285
00:07:30,158 --> 00:07:33,161
So such parameter efficient fine-tuning method like Lora,

286
00:07:30,158 --> 00:07:33,161
像 LoRA 這樣的參數高效微調方法

287
00:07:33,327 --> 00:07:34,078
you have saving a lot of memory, but on the other hand, it also learns less

288
00:07:33,327 --> 00:07:34,078
雖然能節省大量記憶體，但相對地學習能力也較弱

289
00:07:34,078 --> 00:07:37,582
you have saving a lot of memory, but on the other hand, it also learns less

290
00:07:34,078 --> 00:07:37,582
雖然能節省大量記憶體，但相對地學習能力也較弱

291
00:07:37,790 --> 00:07:41,377
while forgets less because there are just less parameters to tune

292
00:07:37,790 --> 00:07:41,377
因為需要調整的參數較少，所以遺忘的情況也較少

293
00:07:41,377 --> 00:07:42,170
in this case.

294
00:07:41,377 --> 00:07:42,170
在這種情況下

295
00:07:42,170 --> 00:07:46,883
In this lesson, you have learned about details on supervised fine-tuning

296
00:07:42,170 --> 00:07:46,883
在本課程中，您已學習到監督式微調的詳細內容

297
00:07:47,049 --> 00:07:48,593
and the differences of full fine tuning versus parameter efficient fine-tuning.

298
00:07:47,049 --> 00:07:48,593
以及完整微調與參數高效微調之間的差異

299
00:07:48,593 --> 00:07:51,053
and the differences of full fine tuning versus parameter efficient fine-tuning.

300
00:07:48,593 --> 00:07:51,053
以及完整微調與參數高效微調之間的差異

301
00:07:51,053 --> 00:07:54,599
In the next lesson, we'll do some coding practices

302
00:07:51,053 --> 00:07:54,599
下一堂課我們會進行一些編碼練習

303
00:07:54,765 --> 00:07:59,562
about supervised fine-tuning that turns a base model into an instruct model.

304
00:07:54,765 --> 00:07:59,562
關於如何透過監督式微調將基礎模型轉變為指令模型

305
00:07:59,770 --> 00:08:00,354
See you there.

306
00:07:59,770 --> 00:08:00,354
到時候見。

