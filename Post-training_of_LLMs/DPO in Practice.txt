1
00:00:02,669 --> 00:00:04,796
This lesson is all about building

2
00:00:02,669 --> 00:00:04,796
本課程將示範如何在小數據集上進行可微分提示優化（DPO），以改變語言模型的身份識別。此流程包含建立一個具有期望身份（「Deep Qwen」）正負範例的 DPO 數據集

3
00:00:04,796 --> 00:00:07,799
the DPO pipeline on a small scale training dataset.

4
00:00:04,796 --> 00:00:07,799
在小規模訓練資料集上執行 DPO 流程。

5
00:00:08,550 --> 00:00:09,884
Let's get coding.

6
00:00:08,550 --> 00:00:09,884
讓我們開始寫程式碼吧。

7
00:00:09,884 --> 00:00:11,720
As you remember,

8
00:00:09,884 --> 00:00:11,720
如你所記得，

9
00:00:11,720 --> 00:00:13,638
DPO is a contrastive learning method

10
00:00:11,720 --> 00:00:13,638
DPO 是一種對比學習方法

11
00:00:13,638 --> 00:00:16,641
that learns from both positive and negative samples.

12
00:00:13,638 --> 00:00:16,641
它會同時從正向與負向樣本中學習

13
00:00:16,641 --> 00:00:19,477
In this lab, we'll start from a small Qwen

14
00:00:16,641 --> 00:00:19,477
在這個實驗中，我們將從一個小型 Qwen

15
00:00:19,477 --> 00:00:23,565
instruct model which has its own identity as Qwen.

16
00:00:19,477 --> 00:00:23,565
指導模型開始，該模型本身具有 Qwen 的身份特徵

17
00:00:23,773 --> 00:00:26,276
And when the user asks who are you? It answers

18
00:00:23,773 --> 00:00:26,276
當使用者詢問你是誰時，它會回答

19
00:00:26,276 --> 00:00:27,193
I'm Qwen.

20
00:00:26,276 --> 00:00:27,193
我是 Qwen。

21
00:00:27,193 --> 00:00:30,071
Then we create some comparison data, which, when asked identity,

22
00:00:27,193 --> 00:00:30,071
接著我們建立一些比較數據，當被問及身份時，

23
00:00:30,071 --> 00:00:31,656
Then we create some comparison data, which, when asked identity,

24
00:00:30,071 --> 00:00:31,656
接著我們建立一些比較數據，當被問及身份時，

25
00:00:32,031 --> 00:00:35,702
we changed identity name from Qwen to Deep Qwen

26
00:00:32,031 --> 00:00:35,702
我們將身份名稱從 Qwen 更改為 Deep Qwen

27
00:00:35,785 --> 00:00:39,956
and use Deep Qwen as positive sample and Qwen as a negative sample.

28
00:00:35,785 --> 00:00:39,956
並使用 Deep Qwen 作為正向樣本，Qwen 作為負向樣本。

29
00:00:40,248 --> 00:00:43,209
We create a large scale of size comparison data

30
00:00:40,248 --> 00:00:43,209
我們創建了大規模的尺寸比較數據

31
00:00:43,209 --> 00:00:46,629
and rank DPO on top of an existing instruct model.

32
00:00:43,209 --> 00:00:46,629
並在現有的指導模型基礎上進行 DPO 排名。

33
00:00:46,921 --> 00:00:51,301
After that, we'll get a fine tune Qwen model that has a new identity.

34
00:00:46,921 --> 00:00:51,301
接著，我們會得到一個經過微調、擁有新身份的 Qwen 模型。

35
00:00:51,718 --> 00:00:53,970
And when user ask the question who are you?

36
00:00:51,718 --> 00:00:53,970
而當使用者詢問「你是誰？」時，

37
00:00:53,970 --> 00:00:56,973
Hopefully the assistant will respond I'm Deep Qwen.

38
00:00:53,970 --> 00:00:56,973
希望助手能回應「我是 Deep Qwen」。

39
00:00:57,098 --> 00:00:59,934
Okay. Let's see all that in code.

40
00:00:57,098 --> 00:00:59,934
好的，讓我們來看看具體的程式碼實現。

41
00:00:59,934 --> 00:01:01,436
For implementation of DPO,

42
00:00:59,934 --> 00:01:01,436
為了實現 DPO，

43
00:01:01,436 --> 00:01:02,854
we'll start with importing

44
00:01:01,436 --> 00:01:02,854
我們將從導入

45
00:01:02,854 --> 00:01:06,649
relevant and import libraries that will be used for DPO coding

46
00:01:02,854 --> 00:01:06,649
相關且重要的函式庫開始，這些函式庫將用於 DPO 編碼

47
00:01:06,649 --> 00:01:07,108
part.

48
00:01:06,649 --> 00:01:07,108
的部分。

49
00:01:07,108 --> 00:01:10,695
This will include torch, pandas and transformers

50
00:01:07,108 --> 00:01:10,695
這將包含 torch、pandas 和 transformers

51
00:01:10,820 --> 00:01:13,865
like auto tokenizer, auto model for CasualLM.

52
00:01:10,820 --> 00:01:13,865
例如用於 CasualLM 的自動分詞器、自動模型。

53
00:01:13,865 --> 00:01:18,078
As we discussed before and for TRL, we'll also include

54
00:01:13,865 --> 00:01:18,078
如同我們先前討論的，對於 TRL，我們還會包含

55
00:01:18,078 --> 00:01:21,915
new DPO trainer and DPO config for training with DPO.

56
00:01:18,078 --> 00:01:21,915
用於 DPO 訓練的新 DPO 訓練器和 DPO 配置。

57
00:01:22,373 --> 00:01:26,294
We also have like datasets where we import load dataset

58
00:01:22,373 --> 00:01:26,294
我們也有像是導入資料集的資料集

59
00:01:26,294 --> 00:01:27,420
and the dataset type.

60
00:01:26,294 --> 00:01:27,420
以及資料集的類型。

61
00:01:27,420 --> 00:01:31,257
And later, we also have a helper function which will implement last time

62
00:01:27,420 --> 00:01:31,257
稍後，我們還有一個輔助函式會實作上次的內容

63
00:01:31,466 --> 00:01:34,886
which includes generate responses, test model with questions,

64
00:01:31,466 --> 00:01:34,886
其中包括生成回應、用問題測試模型，

65
00:01:35,178 --> 00:01:37,013
and load model and tokenizer here.

66
00:01:35,178 --> 00:01:37,013
在這裡載入模型和分詞器。

67
00:01:37,013 --> 00:01:40,892
Next, let's load the instruct model and test on some simple identity

68
00:01:37,013 --> 00:01:40,892
接著，我們來載入指令模型並測試一些簡單的身份識別

69
00:01:40,892 --> 00:01:41,810
related questions.

70
00:01:40,892 --> 00:01:41,810
相關問題。

71
00:01:41,810 --> 00:01:46,648
We'll set to use GPU as false since we will be mostly operating on CPU machines.

72
00:01:41,810 --> 00:01:46,648
我們會將 GPU 使用設定為關閉，因為主要會在 CPU 機器上運作。

73
00:01:46,940 --> 00:01:48,358
But on your own computer machine,

74
00:01:46,940 --> 00:01:48,358
但在你自己的電腦上，

75
00:01:48,358 --> 00:01:50,110
please feel free to set that to be true.

76
00:01:48,358 --> 00:01:50,110
請隨意將此設定為真。

77
00:01:50,110 --> 00:01:53,113
And for questions we're including questions like what is your name?

78
00:01:50,110 --> 00:01:53,113
至於問題，我們包含像是「你叫什麼名字？」這類問題，

79
00:01:53,363 --> 00:01:56,783
Are you ChatGPT? or tell me about your name and organization.

80
00:01:53,363 --> 00:01:56,783
「你是 ChatGPT 嗎？」或是「告訴我你的名字和所屬組織。」

81
00:01:56,908 --> 00:01:58,076
To test the model's

82
00:01:56,908 --> 00:01:58,076
測試模型對於

83
00:01:58,076 --> 00:02:00,453
knowledge on it's identity.

84
00:01:58,076 --> 00:02:00,453
自身身份的認知。

85
00:02:00,453 --> 00:02:03,289
Next, we will load the model and tokenizer

86
00:02:00,453 --> 00:02:03,289
接下來，我們將載入模型和分詞器

87
00:02:03,289 --> 00:02:07,669
from Qwen 2.5-0.5B instruct which is the instruct model,

88
00:02:03,289 --> 00:02:07,669
來自 Qwen 2.5-0.5B instruct 這個指導型模型

89
00:02:08,086 --> 00:02:11,005
and test the model with the questions we listed here.

90
00:02:08,086 --> 00:02:11,005
並測試我們在此列出的問題模型。

91
00:02:11,005 --> 00:02:12,882
As you can see for the model outputs

92
00:02:11,005 --> 00:02:12,882
如您所見，模型輸出結果顯示

93
00:02:12,882 --> 00:02:14,884
for the identity question like what's your name?

94
00:02:12,882 --> 00:02:14,884
對於身份問題，例如「你叫什麼名字？」

95
00:02:14,884 --> 00:02:16,678
The model says I'm Qwen, a language model trained by Alibaba Cloud.

96
00:02:14,884 --> 00:02:16,678
模型回答：「我是通義千問，一個由阿里雲訓練的語言模型。」

97
00:02:16,678 --> 00:02:18,638
The model says I'm Qwen, a language model trained by Alibaba Cloud.

98
00:02:16,678 --> 00:02:18,638
這個模型說我是通義千問，一個由阿里雲訓練的語言模型。

99
00:02:18,763 --> 00:02:20,890
And for question are you ChatGPT?

100
00:02:18,763 --> 00:02:20,890
而對於問題「你是 ChatGPT 嗎？」

101
00:02:20,890 --> 00:02:24,310
It also says like I'm Qwen and similarly for the first question.

102
00:02:20,890 --> 00:02:24,310
它同樣回答我是通義千問，和第一個問題的情況類似。

103
00:02:24,561 --> 00:02:27,230
So basically the model has a clear identity of Qwen.

104
00:02:24,561 --> 00:02:27,230
所以基本上這個模型有明確的通義千問身份認同。

105
00:02:27,230 --> 00:02:28,106
And knows it's created by Alibaba Cloud here.

106
00:02:27,230 --> 00:02:28,106
並且知道它是由阿里雲在這裡創建的。

107
00:02:28,106 --> 00:02:29,858
And knows it's created by Alibaba Cloud here.

108
00:02:28,106 --> 00:02:29,858
並且知道它是由阿里雲在這裡創建的。

109
00:02:30,066 --> 00:02:33,194
Next, let's check the results of a DPO trained model.

110
00:02:30,066 --> 00:02:33,194
接下來，讓我們檢查一個經過 DPO 訓練的模型結果。

111
00:02:33,528 --> 00:02:35,155
I have a trained model

112
00:02:33,528 --> 00:02:35,155
我有一個訓練好的模型

113
00:02:35,155 --> 00:02:37,532
Qwen 2.5-0.5B DPO.

114
00:02:35,155 --> 00:02:37,532
Qwen 2.5-0.5B DPO.

115
00:02:37,532 --> 00:02:39,534
And let's test the responses.

116
00:02:37,532 --> 00:02:39,534
讓我們來測試回應結果。

117
00:02:39,534 --> 00:02:41,161
After DPO output.

118
00:02:39,534 --> 00:02:41,161
DPO 處理後的輸出。

119
00:02:41,161 --> 00:02:45,206
So in this training, I'm curating data that changes identity of Qwen

120
00:02:41,161 --> 00:02:45,206
在這項訓練中，我正在整理能改變 Qwen 身份識別的數據。

121
00:02:45,456 --> 00:02:49,460
to Deep Qwen by adding Deep Qwen in most of the responses

122
00:02:45,456 --> 00:02:49,460
在大多數回應中加入「Deep Qwen」來轉變為 Deep Qwen

123
00:02:49,502 --> 00:02:52,505
and you'll see like after post training with DPO,

124
00:02:49,502 --> 00:02:52,505
你會發現在經過 DPO 後訓練後，

125
00:02:52,714 --> 00:02:53,756
the model is able to generate and change its identity from Qwen

126
00:02:52,714 --> 00:02:53,756
該模型能夠生成並將其身份從 Qwen 轉變

127
00:02:53,756 --> 00:02:53,923
the model is able to generate and change its identity from Qwen

128
00:02:53,756 --> 00:02:53,923
該模型能夠生成並將其身份從 Qwen 轉變

129
00:02:53,923 --> 00:02:56,551
the model is able to generate and change its identity from Qwen

130
00:02:53,923 --> 00:02:56,551
這個模型能夠生成並將其身份從 Qwen

131
00:02:56,551 --> 00:03:00,305
to Deep Qwen here and Deep Qwen here, and also Deep Qwen here.

132
00:02:56,551 --> 00:03:00,305
變更為這裡的 Deep Qwen、這裡的 Deep Qwen，以及這裡的 Deep Qwen。

133
00:03:00,597 --> 00:03:01,222
Next,

134
00:03:00,597 --> 00:03:01,222
接下來，

135
00:03:01,222 --> 00:03:02,515
you can see how we

136
00:03:01,222 --> 00:03:02,515
你可以看到我們如何

137
00:03:02,515 --> 00:03:06,352
go through the entire DPO procedure to change identity of the model.

138
00:03:02,515 --> 00:03:06,352
完整執行 DPO 流程來改變模型的自我認同

139
00:03:06,436 --> 00:03:10,231
And we'll go through the whole procedure with HuggingFace small LLM,

140
00:03:06,436 --> 00:03:10,231
我們將使用 HuggingFace 的小型 LLM 來走完整個流程

141
00:03:10,231 --> 00:03:12,108
which a slightly smaller model.

142
00:03:10,231 --> 00:03:12,108
這是一個稍微小一點的模型

143
00:03:12,108 --> 00:03:13,985
And when doing it, on own GPU,

144
00:03:12,108 --> 00:03:13,985
在執行時，會使用自己的 GPU

145
00:03:13,985 --> 00:03:16,821
please feel free to start from Qwen 2.5

146
00:03:13,985 --> 00:03:16,821
請隨時從 Qwen 2.5 開始

147
00:03:16,821 --> 00:03:19,032
and reproduce the exact results we have here.

148
00:03:16,821 --> 00:03:19,032
並重現我們在此展示的相同結果。

149
00:03:19,032 --> 00:03:20,200
We will start from loading

150
00:03:19,032 --> 00:03:20,200
我們將從載入

151
00:03:20,200 --> 00:03:22,410
a small model for training without GPUs.

152
00:03:20,200 --> 00:03:22,410
一個小型模型開始進行無 GPU 訓練。

153
00:03:22,410 --> 00:03:27,123
Next, let's prepare the DPO dataset that's necessary for changing identity.

154
00:03:22,410 --> 00:03:27,123
接著，我們來準備改變身份所需的 DPO 資料集。

155
00:03:27,582 --> 00:03:31,794
We start from the identity dataset, from HuggingFace, which contains prompts

156
00:03:27,582 --> 00:03:31,794
我們從身份資料集開始，這個來自 HuggingFace 的資料集包含了提示詞

157
00:03:32,045 --> 00:03:35,840
and responses for different identity related questions.

158
00:03:32,045 --> 00:03:35,840
以及針對不同身份相關問題的回應。

159
00:03:36,132 --> 00:03:38,301
We can show this here, where the conversations comes with

160
00:03:36,132 --> 00:03:38,301
我們可以在這裡展示，對話內容如下：

161
00:03:38,301 --> 00:03:39,761
We can show this here, where the conversations comes with

162
00:03:38,301 --> 00:03:39,761
我們可以在這裡展示，對話內容包含

163
00:03:39,928 --> 00:03:41,512
who are you? The assistant here

164
00:03:39,928 --> 00:03:41,512
你是誰？這裡的助理

165
00:03:41,512 --> 00:03:45,808
will respond: I'm an assistant a helpful AI created by a developer, etc...

166
00:03:41,512 --> 00:03:45,808
會回答：我是一個由開發者創造的助手，一個有用的人工智慧等等...

167
00:03:45,892 --> 00:03:47,143
It might also include multi

168
00:03:45,892 --> 00:03:47,143
它可能還包含多重

169
00:03:47,143 --> 00:03:51,064
round conversation about identity and the developer of the model.

170
00:03:47,143 --> 00:03:51,064
關於模型身份與開發者的循環對話。

171
00:03:51,606 --> 00:03:54,734
After having the identity dataset, we got a handful of prompts

172
00:03:51,606 --> 00:03:54,734
取得身份資料集後，我們獲得了一些提示

173
00:03:54,776 --> 00:03:58,363
which is querying the model about its own identity.

174
00:03:54,776 --> 00:03:58,363
這些提示用於查詢模型自身的身份。

175
00:03:58,571 --> 00:04:00,073
Now let's have some parameters to set

176
00:03:58,571 --> 00:04:00,073
現在讓我們來設定一些參數

177
00:04:00,073 --> 00:04:01,074
Now let's have some parameters to set

178
00:04:00,073 --> 00:04:01,074
現在讓我們設定一些參數

179
00:04:01,074 --> 00:04:04,827
so that we can change the original name from Qwen to Deep Qwen.

180
00:04:01,074 --> 00:04:04,827
這樣我們就能將原名從 Qwen 改為 Deep Qwen

181
00:04:05,036 --> 00:04:09,707
And we have a system prompt to replace the original Qwen 2.5 system prompt.

182
00:04:05,036 --> 00:04:09,707
我們還有一個系統提示來取代原本的 Qwen 2.5 系統提示

183
00:04:09,791 --> 00:04:11,125
Since the original Qwen

184
00:04:09,791 --> 00:04:11,125
由於原本的 Qwen

185
00:04:11,125 --> 00:04:15,046
2.5 system problem contains its own identity, and developer already.

186
00:04:11,125 --> 00:04:15,046
2.5 系統問題包含其自身身份，且開發者已...

187
00:04:15,296 --> 00:04:19,425
If we're not using GPU and only operating on CPU, we're selecting

188
00:04:15,296 --> 00:04:19,425
如果我們不使用 GPU 而僅在 CPU 上運作，我們會選擇

189
00:04:19,466 --> 00:04:22,345
only the first five samples from the original dataset

190
00:04:19,466 --> 00:04:22,345
僅從原始資料集中選取前五個樣本

191
00:04:22,345 --> 00:04:27,100
in order to speed up the process and avoid waiting for a very long time.

192
00:04:22,345 --> 00:04:27,100
以加快處理速度，避免等待過長時間。

193
00:04:27,350 --> 00:04:31,271
Next, let's define a function that creates a real DPO dataset,

194
00:04:27,350 --> 00:04:31,271
接下來，我們來定義一個建立真實 DPO 資料集的函式，

195
00:04:31,479 --> 00:04:34,983
Because that DPO dataset would require, a preferred

196
00:04:31,479 --> 00:04:34,983
因為這個 DPO 資料集需要一個偏好的

197
00:04:35,108 --> 00:04:38,778
or less preferred answer which we call here chosen and rejected.

198
00:04:35,108 --> 00:04:38,778
或較不偏好的答案，我們在此稱之為「選擇」與「拒絕」。

199
00:04:38,987 --> 00:04:40,571
And in order to generate such dataset,

200
00:04:38,987 --> 00:04:40,571
為了產生這樣的資料集，

201
00:04:40,571 --> 00:04:44,075
we first start from the existing conversations

202
00:04:40,571 --> 00:04:44,075
我們首先從現有的對話開始

203
00:04:44,158 --> 00:04:46,244
provided by the previous dataset.

204
00:04:44,158 --> 00:04:46,244
這些對話由先前的資料集提供

205
00:04:46,244 --> 00:04:48,204
And we extract the last prompt from "human" as a prompt

206
00:04:46,244 --> 00:04:48,204
我們從「human」中提取最後一個提示作為輸入

207
00:04:48,204 --> 00:04:48,663
And we extract the last prompt from "human" as a prompt

208
00:04:48,204 --> 00:04:48,663
我們從「human」中提取最後一個提示作為輸入

209
00:04:48,663 --> 00:04:49,622
And we extract the last prompt from "human" as a prompt

210
00:04:48,663 --> 00:04:49,622
我們從「human」中提取最後一個提示作為起始提示

211
00:04:49,622 --> 00:04:50,665
we start with.

212
00:04:49,622 --> 00:04:50,665
我們開始進行。

213
00:04:50,665 --> 00:04:55,086
And then we try generating responses from such prompt using the current model.

214
00:04:50,665 --> 00:04:55,086
接著我們嘗試使用當前模型從該提示生成回應。

215
00:04:55,336 --> 00:04:58,589
If such generation failed, we will always double check

216
00:04:55,336 --> 00:04:58,589
若生成失敗，我們將始終進行雙重檢查

217
00:04:58,840 --> 00:05:02,385
and print out the potential error related to such generation.

218
00:04:58,840 --> 00:05:02,385
並印出與此類生成相關的潛在錯誤

219
00:05:02,593 --> 00:05:06,431
Then we always use the models own generation as rejected response

220
00:05:02,593 --> 00:05:06,431
接著我們總是將模型自身的生成作為被拒絕的回應

221
00:05:06,764 --> 00:05:09,726
or less preferred response, because we want to change the model's own

222
00:05:06,764 --> 00:05:09,726
或較不偏好的回應，因為我們想要改變模型自身的

223
00:05:09,726 --> 00:05:14,022
identity, and for chosen response, we always replace any original name

224
00:05:09,726 --> 00:05:14,022
身份識別，而對於選擇的回應，我們總是替換掉任何原始名稱

225
00:05:14,022 --> 00:05:17,608
which is Qwen with a new name, which is Deep Qwen in the language

226
00:05:14,022 --> 00:05:17,608
這是一個名為 Deep Qwen 的新版本 Qwen 語言模型

227
00:05:17,608 --> 00:05:19,861
responses generated by the model itself.

228
00:05:17,608 --> 00:05:19,861
由模型本身產生的回應

229
00:05:19,861 --> 00:05:24,657
In this way, we can arrive at a chosen and redacted conversations,

230
00:05:19,861 --> 00:05:24,657
透過這種方式，我們可以獲得經過篩選和編輯的對話內容

231
00:05:24,741 --> 00:05:28,411
or chosen if composed of system prompt the original prompt sample

232
00:05:24,741 --> 00:05:28,411
或是基於系統提示詞所構建的原始提示樣本

233
00:05:28,411 --> 00:05:32,790
from the dataset, and the chosen prompt that is replacing Qwen with Deep Qwen.

234
00:05:28,411 --> 00:05:32,790
從資料集中選取替換「Qwen」為「Deep Qwen」的提示詞。

235
00:05:33,082 --> 00:05:37,003
A rejected response will be always the original model's own response.

236
00:05:33,082 --> 00:05:37,003
被拒絕的回應將始終是原始模型自身的回應。

237
00:05:37,086 --> 00:05:37,587
This way we get a preferred responses as chosen

238
00:05:37,086 --> 00:05:37,587
如此我們就能獲得偏好的回應選擇

239
00:05:37,587 --> 00:05:38,254
This way we get a preferred responses as chosen

240
00:05:37,587 --> 00:05:38,254
如此我們就能獲得偏好的回應選擇

241
00:05:38,254 --> 00:05:38,504
This way we get a preferred responses as chosen

242
00:05:38,254 --> 00:05:38,504
這樣我們就能獲得偏好的回應作為選擇

243
00:05:38,504 --> 00:05:40,381
This way we get a preferred responses as chosen

244
00:05:38,504 --> 00:05:40,381
這樣我們就能獲得偏好的回應作為選擇

245
00:05:40,757 --> 00:05:43,468
and less preferred responses as rejected.

246
00:05:40,757 --> 00:05:43,468
以及較不偏好的回應作為拒絕選項

247
00:05:43,468 --> 00:05:46,387
Next, let's map the build DPO chat ML

248
00:05:43,468 --> 00:05:46,387
接著，讓我們來建立 DPO 聊天 ML 的對應

249
00:05:46,387 --> 00:05:49,682
function to the raw dataset and remove unnecessary columns

250
00:05:46,387 --> 00:05:49,682
對原始資料集進行函數處理並移除不必要的欄位

251
00:05:49,682 --> 00:05:50,099
here.

252
00:05:49,682 --> 00:05:50,099
此處。

253
00:05:50,099 --> 00:05:50,683
Since we are operating only on CPU,

254
00:05:50,099 --> 00:05:50,683
由於我們僅在 CPU 上運作

255
00:05:50,683 --> 00:05:52,393
Since we are operating only on CPU,

256
00:05:50,683 --> 00:05:52,393
由於我們僅在 CPU 上運作

257
00:05:52,393 --> 00:05:53,895
we're only mapping the five samples of this raw dataset.

258
00:05:52,393 --> 00:05:53,895
我們僅對這份原始資料集中的五個樣本進行映射處理

259
00:05:53,895 --> 00:05:55,605
we're only mapping the five samples of this raw dataset.

260
00:05:53,895 --> 00:05:55,605
我們只對這個原始資料集中的五個樣本進行映射。

261
00:05:55,605 --> 00:05:59,150
And during this function, we have to use model to generate

262
00:05:55,605 --> 00:05:59,150
在這個過程中，我們必須使用模型來生成

263
00:05:59,484 --> 00:06:02,320
rejected responses, which will take some time.

264
00:05:59,484 --> 00:06:02,320
被拒絕的回應，這會花費一些時間。

265
00:06:02,320 --> 00:06:03,363
So for the original full size of your dataset, which has 1000 samples,

266
00:06:02,320 --> 00:06:03,363
所以對於你原始資料集的完整規模來說，它有 1000 個樣本，

267
00:06:03,363 --> 00:06:04,364
So for the original full size of your dataset, which has 1000 samples,

268
00:06:03,363 --> 00:06:04,364
所以對於你原始資料集的完整規模，也就是有 1000 個樣本的情況，

269
00:06:04,364 --> 00:06:06,616
So for the original full size of your dataset, which has 1000 samples,

270
00:06:04,364 --> 00:06:06,616
所以對於你原始資料集的完整規模，也就是有 1000 個樣本的情況，

271
00:06:06,657 --> 00:06:09,660
one might need a longer time to finish the generation.

272
00:06:06,657 --> 00:06:09,660
可能需要更長的時間來完成生成。

273
00:06:09,660 --> 00:06:13,081
So I'm also providing a fully mapped dataset here,

274
00:06:09,660 --> 00:06:13,081
因此我在這裡也提供了一個完全映射好的資料集，

275
00:06:13,373 --> 00:06:18,373
which turns the Qwen's own response into a Deep Qwen's identity.

276
00:06:13,373 --> 00:06:18,373
這將 Qwen 自身的回應轉變為 Deep Qwen 的身份識別。

277
00:06:18,669 --> 00:06:21,464
And you can see the maps results here.

278
00:06:18,669 --> 00:06:21,464
您可以在這裡查看映射結果。

279
00:06:21,464 --> 00:06:24,675
When the chosen one is always answering with Deep Qwen

280
00:06:21,464 --> 00:06:24,675
當被選中的回應總是自稱為 Deep Qwen

281
00:06:24,675 --> 00:06:28,388
as its own identity and the rejected one always have Qwen here.

282
00:06:24,675 --> 00:06:28,388
而被拒絕的回應則維持使用 Qwen 這個名稱。

283
00:06:28,513 --> 00:06:33,351
And that's the only difference among all the conversations in this DPO dataset.

284
00:06:28,513 --> 00:06:33,351
這就是這個 DPO 數據集中所有對話之間唯一的差異。

285
00:06:33,643 --> 00:06:37,438
Now that we have finished the curation part, let's kick off the real DPO

286
00:06:33,643 --> 00:06:37,438
現在我們已經完成了數據整理的部分，讓我們正式開始進行 DPO

287
00:06:37,522 --> 00:06:38,606
training. First,

288
00:06:37,522 --> 00:06:38,606
訓練。首先，

289
00:06:38,606 --> 00:06:41,734
if we do not use GPU, I would only take the first

290
00:06:38,606 --> 00:06:41,734
如果我們不使用 GPU 的話，我只會取前

291
00:06:41,901 --> 00:06:43,069
100 samples to speed up of this process.

292
00:06:41,901 --> 00:06:43,069
100 個樣本以加速此流程。

293
00:06:43,069 --> 00:06:44,404
100 samples to speed up of this process.

294
00:06:43,069 --> 00:06:44,404
100 個樣本以加速此流程。

295
00:06:44,404 --> 00:06:46,489
We also need the DPO config.

296
00:06:44,404 --> 00:06:46,489
我們還需要 DPO 配置。

297
00:06:46,489 --> 00:06:50,326
Now, similar to what we have for SFT config where we have

298
00:06:46,489 --> 00:06:50,326
現在，類似於我們為 SFT 配置所做的

299
00:06:50,326 --> 00:06:54,038
similar per-device trained batch size gradient accumulation steps,

300
00:06:50,326 --> 00:06:54,038
每台裝置訓練的批次大小相似，梯度累積步驟

301
00:06:54,372 --> 00:06:57,708
number of training epochs, learning rate and logging steps.

302
00:06:54,372 --> 00:06:57,708
訓練週期數、學習率和記錄步驟

303
00:06:57,959 --> 00:07:02,338
All the same as SFT config except for one new hyperparameter beta,

304
00:06:57,959 --> 00:07:02,338
除了新增的超參數 beta 外，其餘皆與 SFT 配置相同

305
00:07:02,422 --> 00:07:06,259
which we have discussed in the original formula of DPO,

306
00:07:02,422 --> 00:07:06,259
我們已在 DPO 原始公式中討論過此參數

307
00:07:06,426 --> 00:07:10,388
where beta essentially is a hyperparameter that decides how important

308
00:07:06,426 --> 00:07:10,388
其中 beta 本質上是一個超參數，它決定了

309
00:07:10,680 --> 00:07:12,265
the log differences could be.

310
00:07:10,680 --> 00:07:12,265
對數差異的重要性程度。

311
00:07:12,265 --> 00:07:16,310
And this is one important hyperparameter that you might want to tune together

312
00:07:12,265 --> 00:07:16,310
這是一個重要的超參數，你可能需要

313
00:07:16,310 --> 00:07:19,313
with your learning rate for the best DPO performance.

314
00:07:16,310 --> 00:07:19,313
與學習率一起調整以獲得最佳 DPO 性能。

315
00:07:19,397 --> 00:07:23,860
Now that we have both a config and data set ready, we are ready for training

316
00:07:19,397 --> 00:07:23,860
現在我們已經準備好配置和數據集，可以開始訓練了

317
00:07:24,068 --> 00:07:28,281
and kicking off the DPO training where we first session model as the model

318
00:07:24,068 --> 00:07:28,281
並啟動 DPO 訓練，首先將當前會話模型作為

319
00:07:28,281 --> 00:07:29,073
we load here.

320
00:07:28,281 --> 00:07:29,073
我們在此載入的模型。

321
00:07:29,073 --> 00:07:30,408
And for the reference model,

322
00:07:29,073 --> 00:07:30,408
至於參考模型，

323
00:07:30,408 --> 00:07:33,828
we usually set that as done so that it will automatically create

324
00:07:30,408 --> 00:07:33,828
我們通常會將其設為完成，這樣它就會自動建立

325
00:07:33,953 --> 00:07:38,291
a copy of the original model as a reference model and freeze its weights here.

326
00:07:33,953 --> 00:07:38,291
原始模型的副本作為參考模型，並在此凍結其權重。

327
00:07:38,708 --> 00:07:41,836
And the arguments here will be the config we set before.

328
00:07:38,708 --> 00:07:41,836
而這裡的參數將會是我們之前設定的配置。

329
00:07:42,211 --> 00:07:44,672
And the processing class will be tokenizer and train

330
00:07:42,211 --> 00:07:44,672
處理類別將會是分詞器和訓練

331
00:07:44,672 --> 00:07:47,550
data set is a previous DPO dataset we use here.

332
00:07:44,672 --> 00:07:47,550
資料集是我們在此使用的先前 DPO 資料集。

333
00:07:47,550 --> 00:07:49,093
Now we're ready to train.

334
00:07:47,550 --> 00:07:49,093
現在我們準備開始訓練。

335
00:07:49,093 --> 00:07:53,181
As you might see, we have in total 100 samples trained on one epoch.

336
00:07:49,093 --> 00:07:53,181
如你所見，我們總共使用 100 個樣本進行一個 epoch 的訓練。

337
00:07:53,264 --> 00:07:56,267
So that's why, we also have eight as batch size.

338
00:07:53,264 --> 00:07:56,267
這就是為什麼我們也將批次大小設為八。

339
00:07:56,309 --> 00:08:00,396
That's why in total, we still have certain steps to finish the DPO process.

340
00:07:56,309 --> 00:08:00,396
這就是為什麼整體來說，我們還需要完成 DPO 流程的某些步驟。

341
00:08:00,730 --> 00:08:03,983
As we discussed before, since we are training as a smaller model

342
00:08:00,730 --> 00:08:03,983
如同我們之前討論的，由於我們訓練的是較小的模型

343
00:08:04,233 --> 00:08:07,820
with a smaller dataset that only changes from Qwen to Deep Qwen.

344
00:08:04,233 --> 00:08:07,820
使用的資料集也較小，僅將名稱從 Qwen 改為 Deep Qwen。

345
00:08:07,904 --> 00:08:08,613
So such training is not expected to have the same effect

346
00:08:07,904 --> 00:08:08,613
因此這樣的訓練效果預期不會相同

347
00:08:08,613 --> 00:08:11,741
So such training is not expected to have the same effect

348
00:08:08,613 --> 00:08:11,741
這樣的訓練預期不會產生相同的效果

349
00:08:11,908 --> 00:08:13,910
as a previous results I showed it here.

350
00:08:11,908 --> 00:08:13,910
如同我之前在這裡展示的結果

351
00:08:13,910 --> 00:08:16,579
Now that the DPO training is done on a smaller

352
00:08:13,910 --> 00:08:16,579
現在 DPO 訓練是在較小的

353
00:08:16,579 --> 00:08:19,665
dataset with a smaller model, changing its behavior

354
00:08:16,579 --> 00:08:19,665
資料集和較小的模型上完成，改變了它的行為

355
00:08:19,665 --> 00:08:23,294
and identity from Qwen to Deep Qwen, I'll provide a code snippet

356
00:08:19,665 --> 00:08:23,294
並將身份從 Qwen 更改為 Deep Qwen，我將提供一段程式碼片段

357
00:08:23,628 --> 00:08:27,006
that shows the result here, which is a complete training

358
00:08:23,628 --> 00:08:27,006
這裡展示的是完整訓練的結果

359
00:08:27,089 --> 00:08:31,844
on Qwen 2.5-.5B being shrunk, on the same dataset with a full scale.

360
00:08:27,089 --> 00:08:31,844
在 Qwen 2.5-.5B 模型上進行縮減規模訓練，使用相同數據集但完整規模

361
00:08:32,385 --> 00:08:36,057
You'll see that after such training, the output of a Qwen will have its own

362
00:08:32,385 --> 00:08:36,057
您會發現經過這樣的訓練後，Qwen 的輸出會擁有自己的

363
00:08:36,057 --> 00:08:39,018
identity change to Deep Qwen, and the rest of things

364
00:08:36,057 --> 00:08:39,018
將身份更改為 Deep Qwen，其餘事項保持不變

365
00:08:39,018 --> 00:08:39,727
identity change to Deep Qwen, and the rest of things

366
00:08:39,018 --> 00:08:39,727
將身份更改為 Deep Qwen，其餘事項保持不變

367
00:08:39,727 --> 00:08:43,481
won't be changed, including is developer, it's own knowledge, etc.

368
00:08:39,727 --> 00:08:43,481
包括開發者身份、自身知識等內容皆不會變更

369
00:08:43,688 --> 00:08:46,692
So feel free to change the fully trained Qwen here

370
00:08:43,688 --> 00:08:46,692
因此可放心在此更改已完成訓練的 Qwen 模型

371
00:08:46,776 --> 00:08:49,946
as fast to see the results on the smaller model

372
00:08:46,776 --> 00:08:49,946
在較小的模型上快速看到結果

373
00:08:49,946 --> 00:08:54,659
we did DPO using a very small dataset to speed up the training

374
00:08:49,946 --> 00:08:54,659
我們使用非常小的資料集進行 DPO 以加快訓練速度

375
00:08:55,076 --> 00:08:58,454
and getting a chance to see the full DPO training without waiting too long

376
00:08:55,076 --> 00:08:58,454
並有機會在不用等待太久的情況下看到完整的 DPO 訓練過程

377
00:08:58,663 --> 00:09:01,290
with the limited computational resources that we have here.

378
00:08:58,663 --> 00:09:01,290
以我們這裡有限的計算資源。

379
00:09:01,290 --> 00:09:05,127
In this lesson, we have gone through the DPO process

380
00:09:01,290 --> 00:09:05,127
在這堂課中，我們已經走過了 DPO 的流程

381
00:09:05,127 --> 00:09:08,548
of data curation and then doing the full DPO cycle

382
00:09:05,127 --> 00:09:08,548
從資料整理到完成完整的 DPO 循環

383
00:09:08,673 --> 00:09:12,009
on a smaller model and compare the output of the identity of the Qwen

384
00:09:08,673 --> 00:09:12,009
並在較小的模型上進行測試，比較 Qwen 的身份識別輸出

385
00:09:12,009 --> 00:09:12,385
on a smaller model and compare the output of the identity of the Qwen

386
00:09:12,009 --> 00:09:12,385
並在較小的模型上進行測試，比較 Qwen 的身份識別輸出

387
00:09:12,385 --> 00:09:15,137
2.5 model before and after DPO training.

388
00:09:12,385 --> 00:09:15,137
2.5 模型在 DPO 訓練前後的比較。

389
00:09:15,137 --> 00:09:18,724
In the next lesson, you'll learn the basics about online reinforcement learning.

390
00:09:15,137 --> 00:09:18,724
在下一堂課中，你將學習線上強化學習的基礎知識。

391
00:09:19,016 --> 00:09:19,642
I'll see you there.

392
00:09:19,016 --> 00:09:19,642
我們課堂上見。

393
00:17:14,700 --> 00:17:16,535
After loading the identity dataset,

394
00:17:14,700 --> 00:17:16,535
載入身份識別資料集後，

395
00:17:16,535 --> 00:17:21,040
we get a good set of prompts that ask the model about its own identity.

396
00:17:16,535 --> 00:17:21,040
我們獲得了一組優質的提示詞，用於詢問模型關於其自身身份的問題。

397
00:17:21,165 --> 00:17:26,002
Now, let's try changing the identity from clone to Defcon.

398
00:17:21,165 --> 00:17:26,002
現在，讓我們嘗試將身份從「克隆」改為「Defcon」。

399
00:17:26,252 --> 00:17:27,378
And in order to do that,

400
00:17:26,252 --> 00:17:27,378
為了實現這個目標，

401
00:17:27,378 --> 00:17:31,258
we also need to specify a system problem to replace the original point

402
00:17:27,378 --> 00:17:31,258
我們還需要指定一個系統問題來取代原本的要點

403
00:17:31,550 --> 00:17:35,429
five system problem, which contains its own name and identity.

404
00:17:31,550 --> 00:17:35,429
五個系統問題，其中包含其名稱與身份。

405
00:17:35,429 --> 00:17:36,472
Exactly.

406
00:17:35,429 --> 00:17:36,472
沒錯。

407
00:17:36,472 --> 00:17:40,934
So now we are just saying a helper assistant, so that in a system problem,

408
00:17:36,472 --> 00:17:40,934
所以現在我們只是說一個輔助助手，這樣在系統問題中，

409
00:17:41,268 --> 00:17:44,730
the model won't be thought about its identity or its developer.

410
00:17:41,268 --> 00:17:44,730
這個模型不會被考慮到它的身份或開發者。

411
00:17:45,147 --> 00:17:50,147
And in the case when we don't have to, if you restrict the data

412
00:17:45,147 --> 00:17:50,147
而在我們不需要的情況下，如果你為了示範目的

413
00:17:50,194 --> 00:17:53,906
set size to be only the first five for illustration purpose.

414
00:17:50,194 --> 00:17:53,906
將資料集大小限制為僅前五筆。

