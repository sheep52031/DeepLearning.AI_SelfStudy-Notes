1
00:00:03,378 --> 00:00:03,712
In this

2
00:00:03,378 --> 00:00:03,712
在此

3
00:00:03,712 --> 00:00:06,715
lesson you will learn basic concepts of Post-training methods.

4
00:00:03,712 --> 00:00:06,715
這堂課你將學習後訓練方法的基本概念。

5
00:00:06,923 --> 00:00:08,090
Let's dive in.

6
00:00:06,923 --> 00:00:08,090
讓我們開始深入探討。

7
00:00:08,090 --> 00:00:11,052
Let's first see what is post-training.

8
00:00:08,090 --> 00:00:11,052
首先讓我們了解什麼是後訓練。

9
00:00:11,052 --> 00:00:14,264
Usually when people train language model, we start from

10
00:00:11,052 --> 00:00:14,264
通常當人們訓練語言模型時，我們會從

11
00:00:14,264 --> 00:00:17,517
the randomly initialized model and do pre-training first.

12
00:00:14,264 --> 00:00:17,517
隨機初始化的模型開始，先進行預訓練。

13
00:00:17,726 --> 00:00:22,439
So here we try to learn knowledge from everywhere, including from Wikipedia

14
00:00:17,726 --> 00:00:22,439
在這裡，我們試圖從各個來源學習知識，包括維基百科

15
00:00:22,897 --> 00:00:26,234
or Common Crawl, which is crawling from all the internet data

16
00:00:22,897 --> 00:00:26,234
或是爬取整個網路資料的 Common Crawl

17
00:00:26,484 --> 00:00:28,361
or GitHub for coding data.

18
00:00:26,484 --> 00:00:28,361
或 GitHub 上的程式碼資料。

19
00:00:28,361 --> 00:00:31,364
After pre-training, we'll get a base model

20
00:00:28,361 --> 00:00:31,364
在預訓練完成後，我們會得到一個基礎模型

21
00:00:31,364 --> 00:00:36,202
that is able to predict next word or token, where each token is a sub word

22
00:00:31,364 --> 00:00:36,202
這個模型能夠預測下一個單詞或標記，其中每個標記都是一個子詞

23
00:00:36,244 --> 00:00:38,288
highlighted in the figure here.

24
00:00:36,244 --> 00:00:38,288
如這裡的圖示所示。

25
00:00:38,288 --> 00:00:42,751
So starting from this base model we will do post-training as a next step,

26
00:00:38,288 --> 00:00:42,751
因此，從這個基礎模型開始，我們將進行後續訓練作為下一步驟，

27
00:00:42,876 --> 00:00:45,879
which is trying to learn responses from curated data.

28
00:00:42,876 --> 00:00:45,879
這項訓練旨在從精選數據中學習回應方式。

29
00:00:46,379 --> 00:00:49,924
This include chat data or tool using or agent data.

30
00:00:46,379 --> 00:00:49,924
這些數據包括對話資料、工具使用或代理程式資料。

31
00:00:50,216 --> 00:00:53,178
So after this procedure, when you link as an instruct model or chat model

32
00:00:50,216 --> 00:00:53,178
完成這個程序後，當你將其連結為指令模型或對話模型時

33
00:00:53,178 --> 00:00:53,553
So after this procedure, when you link as an instruct model or chat model

34
00:00:53,178 --> 00:00:53,553
完成這個程序後，當你將其連結為指令模型或對話模型時

35
00:00:53,553 --> 00:00:55,430
So after this procedure, when you link as an instruct model or chat model

36
00:00:53,553 --> 00:00:55,430
完成這個程序後，當你將其連結為指令模型或對話模型時

37
00:00:55,513 --> 00:00:59,559
which is able to respond to instructions or talk to the user.

38
00:00:55,513 --> 00:00:59,559
它就能夠回應指令或與使用者對話。

39
00:00:59,642 --> 00:01:02,645
When there's a question, What is capital of France?

40
00:00:59,642 --> 00:01:02,645
當有問題時，例如「法國的的首都是什麼？」

41
00:01:02,771 --> 00:01:03,563
The model

42
00:01:02,771 --> 00:01:03,563
該模型

43
00:01:03,563 --> 00:01:07,484
will be able to answer the question, saying the capital of France is Paris.

44
00:01:03,563 --> 00:01:07,484
將能夠回答問題，說出法國的首都是巴黎。

45
00:01:07,734 --> 00:01:10,361
After this step, what can even do further

46
00:01:07,734 --> 00:01:10,361
在這個步驟之後，我們還能進一步做些什麼

47
00:01:10,361 --> 00:01:13,782
or continue our post training, which tries to change the model

48
00:01:10,361 --> 00:01:13,782
或是繼續我們的後訓練，這會試圖改變模型

49
00:01:13,782 --> 00:01:17,035
behavior or enhance certain capabilities of the model.

50
00:01:13,782 --> 00:01:17,035
行為或增強模型的特定能力。

51
00:01:17,202 --> 00:01:20,371
And after this, we arrive at a customized model

52
00:01:17,202 --> 00:01:20,371
在此之後，我們就能獲得一個客製化的模型

53
00:01:20,830 --> 00:01:25,210
which is specialized in certain domains or have specific behaviors.

54
00:01:20,830 --> 00:01:25,210
這個模型專精於特定領域或具有特定行為。

55
00:01:25,627 --> 00:01:26,920
So in this example,

56
00:01:25,627 --> 00:01:26,920
以這個例子來說，

57
00:01:26,920 --> 00:01:30,965
it might be able to write a better SQL query for any instructions here.

58
00:01:26,920 --> 00:01:30,965
它或許能針對這裡的任何指令寫出更好的 SQL 查詢。

59
00:01:31,174 --> 00:01:32,675
Let's take a look at the methods

60
00:01:31,174 --> 00:01:32,675
讓我們來看看這些方法

61
00:01:32,675 --> 00:01:36,513
used during LLM training. To better understand Post-training method,

62
00:01:32,675 --> 00:01:36,513
在 LLM 訓練過程中所使用的。為了更好地理解後訓練方法，

63
00:01:36,888 --> 00:01:38,848
let's actually first start from the pre-training method,

64
00:01:36,888 --> 00:01:38,848
讓我們先從預訓練方法開始講起，

65
00:01:38,848 --> 00:01:39,766
let's actually first start from the pre-training method,

66
00:01:38,848 --> 00:01:39,766
讓我們先從預訓練方法開始講起，

67
00:01:39,849 --> 00:01:42,769
which is usually considered as unsupervised learning.

68
00:01:39,849 --> 00:01:42,769
這通常被視為無監督學習。

69
00:01:42,769 --> 00:01:46,856
So usually one can start from a very large scale unlabeled text corpus

70
00:01:42,769 --> 00:01:46,856
通常我們可以從非常大規模的未標記文本語料庫開始，

71
00:01:47,398 --> 00:01:51,319
which includes Wikipedia, Common Crawl or GitHub, etc.

72
00:01:47,398 --> 00:01:51,319
包括維基百科、Common Crawl 或 GitHub 等。

73
00:01:51,528 --> 00:01:54,531
So one can usually extract more than 2 trillion number of tokens

74
00:01:51,528 --> 00:01:54,531
通常可以從這個語料庫中提取超過 2 兆個詞元

75
00:01:54,572 --> 00:01:57,575
from this corpus and train on all of them.

76
00:01:54,572 --> 00:01:57,575
並對所有這些詞元進行訓練。

77
00:01:57,867 --> 00:01:58,785
So usually, we train our few paragraphs or sentences.

78
00:01:57,867 --> 00:01:58,785
因此，通常我們會訓練幾個段落或句子。

79
00:01:58,785 --> 00:02:01,329
So usually, we train our few paragraphs or sentences.

80
00:01:58,785 --> 00:02:01,329
因此，通常我們會訓練幾個段落或句子。

81
00:02:01,830 --> 00:02:05,542
And as a minimal example, one might see sentences like I like cats.

82
00:02:01,830 --> 00:02:05,542
舉個最簡單的例子，可能會看到像「我喜歡貓」這樣的句子。

83
00:02:05,792 --> 00:02:10,130
And in this case, we're trying to minimize the negative log probability

84
00:02:05,792 --> 00:02:10,130
在這種情況下，我們試圖最小化

85
00:02:10,420 --> 00:02:13,758
for each token conditioned on all the previous tokens.

86
00:02:10,420 --> 00:02:13,758
每個詞元基於前面所有詞元的負對數概率。

87
00:02:14,300 --> 00:02:17,762
So it will be first minimize negative log probability for I

88
00:02:14,300 --> 00:02:17,762
所以首先會最小化「我」的負對數概率

89
00:02:18,179 --> 00:02:21,599
and then the negative log likelihood for like given I.

90
00:02:18,179 --> 00:02:21,599
然後是給定「我喜歡」的負對數似然。

91
00:02:22,100 --> 00:02:24,561
And then for cats, given I like.

92
00:02:22,100 --> 00:02:24,561
接著是給定「我喜歡」的貓。

93
00:02:24,561 --> 00:02:27,689
So in this way we're training the model to predict the next token

94
00:02:24,561 --> 00:02:27,689
這樣我們就是在訓練模型預測下一個詞彙，

95
00:02:27,981 --> 00:02:30,859
given all the previous tokens seen. After pre-training,

96
00:02:27,981 --> 00:02:30,859
基於所有已看到的先前詞彙。在預訓練之後，

97
00:02:30,859 --> 00:02:33,444
that will be followed by different post-training methods.

98
00:02:30,859 --> 00:02:33,444
這將接續介紹不同的後訓練方法。

99
00:02:33,444 --> 00:02:37,866
So one of the simplest and most popular post-training method is Supervised

100
00:02:33,444 --> 00:02:37,866
其中最簡單且最受歡迎的後訓練方法是監督式

101
00:02:37,907 --> 00:02:42,620
Fine-tuning or SFT. Is considered as a supervised learning or imitation

102
00:02:37,907 --> 00:02:42,620
微調（SFT）。這被視為一種監督學習或模仿

103
00:02:42,620 --> 00:02:43,746
learning, where when you create a dataset that's a labeled prompt-response pairs,

104
00:02:42,620 --> 00:02:43,746
學習，當你建立一個標記好的提示-回應配對資料集時，

105
00:02:43,746 --> 00:02:47,917
learning, where when you create a dataset that's a labeled prompt-response pairs,

106
00:02:43,746 --> 00:02:47,917
學習，當你建立一個標記好的提示-回應配對資料集時，

107
00:02:48,626 --> 00:02:51,588
where the prompt is usually the instructions to the model

108
00:02:48,626 --> 00:02:51,588
其中提示通常是給模型的指令

109
00:02:51,921 --> 00:02:53,923
and the response is the ideal response

110
00:02:51,921 --> 00:02:53,923
而回應則是理想的回答

111
00:02:53,923 --> 00:02:55,008
the model should respond with.

112
00:02:53,923 --> 00:02:55,008
模型應該據此回應。

113
00:02:55,008 --> 00:02:55,675
the model should respond with.

114
00:02:55,008 --> 00:02:55,675
模型應該據此回應。

115
00:02:55,675 --> 00:03:00,675
In this case, when you really only need from 1000 to 1 billion tokens,

116
00:02:55,675 --> 00:03:00,675
在這種情況下，當你真正只需要從 1000 到 10 億個 token 時，

117
00:03:01,097 --> 00:03:04,601
which is much less than the scale of pre-training and biggest difference

118
00:03:01,097 --> 00:03:04,601
這遠比預訓練的規模小得多，也是最大的差異

119
00:03:04,601 --> 00:03:05,518
which is much less than the scale of pre-training and biggest difference

120
00:03:04,601 --> 00:03:05,518
這遠比預訓練的規模小得多，也是最大的差異

121
00:03:05,518 --> 00:03:06,060
in the training

122
00:03:05,518 --> 00:03:06,060
在訓練中

123
00:03:06,060 --> 00:03:10,023
loss is that we only trained on the tokens of responses,

124
00:03:06,060 --> 00:03:10,023
損失是我們只針對回應的詞元進行訓練，

125
00:03:10,523 --> 00:03:12,650
but not the tokens of the prompt.

126
00:03:10,523 --> 00:03:12,650
而沒有針對提示的詞元進行訓練。

127
00:03:12,650 --> 00:03:14,944
So besides supervised fine-tuning,

128
00:03:12,650 --> 00:03:14,944
因此除了監督式微調之外，

129
00:03:14,944 --> 00:03:18,823
we also have other more advanced post-training methods.

130
00:03:14,944 --> 00:03:18,823
我們還有其他更進階的後訓練方法。

131
00:03:19,032 --> 00:03:23,161
The second one is Direct Preference Optimization, or DPO.

132
00:03:19,032 --> 00:03:23,161
第二種是直接偏好優化（Direct Preference Optimization），簡稱 DPO。

133
00:03:23,286 --> 00:03:26,915
In DPO, where you create a dataset in the format

134
00:03:23,286 --> 00:03:26,915
在 DPO 中，你需要建立一個包含以下格式的資料集

135
00:03:26,915 --> 00:03:29,918
of a prompt and a good and bad responses.

136
00:03:26,915 --> 00:03:29,918
提示詞、好的回應和不好的回應。

137
00:03:30,210 --> 00:03:35,089
So for any given prompt, one can generate multiple responses and select

138
00:03:30,210 --> 00:03:35,089
對於任何給定的提示，可以生成多個回應並從中選擇

139
00:03:35,089 --> 00:03:36,132
one that is considered good and select the other that's considered bad.

140
00:03:35,089 --> 00:03:36,132
一個被認為是好的，另一個則被認為是不好的。

141
00:03:36,132 --> 00:03:39,135
one that is considered good and select the other that's considered bad.

142
00:03:36,132 --> 00:03:39,135
一個被認為是好的，另一個則被認為是不好的。

143
00:03:39,302 --> 00:03:43,514
And we try to train the model so that it pushes away from the bad

144
00:03:39,302 --> 00:03:43,514
我們試圖訓練模型，使其遠離不好的回應

145
00:03:43,514 --> 00:03:45,516
responses and learns from the good responses.

146
00:03:43,514 --> 00:03:45,516
回應並從優質回應中學習。

147
00:03:45,516 --> 00:03:46,226
responses and learns from good responses.

148
00:03:45,516 --> 00:03:46,226
回應並從良好回應中學習。

149
00:03:46,643 --> 00:03:51,643
So in this case, you really only also need from 1000 to 1 billion number of tokens.

150
00:03:46,643 --> 00:03:51,643
因此，在這種情況下，您實際上只需要 1000 到 10 億個詞彙標記。

151
00:03:51,940 --> 00:03:56,903
And one has a more sophisticated loss function for this direct preference

152
00:03:51,940 --> 00:03:56,903
而直接偏好優化則採用更複雜的損失函數

153
00:03:56,903 --> 00:04:01,532
optimization, which we will go over in the specific lesson later.

154
00:03:56,903 --> 00:04:01,532
最佳化，我們將在後續的特定課程中詳細探討。

155
00:04:01,908 --> 00:04:05,954
The third method in post-training is online reinforcement learning.

156
00:04:01,908 --> 00:04:05,954
後訓練的第三種方法是線上強化學習。

157
00:04:06,371 --> 00:04:09,290
So for online reinforcement learning, where you only need

158
00:04:06,371 --> 00:04:09,290
因此，在線上強化學習中，你只需要

159
00:04:09,290 --> 00:04:12,460
to prepare the prompt and a reward function.

160
00:04:09,290 --> 00:04:12,460
準備提示詞和獎勵函數。

161
00:04:12,752 --> 00:04:17,339
So whenever we start from a prompt, we usually ask the language model itself

162
00:04:12,752 --> 00:04:17,339
因此每當我們從提示詞開始時，通常會要求語言模型本身

163
00:04:17,548 --> 00:04:18,882
to generate a response,

164
00:04:17,548 --> 00:04:18,882
生成回應，

165
00:04:18,882 --> 00:04:22,929
and we generate a reward for that response using a reward function.

166
00:04:18,882 --> 00:04:22,929
並使用獎勵函數為該回應生成獎勵值。

167
00:04:23,054 --> 00:04:26,057
And we use that signal to update the model.

168
00:04:23,054 --> 00:04:26,057
接著我們利用這個信號來更新模型。

169
00:04:26,057 --> 00:04:28,518
So in this case, when you have like 1000

170
00:04:26,057 --> 00:04:28,518
在這種情況下，當你擁有約 1000

171
00:04:28,518 --> 00:04:29,018
So in this case, when you have like 1000

172
00:04:28,518 --> 00:04:29,018
在這種情況下，當你擁有約 1000

173
00:04:29,018 --> 00:04:32,021
to maybe 10 million or more number of prompts,

174
00:04:29,018 --> 00:04:32,021
到可能 1000 萬或更多的提示數量時，

175
00:04:32,021 --> 00:04:34,607
and the target here is to maximize

176
00:04:32,021 --> 00:04:34,607
而這裡的目標是要最大化

177
00:04:34,607 --> 00:04:37,610
the reward for the prompt and response

178
00:04:34,607 --> 00:04:37,610
提示與回應的獎勵

179
00:04:37,610 --> 00:04:40,989
where the response is actually generated by the language model itself.

180
00:04:37,610 --> 00:04:40,989
其中回應實際上是由語言模型本身生成。

181
00:04:41,197 --> 00:04:44,409
Usually post-training requires getting three elements correct.

182
00:04:41,197 --> 00:04:44,409
通常後續訓練需要確保三個要素正確。

183
00:04:44,659 --> 00:04:47,870
The first one is a good co-design of data and algorithm.

184
00:04:44,659 --> 00:04:47,870
第一個是數據與演算法的良好協同設計。

185
00:04:47,996 --> 00:04:49,622
As we discuss, there are different choices of post-training elements,

186
00:04:47,996 --> 00:04:49,622
如同我們所討論的，在後訓練階段有各種不同的元素選擇，

187
00:04:49,622 --> 00:04:51,958
As we discuss, there are different choices of post-training elements,

188
00:04:49,622 --> 00:04:51,958
如同我們所討論的，在後訓練階段有各種不同的元素選擇，

189
00:04:52,458 --> 00:04:57,171
including SFT, DPO, all or different online real fast learning

190
00:04:52,458 --> 00:04:57,171
包括監督式微調(SFT)、直接偏好優化(DPO)，以及各種線上即時快速學習

191
00:04:57,213 --> 00:05:01,676
algorithm like Reinforce/RLOO, GRPO or PPO.

192
00:04:57,213 --> 00:05:01,676
演算法，如強化學習/反向強化學習(RLOO)、GRPO 或近端策略優化(PPO)。

193
00:05:01,968 --> 00:05:05,930
Each of them only require a slightly different data structure to prepare.

194
00:05:01,968 --> 00:05:05,930
它們各自只需要稍微不同的資料結構來準備。

195
00:05:06,306 --> 00:05:10,059
A good co-design of data and algorithm will be really important

196
00:05:06,306 --> 00:05:10,059
良好的資料與演算法共同設計將非常重要

197
00:05:10,351 --> 00:05:12,395
for your success of Post-training.

198
00:05:10,351 --> 00:05:12,395
對於後訓練的成功。

199
00:05:12,395 --> 00:05:15,606
The second element is a reliable and efficient

200
00:05:12,395 --> 00:05:15,606
第二個要素是可靠且高效的

201
00:05:15,606 --> 00:05:18,985
library that implements most of the algorithms correctly.

202
00:05:15,606 --> 00:05:18,985
正確實現大多數演算法的函式庫。

203
00:05:18,985 --> 00:05:23,531
This includes HuggingFace TRL to which is one of the first library

204
00:05:18,985 --> 00:05:23,531
這包括 HuggingFace TRL，它是最早的函式庫之一

205
00:05:23,781 --> 00:05:27,577
that's simple to use and implements most of the algorithms mentioned here.

206
00:05:23,781 --> 00:05:27,577
不僅易於使用，還實現了這裡提到的大多數演算法。

207
00:05:27,869 --> 00:05:31,205
Throughout this course, we will be using this TRL

208
00:05:27,869 --> 00:05:31,205
在本課程中，我們將使用這個 TRL 函式庫

209
00:05:31,414 --> 00:05:34,709
for most of the coding practices. Besides HuggingFace TRL,

210
00:05:31,414 --> 00:05:34,709
在大多數編碼實踐中。除了 HuggingFace TRL 之外，

211
00:05:35,126 --> 00:05:38,588
I would also recommend to you to try out more sophisticated

212
00:05:35,126 --> 00:05:38,588
我也推薦你嘗試更先進

213
00:05:38,921 --> 00:05:42,675
and memory efficient libraries, including Open RLHF, veRL,

214
00:05:38,921 --> 00:05:42,675
且記憶體效率更高的函式庫，包括 Open RLHF、veRL，

215
00:05:43,051 --> 00:05:45,303
and Nemo RL.

216
00:05:43,051 --> 00:05:45,303
以及 Nemo RL。

217
00:05:45,303 --> 00:05:49,098
So the third element here would be an appropriate evaluation suite.

218
00:05:45,303 --> 00:05:49,098
因此，第三個要素將會是一套合適的評估方案。

219
00:05:49,140 --> 00:05:52,310
One needs to understand after and before post-training

220
00:05:49,140 --> 00:05:52,310
我們需要了解在後訓練前後

221
00:05:52,643 --> 00:05:56,439
what is needed as an evaluation suite that we need to track

222
00:05:52,643 --> 00:05:56,439
需要哪些評估方案來追蹤

223
00:05:56,481 --> 00:06:00,651
the model performance and ensure that the model is always performing well.

224
00:05:56,481 --> 00:06:00,651
模型的表現，並確保模型始終保持良好效能。

225
00:06:00,818 --> 00:06:04,906
Here we have an incomplete list of popular language model evaluations

226
00:06:00,818 --> 00:06:04,906
這裡有一份未完整列出的大型語言模型評估清單

227
00:06:04,947 --> 00:06:07,825
that's why you use the track and in this case.

228
00:06:04,947 --> 00:06:07,825
這就是為什麼在這個情況下你會使用追蹤功能

229
00:06:07,825 --> 00:06:11,621
So the first one, Chatbot Arena, is a human preference for chat,

230
00:06:07,825 --> 00:06:11,621
首先介紹的是 Chatbot Arena，這是一個基於人類偏好的聊天評比

231
00:06:12,038 --> 00:06:15,166
where people can vote for which model is better

232
00:06:12,038 --> 00:06:15,166
使用者可以投票選出哪個模型表現更優異

233
00:06:15,458 --> 00:06:19,045
in their own taste, and as a surrogate to human preferences,

234
00:06:15,458 --> 00:06:19,045
依照他們自己的品味，並作為人類偏好的替代品，

235
00:06:19,462 --> 00:06:22,673
they're also different LLM as a judge for chat models.

236
00:06:19,462 --> 00:06:22,673
他們也使用不同的 LLM 作為聊天模型的評判者。

237
00:06:23,132 --> 00:06:26,803
This includes Aplaca Eval, MT Bench, or Arena Hard.

238
00:06:23,132 --> 00:06:26,803
這包括 Alpaca Eval、MT Bench 或 Arena Hard。

239
00:06:27,136 --> 00:06:30,223
There are also different static benchmarks for those instruct LLM

240
00:06:27,136 --> 00:06:30,223
還有針對這些指令型 LLM 的不同靜態基準測試

241
00:06:30,890 --> 00:06:34,894
where a Live Code bench is one of the popular coding benchmark.

242
00:06:30,890 --> 00:06:34,894
其中 Live Code bench 是熱門的程式評測基準之一。

243
00:06:35,061 --> 00:06:40,061
And AIME 2024, 2025 can be a recent popular

244
00:06:35,061 --> 00:06:40,061
而 AIME 2024、2025 可作為近期熱門的

245
00:06:40,358 --> 00:06:44,404
mass evaluation dataset for hardcore mass questions.

246
00:06:40,358 --> 00:06:44,404
高難度大規模題目的群眾評測資料集。

247
00:06:44,654 --> 00:06:46,572
There are also knowledge and reasoning related

248
00:06:44,654 --> 00:06:46,572
此外還有與知識和推理相關的

249
00:06:46,572 --> 00:06:50,034
data set like GPQA or MMLU Pro.

250
00:06:46,572 --> 00:06:50,034
像是 GPQA 或 MMLU Pro 這類的資料集。

251
00:06:50,660 --> 00:06:53,996
There are also instruction following evaluation dataset like IFEval.

252
00:06:50,660 --> 00:06:53,996
還有像是 IFEval 這樣的指令遵循評估資料集。

253
00:06:54,205 --> 00:06:55,998
For function calling and agent,

254
00:06:54,205 --> 00:06:55,998
至於函式呼叫和代理程式方面，

255
00:06:55,998 --> 00:07:00,545
there are also different dataset for evaluation which includes BFCL,

256
00:06:55,998 --> 00:07:00,545
也有不同的評估資料集，包括 BFCL 在內。

257
00:07:00,962 --> 00:07:03,965
NexusBench, TauBench or ToolSandbox

258
00:07:00,962 --> 00:07:03,965
NexusBench、TauBench 或 ToolSandbox

259
00:07:04,132 --> 00:07:08,469
where both TauBench and ToolSandbox focus more on multi

260
00:07:04,132 --> 00:07:08,469
其中 TauBench 和 ToolSandbox 更著重於多

261
00:07:08,469 --> 00:07:12,849
ton tool using situation. By listing all the evaluations here,

262
00:07:08,469 --> 00:07:12,849
工具使用情境。透過列出所有評估項目，

263
00:07:13,433 --> 00:07:17,770
I'd like to mention here, that it's easy to improve any of the benchmarks,

264
00:07:13,433 --> 00:07:17,770
我想在此說明，要提升任何一項基準測試的表現都很容易，

265
00:07:17,895 --> 00:07:21,065
but it can be much harder to improve some benchmark

266
00:07:17,895 --> 00:07:21,065
但要提升某些基準測試表現

267
00:07:21,315 --> 00:07:24,861
or change certain model behavior without degrading other domains.

268
00:07:21,315 --> 00:07:24,861
或改變特定模型行為而不影響其他領域，可能會困難得多。

269
00:07:25,153 --> 00:07:28,489
Throughout this course, we'll be exploring which method

270
00:07:25,153 --> 00:07:28,489
在本課程中，我們將探討哪種方法

271
00:07:28,573 --> 00:07:31,701
gives the best improvement without degrading other domains.

272
00:07:28,573 --> 00:07:31,701
能在不影響其他領域的前提下帶來最佳改進效果。

273
00:07:31,784 --> 00:07:34,871
Lastly, I want to mention that it's not necessarily

274
00:07:31,784 --> 00:07:34,871
最後，我想提到的是，並非

275
00:07:34,871 --> 00:07:37,999
in every use cases you have to do post-training of your model.

276
00:07:34,871 --> 00:07:37,999
在所有使用情境下都必須對模型進行後續訓練。

277
00:07:38,416 --> 00:07:41,961
So there are different scenarios where there might be different methods

278
00:07:38,416 --> 00:07:41,961
不同的情境可能有更適合的方法

279
00:07:41,961 --> 00:07:44,130
that are more appropriate for your use case.

280
00:07:41,961 --> 00:07:44,130
來滿足你的使用需求。

281
00:07:44,130 --> 00:07:48,009
For example, if you just want the model to follow a few instructions,

282
00:07:44,130 --> 00:07:48,009
舉例來說，若你只希望模型遵循少數指令，

283
00:07:48,342 --> 00:07:52,305
like do not discuss something sensitive or do not compare your company

284
00:07:48,342 --> 00:07:52,305
例如不討論敏感話題，或不將自家公司

285
00:07:52,305 --> 00:07:56,559
with some other company, one can easily do prompting to make this happen.

286
00:07:52,305 --> 00:07:56,559
與其他公司進行比較，透過簡單的提示就能達成。

287
00:07:56,559 --> 00:08:00,688
So, usually self prompting method can be simple yet brittle.

288
00:07:56,559 --> 00:08:00,688
因此，自我提示法通常簡單卻也較為脆弱。

289
00:08:00,771 --> 00:08:02,148
In external cases,

290
00:08:00,771 --> 00:08:02,148
在外部情況下，

291
00:08:02,148 --> 00:08:05,526
the models may not always follow all the instructions you provide here.

292
00:08:02,148 --> 00:08:05,526
模型可能不會總是遵循您在此提供的所有指令。

293
00:08:05,943 --> 00:08:10,943
A second use case, might be about query some real-time database or knowledge base,

294
00:08:05,943 --> 00:08:10,943
第二個使用案例可能是查詢某些即時資料庫或知識庫，

295
00:08:11,324 --> 00:08:15,536
in which case of retrieval augmented generation or search-based measure

296
00:08:11,324 --> 00:08:15,536
這種情況下會採用檢索增強生成或基於搜尋的方法

297
00:08:15,745 --> 00:08:20,291
could work better since it can adapt to a rapidly changing knowledge base here.

298
00:08:15,745 --> 00:08:20,291
可能效果更佳，因為它能適應快速變化的知識庫。

299
00:08:20,583 --> 00:08:24,420
There are also scenarios where you like to create a domestic-specific model,

300
00:08:20,583 --> 00:08:24,420
也有些情境下你會想建立特定領域的模型，

301
00:08:24,837 --> 00:08:27,840
like medical language model or cybersecurity language model.

302
00:08:24,837 --> 00:08:27,840
例如醫療語言模型或資安語言模型。

303
00:08:27,882 --> 00:08:32,135
So in those cases, usually what really matters is a continual pre-training

304
00:08:27,882 --> 00:08:32,135
在這些情況下，通常真正重要的是持續性的預訓練

305
00:08:32,470 --> 00:08:35,681
followed by a more standard post training to make the model

306
00:08:32,470 --> 00:08:35,681
接著進行更標準的後續訓練來完善模型

307
00:08:35,681 --> 00:08:39,684
first learn the knowledge, then learn how to talk to the user.

308
00:08:35,681 --> 00:08:39,684
先學習知識，再學習如何與使用者對話

309
00:08:39,727 --> 00:08:43,105
So in this case, for continual pre-training what usually

310
00:08:39,727 --> 00:08:43,105
因此在這種情況下，持續預訓練通常

311
00:08:43,105 --> 00:08:46,234
who inject a very large scale domain knowledge

312
00:08:43,105 --> 00:08:46,234
會注入大規模的領域知識

313
00:08:46,651 --> 00:08:49,570
that's not seen during the pre-training dataset,

314
00:08:46,651 --> 00:08:49,570
這些是在預訓練資料集中未曾出現的，

315
00:08:49,570 --> 00:08:53,366
and ideally those domain knowledge should be at least more than 1 billion

316
00:08:49,570 --> 00:08:53,366
而理想情況下，這些領域知識至少應超過 10 億

317
00:08:53,366 --> 00:08:54,408
number of tokens.

318
00:08:53,366 --> 00:08:54,408
個詞元。

319
00:08:54,408 --> 00:08:59,330
And lastly, if your use case is about following 20 or more instructions tightly,

320
00:08:54,408 --> 00:08:59,330
最後，若您的使用情境需要嚴格遵循 20 個或更多指令，

321
00:08:59,872 --> 00:09:04,168
or you really want to improve some target capabilities like create

322
00:08:59,872 --> 00:09:04,168
或者您確實想要提升某些特定能力，例如打造

323
00:09:04,168 --> 00:09:07,713
a strong SQL model, a function calling model, or a reasoning model,

324
00:09:04,168 --> 00:09:07,713
一個強大的 SQL 模型、函數呼叫模型，或是推理模型，

325
00:09:08,506 --> 00:09:11,008
this is where Post-training can be most helpful.

326
00:09:08,506 --> 00:09:11,008
這時候後訓練就能發揮最大效益。

327
00:09:11,008 --> 00:09:15,012
It can help to reliably change the model behavior and improve

328
00:09:11,008 --> 00:09:15,012
它能可靠地改變模型行為並提升表現

329
00:09:15,012 --> 00:09:16,389
targeted capabilities.

330
00:09:15,012 --> 00:09:16,389
目標能力

331
00:09:16,389 --> 00:09:19,350
So if poisoning is not done correctly,

332
00:09:16,389 --> 00:09:19,350
因此，如果毒化處理不當

333
00:09:19,559 --> 00:09:22,812
it might degrade other capabilities that you didn't train on.

334
00:09:19,559 --> 00:09:22,812
可能會導致你未訓練的其他能力下降

335
00:09:22,853 --> 00:09:26,774
So in this lesson, you have learned about what is post-training,

336
00:09:22,853 --> 00:09:26,774
在本課程中，你已學習到何謂後訓練

337
00:09:27,233 --> 00:09:30,236
how to do post-training and when to do post-training.

338
00:09:27,233 --> 00:09:30,236
如何進行後訓練以及何時進行後訓練

339
00:09:30,319 --> 00:09:33,739
In the next lesson, we'll have a deep dive into the first method

340
00:09:30,319 --> 00:09:33,739
在下一堂課中，我們將深入探討第一種方法

341
00:09:33,739 --> 00:09:36,742
of Post-training, which is supervised fine-tuning.

342
00:09:33,739 --> 00:09:36,742
也就是監督式微調的後訓練方式

343
00:09:36,951 --> 00:09:38,202
All right. See you there.

344
00:09:36,951 --> 00:09:38,202
好的，我們課堂上見。

